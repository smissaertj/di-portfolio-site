<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Joeri JM Smissaert</title>
        <link>https://joerismissaert.dev/posts/</link>
        <description>Recent content in Posts on Joeri JM Smissaert</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>&lt;a href=&#34;https://joerismissaert.dev/privacy-policy&#34;&gt;Privacy Policy&lt;/a&gt; &amp; &lt;a href=&#34;https://joerismissaert.dev/terms-of-service&#34;&gt;Terms of Service&lt;/a&gt;</copyright>
        <lastBuildDate>Mon, 07 Dec 2020 00:00:00 +0000</lastBuildDate>
        <atom:link href="https://joerismissaert.dev/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Configuring and Auto Mounting Remote File Systems Using fstab and automount: NFS &amp; CIFS</title>
            <link>https://joerismissaert.dev/configuring-and-auto-mounting-remote-file-systems-using-fstab-and-automount-NFS-CIFS/</link>
            <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/configuring-and-auto-mounting-remote-file-systems-using-fstab-and-automount-NFS-CIFS/</guid>
            <description>firewalld: nfs, mountd, rpc-bind
NFS Server yum install nfs-utils
Mounting nfs shares showmount -e nfs-server mount nfs-server:/share /mnt /etc/fstab ==&amp;gt; _netdev mount option. nfs-server:/data /nfs nfs _netdev 0 0
Mounting Samba shares cifs-utils samba-client smbclient -L //sambahost mount -o username=sambeuser //sambahost/share /mountpoint _netdev,username=,password= or noauto
If you want to allow samba to modify public files used for public file transfer services. Files/Directories must be labeled public_content_rw_t., you must turn on the smbd_anon_write boolean.</description>
            <content type="html"><![CDATA[<figure class="center">
    <img src="/img/redhat-8-logo.png"
         alt="Red Hat logo"/> 
</figure>

<p>firewalld: nfs, mountd, rpc-bind</p>
<h1 id="nfs-server">NFS Server</h1>
<p>yum install nfs-utils</p>
<h1 id="mounting-nfs-shares">Mounting nfs shares</h1>
<p>showmount -e nfs-server
mount nfs-server:/share /mnt
<code>/etc/fstab</code> ==&gt; <code>_netdev</code> mount option.
nfs-server:/data  /nfs  nfs _netdev 0 0</p>
<h1 id="mounting-samba-shares">Mounting Samba shares</h1>
<p>cifs-utils samba-client
smbclient -L //sambahost
mount -o username=sambeuser //sambahost/share /mountpoint
<code>_netdev,username=,password=</code> or <code>noauto</code></p>
<p>If you want to allow samba to modify public files used for public file transfer services.  Files/Directories must be labeled public_content_rw_t., you must turn on the smbd_anon_write boolean.</p>
<p>semanage fcontext -a -t public_content_rw_t &ldquo;/mnt/int(/.*)?&rdquo;
restorecon -RvvF /mnt/int</p>
<p>setsebool -P smbd_anon_write on</p>
<h1 id="automount">automount</h1>
<h1 id="using-nfs-services">Using NFS Services</h1>
<p>The Network File System is a protocol that was developed for UNIX by Sun in the early 1980s. Its purpose is to make mounting of remote file systems in the local file system hierarchy possible. It was often used with Network Information Services (NIS) which provides network-based authentication, all machines connected to the NIS server used the same user accounts and security was handled by the NIS server. NFS security by default is limited to allowing and restricting specific hosts.</p>
<p>Without NIS, NFS seems to be an unsecure solution: if on server1 the user X has UID 1001 and on server2 user Y has UID 1001, then user X would have the same access to server2 resources as user Y. To prevent situations like this, NFS should be used together with a centralized authentication service like the Lightweight Directory Access Protocol (LDAP) and Kerberos. This solution is not covered in this post.</p>
<p>On RHEL8, NFSv4 is the default version of NFS wichi you can override when mounting using the <code>nfsvers=</code> mount option. Typically, clients will automatically fallback to a previous version of NFS if required.</p>
<h2 id="offering-an-nfs-share">Offering an NFS Share</h2>
<p>To setup an NFS share you would need to go through a few tasks:</p>
<ul>
<li>Create local directories which you want to share and copy some data into them:</li>
</ul>
<pre><code>[root@server1 ~]# mkdir -p /nfsdata /users/user{1..2}
[root@server1 ~]# cp -r /etc/[a-c]* /nfsdata/
[root@server1 ~]# cp -r /etc/[d-f]* /users/user1/
[root@server1 ~]# cp -r /etc/[g-i]* /users/user2/
</code></pre><ul>
<li>Edit the <code>/etc/exports</code> file to define the NFS shares:</li>
</ul>
<pre><code>[root@server1 ~]# cat /etc/exports
/nfsdata	*(rw,no_root_squash)
/users		*(rw,no_root_squash)
</code></pre><ul>
<li>Start and enable the NFS server:</li>
</ul>
<pre><code>[root@server1 ~]# yum install nfs-utils
[root@server1 ~]# systemctl enable --now nfs-server
</code></pre><ul>
<li>Configure the firewall to allow incoming NFS traffic</li>
</ul>
<pre><code>[root@server1 ~]# firewall-cmd --add-service=nfs --permanent
success
[root@server1 ~]# firewall-cmd --add-service=rpc-bind --permanent
success
[root@server1 ~]# firewall-cmd --add-service=mountd --permanent
success
[root@server1 ~]# firewall-cmd --reload
success
</code></pre><h2 id="mounting-the-nfs-share">Mounting the NFS Share</h2>
<p>In order to mount an NFS share we need to know the name of the share. Typically this information is known by the administrator, but you have multiple options to discover what shares are available:</p>
<ul>
<li>If NFSv4 is used on the server, you can use a root mount. You mount the root directory of the NFS server and you&rsquo;ll see all shares you have access to under your local mount point.</li>
<li>Use the <code>showmount -e</code> command</li>
</ul>
<blockquote>
<p>The <code>showmount</code> command may have issues with NFSv4 servers that are behind a firewall. The command relies on the portmapper service which uses random UDP ports while the firwall nfs service opens port 2049 only which doesn&rsquo;t allow portmapper traffic. In these cases you can use the root mount option to discover the shares.</p>
</blockquote>
<pre><code>[root@server2 ~]# showmount -e server1
Export list for server1:
/users   *
/nfsdata *
</code></pre><pre><code>[root@server2 ~]# mount server1:/ /mnt
[root@server2 ~]# ls /mnt/
nfsdata  users
</code></pre><h1 id="using-cifs-services">Using CIFS Services</h1>
<p>Microsoft published the technical specifications of its Server Message Block (SMB) protocol. This protocol is the foundation of all shares that are created in a Windows environment.
Releasing these specifications led to the start of the Samba project. The goal of this project was to provide SMB services on top of other operating systems. Samba has developed into the standard for file sharing between different operating systems and is now often referred to as the Common Internet File System (CIFS).</p>
<h2 id="setting-up-a-samba-server">Setting Up a Samba Server</h2>
<p>Before jumping into configuring the samba server, let&rsquo;s clearly define our goals.
Server2, the samba server, should be sharing the following directories:</p>
<ul>
<li><code>/var/samba/public_read_share</code> - read only access for guests, mounted on <code>/mnt/public_read_share</code></li>
<li><code>/var/samba/public_write_share</code> - read/write permissions for guests, mounted on <code>/mnt/public_write_share</code></li>
<li><code>/var/samba/student_share</code> - read permissions for guests, read/write permissions for users in the <code>students</code> group. Mounted on <code>/mnt/students_share</code>.</li>
</ul>
<h3 id="installing-and-configuring-samba">Installing and Configuring Samba</h3>
<p>Install the samba package and create the shared directories:</p>
<pre><code>[root@server2 ~]# yum install samba -y
...
[root@server2 ~]# mkdir -p /var/samba/{public_share,public_write_share,students_share}
[root@server2 ~]# ls /var/samba/
public_share  public_write_share  students_share
</code></pre><p>We enable the <code>smbd_anon_write</code> SELinux Boolean which allows anonymous users to modify public files labeled with the <code>public_content_rw_t</code> file context.
Next, we set the appropriate SELinux file contexts:</p>
<ul>
<li><code>public_content_t</code> - Allows Read Only access to public files.</li>
<li><code>public_content_rw_t</code> - Allows Read/Write access to public files.</li>
<li><code>samba_share_t</code> - As samba doesn&rsquo;t have default paths for shares, we make sure SELinux recognizes our share as a standard samba share.</li>
</ul>
<pre><code>[root@server2 samba]# pwd
/var/samba

[root@server2 samba]# ls -lh
total 0
drwxr-xr-x. 2 root root 6 Mar 11 13:24 public_share
drwxr-xr-x. 2 root root 6 Mar 11 13:24 public_write_share
drwxr-xr-x. 2 root root 6 Mar 11 13:24 students_share

[root@server2 samba]# setsebool -P smbd_anon_write on
[root@server2 samba]# getsebool smbd_anon_write 
smbd_anon_write --&gt; on

[root@server2 samba]# semanage fcontext -a -t public_content_t &quot;/var/samba/public_share(/.*)?&quot;
[root@server2 samba]# semanage fcontext -a -t public_content_rw_t &quot;/var/samba/public_write_share(/.*)?&quot;
[root@server2 samba]# semanage fcontext -a -t samba_share_t &quot;/var/samba/students_share(/.*)?&quot;
[root@server2 samba]# restorecon -Rv /var/samba/
Relabeled /var/samba/public_share from unconfined_u:object_r:var_t:s0 to unconfined_u:object_r:public_content_t:s0
Relabeled /var/samba/public_write_share from unconfined_u:object_r:var_t:s0 to unconfined_u:object_r:public_content_rw_t:s0
Relabeled /var/samba/students_share from unconfined_u:object_r:var_t:s0 to unconfined_u:object_r:samba_share_t:s0

[root@server2 samba]# ls -lhZ
total 0
drwxr-xr-x. 2 root root unconfined_u:object_r:public_content_t:s0    6 Mar 11 13:24 public_share
drwxr-xr-x. 2 root root unconfined_u:object_r:public_content_rw_t:s0 6 Mar 11 13:24 public_write_share
drwxr-xr-x. 2 root root unconfined_u:object_r:samba_share_t:s0       6 Mar 11 13:24 students_share
</code></pre><p>Create the <code>students</code> group and, add the user <code>student</code> to the group.
Create the <code>smb_user</code> through which we&rsquo;ll be able to write to the <code>public_write_share</code> directory.
Add the Linux user <code>student</code> to samba and set a password. This credential will be used to authenticate and mount the <code>students_share</code> directory.
Set the Linux permissions on the shared directories:</p>
<pre><code>[root@server2 samba]# groupadd students
[root@server2 samba]# usermod -aG students student
[root@server2 samba]# id student
uid=1000(student) gid=1000(student) groups=1000(student),1001(students)

[root@server2 samba]# useradd smb_user --no-create-home --shell /sbin/nologin
[root@server2 samba]# 

[root@server2 samba]# smbpasswd -a student
New SMB password:
Retype new SMB password:
Added user student.

[root@server2 samba]# chgrp smb_user public_write_share
[root@server2 samba]# chmod 0770 public_write_share
[root@server2 samba]# chmod g+s public_write_share
[root@server2 samba]# 

[root@server2 samba]# chgrp students students_share
[root@server2 samba]# chmod 0775 students_share
[root@server2 samba]# chmod g+s students_share
[root@server2 samba]# 

[root@server2 samba]# ls -lhZ
total 0
drwxr-xr-x. 2 root root     unconfined_u:object_r:public_content_t:s0    6 Mar 11 13:24 public_share
drwxrws---. 2 root smb_user unconfined_u:object_r:public_content_rw_t:s0 6 Mar 11 13:24 public_write_share
drwxrwsr-x. 2 root students unconfined_u:object_r:samba_share_t:s0       6 Mar 11 13:24 students_share
</code></pre><p>Note that we don&rsquo;t change any permissions on <code>public_share</code>, since we only need read access.</p>
<p>Next, we configure the samba shares in <code>/etc/samba/smb.conf</code>:</p>
<pre><code>[root@server2 samba]# cd /etc/samba
[root@server2 samba]# mv smb.conf smb.conf.old
[root@server2 samba]# vim smb.conf
...

[root@server2 samba]# testparm
Load smb config files from /etc/samba/smb.conf
Loaded services file OK.
Server role: ROLE_STANDALONE

Press enter to see a dump of your service definitions

# Global parameters
[global]
	security = USER
	workgroup = SAMBA
	idmap config * : backend = tdb


[public_read]
	comment = Public Read Only Share
	guest ok = Yes
	path = /var/samba/public_share


[public_write]
	comment = Public Read/Write Share
	force user = smb_user
	guest ok = Yes
	path = /var/samba/public_write_share
	read only = No
	write list = smb_user


[students]
	comment = Read/Write access for the students group. Read access for anyone else.
	guest ok = Yes
	path = /var/samba/students_share
	write list = +students
</code></pre><p>We need to allow samba traffic through our firewall:</p>
<pre><code>[root@server2 samba]# firewall-cmd --add-service=samba --permanent
success
[root@server2 samba]# firewall-cmd --reload
success
</code></pre><p>The final step before moving on to the client side would be to start and enable the samba service:</p>
<pre><code>[root@server2 samba]# systemctl enable --now smb
Created symlink /etc/systemd/system/multi-user.target.wants/smb.service → /usr/lib/systemd/system/smb.service.
</code></pre><h2 id="discovering-cifs-shares">Discovering CIFS Shares</h2>
<p>On <code>server1</code>, where the shares are going to be mounted, you discover available shares using the <code>smbclient -L //hostname</code> command.
Make sure you have the <code>cifs-utils</code> and <code>samba-client</code> packages installed:</p>
<pre><code>[root@server1 ~]# yum install -y cifs-utils samba-client
...
</code></pre><p>Let&rsquo;s discover the shares we created on <code>server2</code>. When you&rsquo;re prompted for a password, just hit Enter without providing a password.</p>
<pre><code>[root@server1 ~]# smbclient -L //server2
Enter SAMBA\root's password: 
Anonymous login successful

	Sharename       Type      Comment
	---------       ----      -------
	public_read     Disk      Public Read Only Share
	public_write    Disk      Public Read/Write Share
	students        Disk      Read/Write access for the students group. Read access for anyone else.
	IPC$            IPC       IPC Service (Samba 4.12.3)
SMB1 disabled -- no workgroup available
[root@server1 ~]#
</code></pre><p>We&rsquo;re ready to move to the next step and mount our shares.</p>
<h2 id="mounting-and-authenticating-to-samba-shares">Mounting and Authenticating to Samba Shares</h2>
<p>In the previous steps we created two guest shares and one share that needs authentication.
We can mount these as follows:</p>
<ul>
<li><code>mount -t cifs -o guest //server2/public_read /mnt/public_read_share</code></li>
<li><code>mount -t cifs -o guest //server2/public_write /mnt/public_write_share</code></li>
<li><code>mount -t cifs -o username=student,password=password //server2/students_share /mnt/students_share</code></li>
</ul>
<p>Before you do so, create the local mount points:</p>
<pre><code>[root@server1 ~]# mkdir /mnt/{public_read_share,public_write_share,students_share}
[root@server1 ~]# ls -l /mnt/
total 0
drwxr-xr-x. 2 root root 6 Mar 11 14:41 public_read_share
drwxr-xr-x. 2 root root 6 Mar 11 14:41 public_write_share
drwxr-xr-x. 2 root root 6 Mar 11 14:41 students_share

[root@server1 ~]# mount -t cifs -o guest //server2/public_read /mnt/public_read_share
[root@server1 ~]# mount -t cifs -o guest //server2/public_write /mnt/public_write_share
[root@server1 ~]# mount -t cifs -o username=student,password=password //server2/students /mnt/students_share
</code></pre><p>Next, test the read/write access to the shares. The outcome should be as expected.</p>
<blockquote>
<p>Note that we&rsquo;ve mounted the share as <code>root</code>, this means the <code>/mnt/students_share</code> directory will only be writeable for the user <code>root</code>. In the next step we&rsquo;ll cover how to auto mount the share at boot time.</p>
</blockquote>
<h1 id="mounting-remote-file-systems-through-fstab">Mounting Remote File Systems Through fstab</h1>
<h1 id="using-automount-to-mount-remote-file-systems">Using Automount to Mount Remote File Systems</h1>
<pre><code>//server2/students	/mnt/students_share	cifs	user,username=student,password=password,gid=1002,file_mode=0664,dir_mode=0775 0 0
//server2/public_read	/mnt/public_read_share	cifs	user,guest	0 0
//server2/public_write	/mnt/public_write_share cifs	user,guest,file_mode=0777,dir_mode=0777	0 0
</code></pre><p>sdf</p>
]]></content>
        </item>
        
        <item>
            <title>Managing a Firewall with Firewalld</title>
            <link>https://joerismissaert.dev/managing-a-firewall-with-firewalld/</link>
            <pubDate>Sat, 07 Nov 2020 00:00:00 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/managing-a-firewall-with-firewalld/</guid>
            <description>Understanding Linux Firewalling Firewalling is implemented in the Linux kernel by means of the netfilter subsystem to limit traffic coming in to a server or going out of the server. Netfilter allows kernel modules to inspect every incoming, outgoing, or forwarded packet and act upon it by either allowing it or blocking it. In essence, netfilter controls access to and from the network stack at the Linux kernel module level.</description>
            <content type="html"><![CDATA[<p><figure class="center">
    <img src="/img/redhat-8-logo.png"
         alt="Red Hat logo"/> 
</figure>

<figure class="center">
    <img src="/img/firewalld.png"
         alt="Firewalld logo" width="200px"/> 
</figure>
</p>
<h1 id="understanding-linux-firewalling">Understanding Linux Firewalling</h1>
<p>Firewalling is implemented in the Linux kernel by means of the <a href="http://www.netfilter.org/" target="_blank">netfilter</a>

 subsystem to limit traffic coming in to a server or going out of the server. Netfilter allows kernel modules to inspect every incoming, outgoing, or forwarded packet and act upon it by either allowing it or blocking it. In essence, netfilter controls access to and from the network stack at the Linux kernel module level.</p>
<p>Iptables was the default solution to interact with netfilter, it provides a sophisticated way of defining firewall rules but it&rsquo;s also challenging to use due to the complicated syntax and the ordering of rules which can become complex. The iptables service is no longer offered in RHEL8, it has been replaced with <strong>nftables</strong>, a new solution with more advanced options.</p>
<h2 id="firewalld">Firewalld</h2>
<p>Firewalld is a higher-level netfilter implementation that is more user-friendly compared to iptables or nftables. While administrators can manage the Firewalld rules, applications can also communicate with it using the DBus messaging system: rules can be added or removed without any direct action required from the system administrator. Applications can address the firewall from user space.</p>
<blockquote>
<p>Firewalld applies rules to incoming packets only by default, no filtering happens on outgoing packets.</p>
</blockquote>
<h3 id="firewalld-zones">Firewalld Zones</h3>
<p>Firewalld makes management easier by working with <em>zones</em>. A zone is a collection of rules that are applied to incoming packets matching a specific source address or network interface.</p>
<p>The use of zones is import on servers that have multiple network interfaces. Each interface could be a different zone where different rules would apply. On a machine with only one network interface you can work with one zone, the <em>default</em> zone.</p>
<p>Every packet that comes into a system is analyzed for its source address, based on the source address Firewalld decides if it belongs to a specific zone. If not, the zone for the incoming network interface is used. If no specific zone is available, the packet is handled by the rules in the <em>default</em> zone.</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Zone Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>block</td>
<td>Incoming network connections are rejected with the &ldquo;icmp-host-prohibited&rdquo; message. Connections that were initiated on this system are allowed.</td>
</tr>
<tr>
<td>dmz</td>
<td>For use on computers in the demilitarized zone. Selected incoming connections are accepted, and limited access to the internal network is allowed.</td>
</tr>
<tr>
<td>drop</td>
<td>Any incoming packets are dropped and there is no reply.</td>
</tr>
<tr>
<td>external</td>
<td>For use on external networks with masquarading (Network Address Translation) enabled, used on routers. Selected incoming connections are accepted.</td>
</tr>
<tr>
<td>home</td>
<td>Most computers on the same network are trusted, only selected incoming connections are accepted.</td>
</tr>
<tr>
<td>internal</td>
<td>Most computers on the same network are trusted, only selected incoming connections are accepted.</td>
</tr>
<tr>
<td>public</td>
<td>Other computers on the same network are not trused, limited connections are accepted. This is the <em>default</em> zone for all newly created network interfaces.</td>
</tr>
<tr>
<td>trusted</td>
<td>All network connections are accepted.</td>
</tr>
<tr>
<td>work</td>
<td>Most computers on the same network are trusted, only selected incoming connections are accepted.</td>
</tr>
</tbody>
</table>

<h3 id="firewalld-services">Firewalld Services</h3>
<p>Services are the second key element while working with Firewalld.
A service in Firewalld is not the same as a service in systemd. A Firewalld service defines what exactly should be accepted as incoming traffic in the firewall, it includes ports to be opened and supoorting kernel modules that should be loaded.</p>
<p>Behind each service is an XML configuration file that explains which TCP or UDP ports are involved and, if required, what kernel modules must be loaded.  Default (RPM installed) XML files are stored in <code>/usr/lib/firewalld/services</code> while custom XML files can be added to the <code>/etc/firewalld/services</code> directory.</p>
<pre><code>[root@localhost ~]# firewall-cmd --get-services
RH-Satellite-6 amanda-client amanda-k5-client amqp amqps ...
...


[root@localhost ~]# cat /usr/lib/firewalld/services/ftp.xml 
&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;service&gt;
  &lt;short&gt;FTP&lt;/short&gt;
  &lt;description&gt;FTP is a protocol used for remote file transfer. If you plan to make your FTP server publicly available, enable this option. You need the vsftpd package installed for this option to be useful.&lt;/description&gt;
  &lt;port protocol=&quot;tcp&quot; port=&quot;21&quot;/&gt;
  &lt;helper name=&quot;ftp&quot;/&gt;
&lt;/service&gt;
</code></pre><h2 id="working-with-firewalld">Working with Firewalld</h2>
<p>Firewalld provides a command-line interface tool that works with a runtime and permament (on-disk) configuration state: <strong>firewall-cmd</strong></p>
<p>Below is an example of how you can use the tool to retrieve current settings and make configuration changes. Always make sure to commit changes to disk using the <code>--permanent</code> flag so that your changes survive a reboot, then <code>--reload</code> to apply the changes to the runtime environment.</p>
<pre><code>[root@localhost ~]# firewall-cmd --get-default-zone
public

[root@localhost ~]# firewall-cmd --get-zones
block dmz drop external home internal libvirt public trusted work

[root@localhost ~]# firewall-cmd --list-all --zone=public
public (active)
  target: default
  icmp-block-inversion: no
  interfaces: enp1s0
  sources: 
  services: cockpit dhcpv6-client ftp http https ssh
  ports: 
  protocols: 
  masquerade: no
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules:

[root@localhost ~]# firewall-cmd --get-services
RH-Satellite-6 amanda-client amanda-k5-client amqp amqps apcupsd audit bacula bacula-client bb bgp bitcoin bitcoin-rpc bitcoin-testnet
...

[root@localhost ~]# firewall-cmd --list-services
cockpit dhcpv6-client ftp http https ssh
	
[root@localhost ~]# firewall-cmd --add-service=vnc-server --permanent
success

[root@localhost ~]# firewall-cmd --list-services
cockpit dhcpv6-client ftp http https ssh

[root@localhost ~]# firewall-cmd --reload
success

[root@localhost ~]# firewall-cmd --list-services
cockpit dhcpv6-client ftp http https ssh vnc-server

[root@localhost ~]# firewall-cmd --add-port=2022/tcp --permanent
success

[root@localhost ~]# firewall-cmd --reload
success

[root@localhost ~]# firewall-cmd --list-all
public (active)
  target: default
  icmp-block-inversion: no
  interfaces: enp1s0
  sources: 
  services: cockpit dhcpv6-client ftp http https ssh vnc-server
  ports: 2022/tcp
  protocols: 
  masquerade: no
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 
</code></pre><h4 id="key-commands">Key Commands</h4>
<pre><code>firewall-cmd --list-all
firewall-cmd --list-all --zone=public

firewall-cmd --get-default-zone
firewall-cmd --get-zones

firewall-cmd --get-services
firewall-cmd --list-services

firewall-cmd --add-service ftp
irewall-cmd --add-service ftp --permanent
firewall-cmd --reload

firewall-cmd --add-port=2022/tcp --permanent
firewall-cmd --reload
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Enhancing Linux Security with SELinux</title>
            <link>https://joerismissaert.dev/enhancing-linux-security-with-selinux/</link>
            <pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/enhancing-linux-security-with-selinux/</guid>
            <description>SELinux is a security enhancement module, deployed on top of Linux, which provides improved security via Role Based Access Controls (RBACs) on subjects and objects (processes and resources). Traditional Linux security used Discretionary Access Controls (DACs).
With DAC, a process can access any file, directory, device or other resource that leaves itself open to access. Using RBAC, a process only has access to resources that it is explicitely allowd to access, based on the assigned role.</description>
            <content type="html"><![CDATA[<p><figure class="center">
    <img src="/img/redhat-8-logo.png"
         alt="Red Hat logo"/> 
</figure>

<figure class="center">
    <img src="/img/selinux.png"
         alt="SELinux logo" width="200px"/> 
</figure>
</p>
<p>SELinux is a security enhancement module, deployed on top of Linux, which provides improved security via Role Based Access Controls (RBACs) on subjects and objects (processes and resources). Traditional Linux security used Discretionary Access Controls (DACs).</p>
<p>With DAC, a process can access any file, directory, device or other resource that leaves itself open to access. Using RBAC, a process only has access to resources that it is explicitely allowd to access, based on the assigned role. The way that SELinux implements RBAC is to assign an SELinux policy to a process. That process restricts access as follows:</p>
<ul>
<li>Only let the process access resources that carry the explicit labels</li>
<li>Make potentially insecure features, e.g. write access to a directory, available as Booleans, which can be turned on or off.</li>
</ul>
<p>SELinux is not a replacement for DAC, it&rsquo;s an additional security layer:</p>
<ul>
<li>DAC rules are still used when using SELinux;</li>
<li>DAC rules are checked first, if those allow access then SELinux policies are checked;</li>
<li>If DAC rules deny access then SELinux policies are not checked.</li>
</ul>
<p>In essence, SELinux severaly limits what potentially malicious code may gain access to and generally limits activity on the Linux system.</p>
<h1 id="understanding-how-selinux-works">Understanding How SELinux Works</h1>
<p>SELinux provides a combination of Role Based Access Control and either <em>Type Enforcement</em> (TE) or <em>Multi-Level Security</em> (MLS). In RBAC, access to an object is granted or denied based on the subject&rsquo;s assigned role in the organization. It&rsquo;s not based on usernames or process ID. In this post I will focus only on Type Enforcement, which is the default SELinux <em>targeted policy</em>.</p>
<h2 id="type-enforcement">Type Enforcement</h2>
<p>Type Enforcement is necessary to implement the RBAC model, it secures a system through these methods:</p>
<ul>
<li>Labeling objects as certain security types;</li>
<li>Assigning subjects to particular domains and roles;</li>
<li>Providing rules to allow certain domains and roles to access certain object types.</li>
</ul>
<p>Let&rsquo;s look at an example.
The below <code>ls -l</code> command shows the DAC controls on the files. The output shows the file&rsquo;s owner, group and permissions:</p>
<pre><code>[student@localhost my_stuff]$ ls -l
total 0
-rw-rw-r--. 1 student student 0 Jan 19 06:25 test001
</code></pre><p>We can add the <code>-Z</code> option to display the SELinux RBAC controls too:</p>
<pre><code>[student@localhost my_stuff]$ ls -lZ
total 0
-rw-rw-r--. 1 student student unconfined_u:object_r:user_home_t:s0 0 Jan 19 06:25 test001
</code></pre><p>The last example displays four items assiciated with the file that are specific to SELinux:</p>
<ul>
<li><strong>user</strong> <code>unconfined_u</code></li>
<li><strong>role</strong> <code>object_r</code></li>
<li><strong>type</strong> <code>user_home_t</code></li>
<li><strong>level</strong> <code>s0</code></li>
</ul>
<p>The above four RBAC items are used in the SELinux access control  to determine appropriate access levels. Together, these items are called the SELinux <em>security context</em> or sometimes the <em>security label</em>.</p>
<p>These security contexts are given to to subjects (processes and users). Each security context has a specific name. The name given depends upon what object or subject it has been assigned: Files have a file context, users have a user context, and processes have a process context also referred to as a domain.</p>
<p>The rules allowing access are called allow rules or policy rules. A policy rule is the process SELinux follows to grant or deny access to a particular system security type. Thus, Type Enforcement ensures that only certain &ldquo;types&rdquo; of subjects can access certain &ldquo;types&rdquo; of objects.</p>
<h2 id="implementing-selinux-security-models">Implementing SELinux Security Models</h2>
<p>SELinux implements the RBAC model through a combination of four primary SELinux pieces:</p>
<ul>
<li>Operational modes</li>
<li>Security contexts</li>
<li>Policy types</li>
<li>Policy rule packages</li>
</ul>
<p>We already touched on some of these design elements.</p>
<h3 id="understanding-selinux-operational-modes">Understanding SELinux Operational Modes</h3>
<p>SELinux comes with three operational modes: <em>disabled, permissive</em> and <em>enforcing</em>.
Each of these modes offeres different benefits for Linux system security.</p>
<h4 id="using-disabled-mode">Using Disabled Mode</h4>
<p>In the <em>disabled</em> mode, SELinux is turned off. The default method of access control, Discretionary Access Control, is used instead.</p>
<h4 id="using-permissive-mode">Using Permissive Mode</h4>
<p>In <em>permissive</em> mode, SELinux is turned on, but the security policy rules are not enforced. When a security policy rule should deny access, access will still be allowed. However, a message is sent to a log file denoting that access should&rsquo;ve been denied.</p>
<p>SELinux permissive mode is useful for testing and troubleshooting.</p>
<h4 id="using-enforcing-mode">Using Enforcing Mode</h4>
<p>In <em>enforcing</em> mode SELinux is turned on and all of the security policy rules are enforced.</p>
<h3 id="understanding-selinux-security-contexts">Understanding SELinux Security Contexts</h3>
<p>An SELinux security context is the method used to classify objects (such as files) and subjects (such as users or programs). A security context consists of four attributes: <code>user</code>, <code>role</code>, <code>type</code> and <code>level</code>.</p>
<ul>
<li>
<p><strong>User</strong> - The <code>user</code> attribute is a mapping of a Linux username to an SELinux name. This is not the same as a users&rsquo;s login name, and it&rsquo;s referred to specifically as the SELinux user. The SELinux username ends with a <code>_u</code>, making it easy to identify in the output. Regular unconfined users have an <code>unconfined_u</code> user attribute in the default targeted policy.</p>
</li>
<li>
<p><strong>Role</strong> - The <code>role</code> attribute is assigned to subjects and objects. Each role is granted access to other subjects and objects based on the role&rsquo;s security clearance and the object&rsquo;s classification level. Users are assigned a role and that role is authorized for particular types of domains (or process context). The SELinux role has <code>_r</code> at the end. Processes run by <code>root</code> have a <code>system_r</code> role, while regular users run processes under the <code>unconfined_r</code> role.</p>
</li>
<li>
<p><strong>Type</strong> - The <code>type</code> attribute defines a domain type for processes, a user type for users, and a file type for files. This attribute is also called the security type. Most policy rules are concerned with the security type of a process and what files, ports, devices and other resources that process has access to based on their security types. The SELinux type name ends with a <code>_t</code>.</p>
</li>
<li>
<p><strong>Level</strong> - The <code>level</code> is an attribute of Multi-Level Security (MLS), it&rsquo;s optional in Type Enforcement.</p>
</li>
</ul>
<h4 id="users-files-and-processes-have-security-contexts">Users, Files, and Processes Have Security Contexts</h4>
<p>To see your SELinux user context, enter the <code>id</code> command at the shell prompt:</p>
<pre><code>[student@localhost ~]$ id
uid=1000(student) gid=1000(student) groups=1000(student) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
[student@localhost ~]$
</code></pre><p>Use the <code>-Z</code> option on the <code>ls</code> command to see an individual file&rsquo;s context:</p>
<pre><code>[student@localhost my_stuff]$ ls -lZ
total 0
-rw-rw-r--. 1 student student unconfined_u:object_r:user_home_t:s0 0 Jan 19 06:25 test001
</code></pre><p>Use the <code>-Z</code> option on the <code>ps</code> command to see a process&rsquo;s security context:</p>
<pre><code>[student@localhost my_stuff]$ ps -eZ | grep bash
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 2872 pts/0 00:00:00 bash

[student@localhost my_stuff]$ ps -eZ | grep systemd
system_u:system_r:init_t:s0           1 ?        00:00:01 systemd
system_u:system_r:syslogd_t:s0      638 ?        00:00:00 systemd-journal
</code></pre><h3 id="understanding-selinux-policy-types">Understanding SELinux Policy Types</h3>
<p>The policy type directly determines what sets of policy rules are used to dictate what an object can access. The policy type also determines what specific security context attributes are needed.</p>
<p>SELinux has different policies:</p>
<ul>
<li>Targeted (default)</li>
<li>MLS</li>
<li>Minimum</li>
</ul>
<p>The <em>Targeted policy&rsquo;s</em> primary purpose is to restrict &ldquo;targeted&rdquo; daemons, but it can also restrict other processes and users. Targeted daemons are sandboxed, they run in an environment where their access to other objects is tightly controlled so that no malicious attacks launched through those daemons can affect other services or the Linux system as a whole.</p>
<p>All subjects and objects not targeted are run in the <code>unconfined_t</code> domain. This domain has no SELinux policy restrictions and thus only used traditional Linux security.</p>
<h3 id="selinux-policy-rule-packages">SELinux Policy Rule Packages</h3>
<p>Policy rules are installed with SELinux and are grouped into packages, also called modules.</p>
<p>There is user documentation on these various policy modules in the form of HTML files. To view this documentation on RHEL, open your browser and enter the following url:
<code>file:///usr/share/doc/selinux-policy/html/index.html</code></p>
<p>If you don&rsquo;t have the policy documentation you can install it:
<code>yum install selinux-policy-doc</code></p>
<p>This documentation allows you to review how policy rules are created and packaged.</p>
<h1 id="configuring-selinux">Configuring SELinux</h1>
<p>SELinux comes preconfigured, you can use the SELinux features without any configuration.
The configuration can only be set and modified by <code>root</code>. The primary configuration file is <code>/etc/sysconfig/selinux</code> which is a symlink to <code>/etc/selinux/config</code>:</p>
<pre><code>[root@localhost ~]# ls -lh /etc/sysconfig/selinux 
lrwxrwxrwx. 1 root root 17 Sep 26 09:44 /etc/sysconfig/selinux -&gt; ../selinux/config
[root@localhost ~]# cat /etc/sysconfig/selinux 

# This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=enforcing
# SELINUXTYPE= can take one of these three values:
#     targeted - Targeted processes are protected,
#     minimum - Modification of targeted policy. Only selected processes are protected. 
#     mls - Multi Level Security protection.
SELINUXTYPE=targeted
</code></pre><p>This file allows you to set the mode and policy type.</p>
<h2 id="setting-the-selinux-mode-and-policy-type">Setting the SELinux Mode and Policy Type</h2>
<p>We can use the <code>getenforce</code> command to see the <em>current</em> SELinux mode, to see both the <code>current</code> mode and the mode set in the configuration file, use the <code>sestatus</code> command:</p>
<pre><code>[root@localhost ~]# getenforce
Enforcing

[root@localhost ~]# sestatus
SELinux status:                 enabled
SELinuxfs mount:                /sys/fs/selinux
SELinux root directory:         /etc/selinux
Loaded policy name:             targeted
Current mode:                   enforcing
Mode from config file:          enforcing
Policy MLS status:              enabled
Policy deny_unknown status:     allowed
Memory protection checking:     actual (secure)
Max kernel policy version:      31
</code></pre><p>To change the mode setting, you can use the <code>setenforce</code> command with either the <code>0</code> or <code>permissive</code> argument, or the <code>1</code> or <code>enforcing</code> argument.
This will change the SELinux mode during runtime and leaves the setting in the primary configuration file untouched. Rebooting the system will apply the mode set in the primary configuration file.</p>
<blockquote>
<p>You cannot use <code>setenforce</code> to change SELinux to disabled mode.</p>
</blockquote>
<p>Switching from <code>disabled</code> to <code>enforcing</code> should be done using the primary configuration file and a reboot. Using the <code>setenforce</code> command may hang your system due to incorrect file labels. Rebooting after changing from <code>disabled</code> may take a while as the filesystem will be relabeled.</p>
<p>This means that SELinux checks and changes the security context of any files with incorrect security contexts that can cause problems in the new mode, and any file not labeled will be labeled with contexts. This process can take a long time since each file&rsquo;s context is checked.</p>
<p>The policy type you choose determines whether SELinux enforces TE, MLS or Minimum. The default policy type is <code>targeted</code>.
When setting the policy type to MLS or Minimum you need to make sure you have the policy package installed:
<code>yum list selinux-policy-mls selinux-policy-minimum</code></p>
<h2 id="managing-selinux-security-contexts">Managing SELinux Security Contexts</h2>
<p>Current SELinux file and process security contexts can be viewed using the <code>secon</code> command:</p>
<ul>
<li><code>-u</code> Shows the user of the security context.</li>
<li><code>-r</code> Shows the role of the security context.</li>
<li><code>-t</code> Shows the type of the security context.</li>
</ul>
<p>Without any arguments, the command shows you the current process&rsquo;s security context:</p>
<pre><code>[student@localhost ~]$ secon -urt
user: unconfined_u
role: unconfined_r
type: unconfined_t
</code></pre><p>To view another process&rsquo;s security context, use the <code>-p</code> option followed by the process id.
e.g. <code>systemd</code>:</p>
<pre><code>[student@localhost ~]$ secon -urt -p 1
user: system_u
role: system_r
type: init_t
</code></pre><p>To view a file&rsquo;s security context, use the <code>-f</code> option:</p>
<pre><code>[student@localhost ~]$ secon -urt -f /etc/passwd
user: system_u
role: object_r
type: passwd_file_t
</code></pre><blockquote>
<p>The <code>secon</code> command does not show the security context for the current user, instead use the <code>id</code> command.</p>
</blockquote>
<h3 id="setting-security-context-types">Setting Security Context Types</h3>
<blockquote>
<p>Since the RHCSA exam focuses only on context types, I will not be covering the user and role contexts.</p>
</blockquote>
<p>To set a context type we can use the <code>semanage</code> command.
<code>semanage</code> writes the new context to the SELinux policy from where it can be applied to the file system.</p>
<p>The <code>semanage</code> command may not be available by default. You can find the RPM containing <code>semanage</code> using <code>yum whatprovides */semanage</code>:</p>
<pre><code>[root@localhost ~]# yum whatprovides */semanage
policycoreutils-python-utils-2.9-9.el8.noarch : SELinux policy core python utilities
Repo        : BaseOS
Matched from:
Filename    : /usr/sbin/semanage
Filename    : /usr/share/bash-completion/completions/semanage
</code></pre><p>The <code>policycoreutils-python-utils</code> has to be installed in order to use <code>semanage</code>.</p>
<p>To set context using <code>semanage</code> we need to know the appropriate context type. An easy way to find the appropriate context is by looking at the default context settings on already-existing items:</p>
<pre><code>[root@localhost ~]# ls -lZ /var/www
total 0
drwxr-xr-x. 2 root root system_u:object_r:httpd_sys_script_exec_t:s0  6 Jun  8  2020 cgi-bin
drwxr-xr-x. 4 root root system_u:object_r:httpd_sys_content_t:s0     61 Jan  6 12:19 html
</code></pre><p><code>/var/www/html</code> is a default location for the Apache HTTP Service. If we would want to add a new folder to <code>/var/www</code> to serve content with Apache, we now know we need the <code>http_sys_content_t</code> context type.</p>
<p>For demonstration purposes, let&rsquo;s created the <code>my_dir</code> directory in our home folder, then move it to <code>/var/www</code>. The reason why we do this is because if we create the directory in <code>/var/www</code> it will inherit the correct context type from the parent directory.</p>
<pre><code>[root@localhost ~]# ls -lZ /var/www
total 0
drwxr-xr-x. 2 root root system_u:object_r:httpd_sys_script_exec_t:s0  6 Jun  8  2020 cgi-bin
drwxr-xr-x. 4 root root system_u:object_r:httpd_sys_content_t:s0     61 Jan  6 12:19 html
drwxr-xr-x. 2 root root unconfined_u:object_r:admin_home_t:s0         6 Jan 20 11:44 my_dir
</code></pre><p>The <code>mv</code> command kept the <code>admin_home_t</code> context type on our directory.
We can change the context type as follows:</p>
<pre><code>[root@localhost ~]# semanage fcontext -a -t httpd_sys_content_t &quot;/var/www/my_dir(/.*)?&quot;
[root@localhost ~]# ls -lZd /var/www/my_dir
drwxr-xr-x. 2 root root unconfined_u:object_r:admin_home_t:s0 6 Jan 20 11:44 /var/www/my_dir
</code></pre><p>The <code>-a</code> option is used to add a context type, then we use <code>-t</code> to specify the context type. The last part of the command indicates the folder we apply the changes to and contains a regular expression, <code>(/.*)?</code>, to refer to the directory <code>my_dir</code> and anything that exists below that directory.</p>
<p>Notice how the <code>semanage</code> command didn&rsquo;t provide any output, and our <code>ls -lZd</code> command still shows the original context type.
This is because using <code>semanage</code> we only applied the context type to the SELinux policy but not to the file system. We need to apply the change to the file system using <code>restorecon</code>:</p>
<pre><code>[root@localhost ~]# restorecon -R -v /var/www/my_dir
Relabeled /var/www/my_dir from unconfined_u:object_r:admin_home_t:s0 to unconfined_u:object_r:httpd_sys_content_t:s0
[root@localhost ~]# ls -lZd /var/www/my_dir
drwxr-xr-x. 2 root root unconfined_u:object_r:httpd_sys_content_t:s0 6 Jan 20 11:44 /var/www/my_dir
</code></pre><p>The following example changes the SELinux context type on a network port, assuming you would want to make the <code>ssh</code> service available over port 2222.</p>
<pre><code>[root@localhost ~]# semanage port -l | grep ssh
ssh_port_t                     tcp      22
[root@localhost ~]# semanage port -a -t ssh_port_t -p tcp 2222
[root@localhost ~]# semanage port -l | grep ssh
ssh_port_t                     tcp      2222, 22
</code></pre><h3 id="finding-the-context-type-you-need">Finding the Context Type You Need</h3>
<p>There are three approaches in finding the context type you need:</p>
<ul>
<li>Look at the default environment;</li>
<li>Read the configuration files;</li>
<li>Use <code>man -k _selinux</code> to find the SELinux-specific man pages for your service.</li>
</ul>
<p>The man pages are not installed by default, to install them you need to install the <code>policycoreutils-devel</code> package. Once installed, use the <code>mandb</code> command to update the man page database and issue the <code>sepolicy manpage -a -p /usr/share/man/man8</code> command to install the SELinux man pages:</p>
<pre><code>[root@localhost ~]# yum whatprovides */sepolicy
policycoreutils-devel-2.9-9.el8.i686 : SELinux policy core policy devel utilities
Repo        : BaseOS
Matched from:
Filename    : /usr/bin/sepolicy
Filename    : /usr/share/bash-completion/completions/sepolicy

[root@localhost ~]# yum install -y policycoreutils-devel
...

[root@localhost ~]# sepolicy manpage -a -p /usr/share/man/man8
...

[root@localhost ~]# mandb
...

[root@localhost ~]# man -k _selinux | grep http
apache_selinux (8)   - Security Enhanced Linux Policy for the httpd processes
httpd_helper_selinux (8) - Security Enhanced Linux Policy for the httpd_helper processes
httpd_passwd_selinux (8) - Security Enhanced Linux Policy for the httpd_passwd processes
...

[root@localhost ~]# man apache_selinux
</code></pre><h3 id="restoring-default-file-contexts">Restoring Default File Contexts</h3>
<p>Previously, we applied the context type from the policy to the file system using the <code>restorecon</code> command. The policy contains the default settings for most files and directories, so if ever a wrong context setting is applied we can use <code>restorecon</code> to reapply the default from the policy to the file system.</p>
<p>Using <code>restorecon</code> this way can be useful to fix problems on new files. There&rsquo;s a specific way context settings are applied:</p>
<ul>
<li>If a new file or directory is created, it inherits the context type of the parent directory.</li>
<li>If a file or directory is copied, this is considered a new file or directory.</li>
<li>If a file is moved, or copied using <code>cp -a</code> and thus keeping properties, the original context type is applied.</li>
</ul>
<p>The latter of the above 4 ways can be fixed by using <code>restorecon</code>. It&rsquo;s also possible to relabel the entire file system using <code>restorecon -Rv /</code> or by creating the file <code>/.autorelabel</code> in the root <code>/</code>. The next time you reboot the system will discover the <code>/.autorelabel</code> file and the entire file system will be relabeled.</p>
<h3 id="managing-selinux-via-booleans">Managing SELinux via Booleans</h3>
<p>SELinux Booleans are provided to easily change the behaviour of a rule. A Boolean is a switch that toggles a setting on or off and it allows you to change parts of a SELinux policy rule without any knowledge of policy writing. These changes are applied during runtime and do not require a reboot.</p>
<p>You can get a list of Booleans using the <code>getsebool -a</code> command and filtering that down using <code>grep</code>:</p>
<pre><code>[root@localhost ~]# getsebool -a | grep httpd
httpd_anon_write --&gt; off
httpd_builtin_scripting --&gt; on
httpd_can_check_spam --&gt; off
httpd_can_connect_ftp --&gt; off
httpd_can_connect_ldap --&gt; off
httpd_can_connect_mythtv --&gt; off
httpd_can_connect_zabbix --&gt; off
...
</code></pre><p>The <code>semanage boolean -l</code> command provides more detail, it shows the current setting and the default one.</p>
<pre><code>[root@localhost ~]# semanage boolean -l | head
SELinux boolean                State  Default Description

abrt_anon_write                (off  ,  off)  Allow ABRT to modify public files used for public file transfer services.
abrt_handle_event              (off  ,  off)  Determine whether ABRT can run in the abrt_handle_event_t domain to handle ABRT event scripts.
</code></pre><p>To set a Boolean we use <code>setsebool</code> and to apply the change permanently we add the <code>-P</code> option:</p>
<pre><code>[root@localhost ~]# getsebool -a | grep ftpd
ftpd_anon_write --&gt; off
...

[root@localhost ~]# setsebool ftpd_anon_write on
[root@localhost ~]# semanage boolean -l | grep ftpd_anon
ftpd_anon_write                (on   ,  off)  Determine whether ftpd can modify public files used for public file transfer services. Directories/Files must be labeled public_content_rw_t.

[root@localhost ~]# setsebool -P ftpd_anon_write on
[root@localhost ~]# semanage boolean -l | grep ftpd_anon
ftpd_anon_write                (on   ,   on)  Determine whether ftpd can modify public files used for public file transfer services. Directories/Files must be labeled public_content_rw_t.
</code></pre><h2 id="troubleshooting-selinux-policy-violations">Troubleshooting SELinux Policy Violations</h2>
<p>SELinux logs everything it is doing, the primary source to get logging information is the audit log which is in <code>/var/log/audit/audit.log</code>. Message are logged with <code>type=AVC</code>, which stands for <em>Access Vector Cache</em>.</p>
<pre><code>[root@localhost ~]# grep AVC /var/log/audit/audit.log | tail -1
type=AVC msg=audit(1611246770.937:136): avc:  denied  { getattr } for  pid=4178 comm=&quot;httpd&quot; path=&quot;/test/index.html&quot; dev=&quot;dm-0&quot; ino=35157701 scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:default_t:s0 tclass=file permissive=0
</code></pre><p>The first relevant part in the output is the text <code>acv: denied { getattr }</code>. This means some process tried to read the attributes of a file and it was denied access.
Further down we can see <code>comm=&quot;httpd&quot;</code> which means the command that was trying to issue the getattr request was <code>httpd</code>. Next, we see <code>path=&quot;test/index.html&quot;</code>, which is the file that this process tried to access.</p>
<p>In the last part we see information about the source context and the target context:
<code>scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:default_t:s0</code></p>
<blockquote>
<p><code>default_t</code> is used for files that do not match any pattern in the SELinux policy. I created <code>/test/index.html</code> in the root and SELinux doesn&rsquo;t know what security context to give to this file, so it assigned <code>default_t</code>.</p>
</blockquote>
<p>We also see that Permissive mode is disabled:
<code>permissive=0</code></p>
<p>The issue here is that the SELinux policy denies access from the <code>httpd_t</code> security context to the <code>default_t</code> security context.
We can solve this issue by setting the correct target security context:</p>
<pre><code>[root@localhost ~]# semanage fcontext -a -t httpd_sys_content_t &quot;/test(/.*)?&quot;
[root@localhost ~]# restorecon -Rv /test
Relabeled /test from unconfined_u:object_r:default_t:s0 to unconfined_u:object_r:httpd_sys_content_t:s0
Relabeled /test/index.html from unconfined_u:object_r:default_t:s0 to unconfined_u:object_r:httpd_sys_content_t:s0
</code></pre><h3 id="analyzing-selinux-with-sealert">Analyzing SELinux with Sealert</h3>
<p>We can use <code>sealert</code> to easier understand SELinux messages in <code>/var/log/audit/audit.log</code>.
First, you need to install <code>sealert</code>: <code>yum install setroubleshoot-server</code></p>
<p>Once this is installed, issue the <code>journalctl | grep sealert</code> command:</p>
<pre><code>Jan 21 11:32:57 localhost.localdomain setroubleshoot[4395]: SELinux is preventing httpd from getattr access on the file /test/index.html. For complete SELinux messages run: sealert -l e4fc58ab-c1c0-4525-a955-eff9a5570a7c
Jan 21 11:33:00 localhost.localdomain setroubleshoot[4395]: SELinux is preventing httpd from getattr access on the file /test/index.html. For complete SELinux messages run: sealert -l e4fc58ab-c1c0-4525-a955-eff9a5570a7c
</code></pre><p>Follow the instructions and run <code>saelert -l UUID</code>.
<code>sealert</code> will analyze what&rsquo;s happened and provide some suggestions what you need to do to fix the problem. Each suggestion will have a confidence score and the higher this score the more likely the suggested solution would be applicable.</p>
<pre><code>[root@localhost ~]# sealert -l e4fc58ab-c1c0-4525-a955-eff9a5570a7c
SELinux is preventing httpd from getattr access on the file /test/index.html.

*****  Plugin catchall_labels (83.8 confidence) suggests   *******************

If you want to allow httpd to have getattr access on the index.html file
Then you need to change the label on /test/index.html
Do
# semanage fcontext -a -t FILE_TYPE '/test/index.html'
...
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Network Services - Managing Apache HTTP</title>
            <link>https://joerismissaert.dev/network-services-managing-apache-http/</link>
            <pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/network-services-managing-apache-http/</guid>
            <description>Managing Apache HTTP Services is not part of the current RHCSA exam objectives, but we need minimal knowledge on this topic in order to master the SELinux-related objectives later on.
The Apache server is provided through different software packages. The basic packages is httpd which contains everything for an operational but basic website. For a complete overview of all the packages use yum search httpd.
Understanding the httpd Package Let&amp;rsquo;s examine the httpd package by downloading it using yumdownloader and running a few rpm commands on it:</description>
            <content type="html"><![CDATA[<p>
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />



    <img src="/img/apache.png"  alt="Apache logo"  class="center"  />

</p>
<p>Managing Apache HTTP Services is not part of the current RHCSA exam objectives, but we need minimal knowledge on this topic in order to master the SELinux-related objectives later on.</p>
<p>The Apache server is provided through different software packages. The basic packages is <code>httpd</code> which contains everything for an operational but basic website. For a complete overview of all the packages use <code>yum search httpd</code>.</p>
<h1 id="understanding-the-httpd-package">Understanding the httpd Package</h1>
<p>Let&rsquo;s examine the <code>httpd</code> package by downloading it using <code>yumdownloader</code> and running a few rpm commands on it:</p>
<pre><code>[root@localhost ~]# yumdownloader httpd
Last metadata expiration check: 0:00:31 ago on Tue 06 Oct 2020 11:05:52 AM EST.
[root@localhost ~]# ls
anaconda-ks.cfg  httpd-2.4.37-21.module_el8.2.0+382+15b0afa8.x86_64.rpm  initial-setup-ks.cfg
[root@localhost ~]# rpm -qpi httpd-2.4.37-21.module_el8.2.0+382+15b0afa8.x86_64.rpm 
Name        : httpd
Version     : 2.4.37
Release     : 21.module_el8.2.0+382+15b0afa8
Architecture: x86_64
Install Date: (not installed)
Group       : System Environment/Daemons
Size        : 5105105
License     : ASL 2.0
Signature   : RSA/SHA256, Mon 08 Jun 2020 05:08:58 PM EDT, Key ID 05b555b38483c65d
Source RPM  : httpd-2.4.37-21.module_el8.2.0+382+15b0afa8.src.rpm
Build Date  : Mon 08 Jun 2020 04:15:29 PM EDT
Build Host  : x86-02.mbox.centos.org
Relocations : (not relocatable)
Packager    : CentOS Buildsys &lt;bugs@centos.org&gt;
Vendor      : CentOS
URL         : https://httpd.apache.org/
Summary     : Apache HTTP Server
Description :
The Apache HTTP Server is a powerful, efficient, and extensible
web server.
[root@localhost ~]#
</code></pre><p>We can see the package was created by CentOS Buildsys and that it is indeed the Apache HTTP Server package.
Next, let&rsquo;s have a look at the configuration files:</p>
<pre><code>[root@localhost ~]# rpm -qpc httpd-2.4.37-21.module_el8.2.0+382+15b0afa8.x86_64.rpm 
/etc/httpd/conf.d/autoindex.conf
/etc/httpd/conf.d/userdir.conf
/etc/httpd/conf.d/welcome.conf
/etc/httpd/conf.modules.d/00-base.conf
/etc/httpd/conf.modules.d/00-dav.conf
/etc/httpd/conf.modules.d/00-lua.conf
/etc/httpd/conf.modules.d/00-mpm.conf
/etc/httpd/conf.modules.d/00-optional.conf
/etc/httpd/conf.modules.d/00-proxy.conf
/etc/httpd/conf.modules.d/00-systemd.conf
/etc/httpd/conf.modules.d/01-cgi.conf
/etc/httpd/conf/httpd.conf
/etc/httpd/conf/magic
/etc/logrotate.d/httpd
/etc/sysconfig/htcacheclean
[root@localhost ~]# 
</code></pre><p>The main configuration file is <code>/etc/httpd/conf/httpd.conf</code>. The <code>welcome.conf</code> file defines the default home page for your website, until you add content. The <code>magic</code> file defines rules that the server can use to figure out a file&rsquo;s type when the server tries to open it. The <code>/etc/logrotate.d/httpd</code> file defines how log files produced by Apache are rotated.</p>
<p>Most Apache modules put their configuration files into the  <code>/etc/httpd/conf.d</code> directory but some may drop their configuration files into the <code>/etc/httpd/conf.modules.d/</code> directory. Any file in those directories that ends with the <code>.conf</code> extension is included in the main <code>httpd.conf</code> file and used to configure Apache.</p>
<h1 id="setting-up-a-basic-web-server">Setting Up a Basic Web Server</h1>
<p>Let&rsquo;s install the <code>httpd</code> package and some of the most commonly used additional packages using the <code>yum module install httpd</code> command:</p>
<pre><code>[root@localhost ~]# yum module install httpd
...
...
Installed:
  apr-1.6.3-9.el8.x86_64                                                    apr-util-1.6.1-6.el8.x86_64                                          apr-util-bdb-1.6.1-6.el8.x86_64                                  
  apr-util-openssl-1.6.1-6.el8.x86_64                                       centos-logos-httpd-80.5-2.el8.noarch                                 httpd-2.4.37-21.module_el8.2.0+382+15b0afa8.x86_64               
  httpd-filesystem-2.4.37-21.module_el8.2.0+382+15b0afa8.noarch             httpd-tools-2.4.37-21.module_el8.2.0+382+15b0afa8.x86_64             mod_http2-1.11.3-3.module_el8.2.0+307+4d18d695.x86_64            
  mod_ssl-1:2.4.37-21.module_el8.2.0+382+15b0afa8.x86_64                   

Complete!
</code></pre><p>Open the main configuration file, <code>/ect/httpd/conf/httpd.conf</code>, and look for the <code>DocumentRoot</code> parameter.
This parameter specifies the default location where the Apache Web Server looks for content to serve. It should be set to <code>DocumentRoot &quot;/var/www/html&quot;</code>.
In the directory <code>/var/www/html</code>, create a file with the name <code>index.html</code> and the content <code>Welcome To My Web Server!</code>. Next, start and enable the <code>httpd</code> service and check if the service is up and running.</p>
<pre><code>[root@localhost ~]# echo &quot;Welcome To My Webserver!&quot; &gt; /var/www/html/index.html
[root@localhost ~]# 
[root@localhost ~]# systemctl enable --now httpd
Created symlink /etc/systemd/system/multi-user.target.wants/httpd.service → /usr/lib/systemd/system/httpd.service.
[root@localhost ~]# 
[root@localhost ~]# systemctl status httpd
● httpd.service - The Apache HTTP Server
   Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2020-10-06 11:28:24 EST; 3s ago
     Docs: man:httpd.service(8)
 Main PID: 33293 (httpd)
   Status: &quot;Started, listening on: port 443, port 80&quot;
    Tasks: 213 (limit: 11323)
   Memory: 17.8M
   CGroup: /system.slice/httpd.service
           ├─33293 /usr/sbin/httpd -DFOREGROUND
           ├─33296 /usr/sbin/httpd -DFOREGROUND
           ├─33298 /usr/sbin/httpd -DFOREGROUND
           ├─33299 /usr/sbin/httpd -DFOREGROUND
           └─33301 /usr/sbin/httpd -DFOREGROUND

Oct 06 11:28:24 localhost.localdomain systemd[1]: Starting The Apache HTTP Server...
Oct 06 11:28:24 localhost.localdomain httpd[33293]: AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using localhost.localdomain. Set the 'ServerName' directive globally to &gt;
Oct 06 11:28:24 localhost.localdomain systemd[1]: Started The Apache HTTP Server.
Oct 06 11:28:24 localhost.localdomain httpd[33293]: Server configured, listening on: port 443, port 80
</code></pre><p>When the <code>httpd</code> service starts, five <code>httpd</code> daemon processes are launched by default to respond to requests for the web server. You can configure more or fewer daemons to be started based on settings in the main configuration file.</p>
<p>We can verify it&rsquo;s working by making an http request to localhost using <code>curl</code>:</p>
<pre><code>[root@localhost ~]# curl http://localhost
Welcome To My Webserver!
</code></pre><h1 id="creating-apache-virtual-hosts">Creating Apache Virtual Hosts</h1>
<p>Apache supports the creation of separate websites within a single server. Individual sites are configured in what we refer to as <em>virtual hosts</em> which is just a way to have the content for multiple domain names available from the same Apache server. The content that is served to a web client is based on the (domain) name used to access the server.</p>
<p>For example, if a client got to the server by requesting the name <code>www.example.org</code>, he would be redirected to a virtual host container that has its <code>ServerName</code> parameter set to <code>www.example.org</code>.</p>
<p>Name-based virtual hosting is the most common solution where virtual hosts use different names but the same IP address.
IP-based virtual hosts are less common but is required if the name of a web server must resolve to a unique IP address. This solution requires multiple IP addresses on the same machine.</p>
<p>In this section we&rsquo;ll be setting up name-based virtual hosts.</p>
<blockquote>
<p>If your Apache server is configured for virtual hosts, all sites it&rsquo;s hosting should be handled by virtual hosts. If someone accesses the server via IP address or a name that is not set in a virtual host then the first virtual host is used as the default location to serve up content.<br>
You can create a catch-all entry for those requests by creating a virtual host for <code>_default:80</code>.</p>
</blockquote>
<p>Create a file named <code>example.org.conf</code> in <code>/etc/httpd/conf.d/</code> using the following template:</p>
<pre><code>&lt;VirtualHost *:80&gt;

	ServerAdmin	webmaster@example.org
	ServerName	example.org
	ServerAlias www.example.org
	DocumentRoot /var/www/html/example.org/

DirectoryIndex index.php index.html index.htm
&lt;/VirtualHost&gt;
</code></pre><p>This example includes the following settings:</p>
<ul>
<li>The <code>*:80</code> specification indicates to what address and port this virtual host applies. If your machine has multiple IP addresses, you can replace the <code>*</code> with an IP. The port is optional but should always be used to prevent interference with SSL virtual hosts (which use port 443).</li>
<li>The <code>ServerName</code> and <code>ServerAlias</code> lines tell Apache which names this virtual host should be recognized as. You can either leave out <code>ServerAlias</code> or specify more than one name on the same line, space separated.</li>
<li>The <code>DocumentRoot</code> specifies where the content for this virtual host is stored.</li>
<li>The <code>DirectoryIndex</code> directive sets the list of files to look for and serve when the web server receives a request.</li>
</ul>
<p>Create the <code>index.html</code> file inside the <code>DocumentRoot</code> with the following content: <code>Welcome To Example.org</code></p>
<pre><code>[root@localhost conf.d]# mkdir /var/www/html/example.org
[root@localhost conf.d]# echo &quot;Welcome To Example.org&quot; &gt; /var/www/html/example.org/index.html
</code></pre><p>Create a second virtual host with different values, e.g.:</p>
<pre><code>[root@localhost conf.d]# cat foobar.com.conf 
&lt;VirtualHost *:80&gt;

	ServerAdmin	webmaster@foobar.com
	ServerName	foobar.com
	ServerAlias	www.foobar.com
	DocumentRoot 	/var/www/html/foobar.com/

DirectoryIndex index.php index.html index.htm
&lt;/VirtualHost&gt;

[root@localhost conf.d]# mkdir /var/www/html/foobar.com
[root@localhost conf.d]# echo &quot;Welcome to Foobar.com!&quot; &gt; /var/www/html/foobar.com/index.html
[root@localhost conf.d]#
</code></pre><p>Next we want to make sure that the domains used in our virtual hosts resolve to our local machine and not to the internet. Edit your hosts file and add the domains to the line that starts with the local loopback address:</p>
<pre><code>[root@localhost conf.d]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4 foobar.com www.foobar.com example.org www.example.org
</code></pre><p>After we restarted the <code>httpd</code> service, we can test if our setup is working correctly:</p>
<pre><code>[root@localhost conf.d]# systemctl restart httpd
[root@localhost conf.d]# curl http://foobar.com
Welcome to Foobar.com!
[root@localhost conf.d]# curl http://example.org
Welcome To Example.org
</code></pre><p>This covered some Apache basics which we will need for testing advanced topics like firewall configuration and SELinux.</p>
]]></content>
        </item>
        
        <item>
            <title>Network Services - Configuring SSH</title>
            <link>https://joerismissaert.dev/network-services-configuring-ssh/</link>
            <pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/network-services-configuring-ssh/</guid>
            <description>Hardening the SSH Server SSH is a convenient and important solution to establish remote connections to servers. If your SSH server is visible directly from the internet, you can be sure that sooner or later intruders will try to connect to it, intending to do harm.
Dictionary attacks are common against an SSH server. SSH servers usually offer their services on port 22, and every Linux servers has a root account.</description>
            <content type="html"><![CDATA[
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />


<h1 id="hardening-the-ssh-server">Hardening the SSH Server</h1>
<p>SSH is a convenient and important solution to establish remote connections to servers. If your SSH server is visible directly from the internet, you can be sure that sooner or later intruders will try to connect to it, intending to do harm.</p>
<p>Dictionary attacks are common against an SSH server. SSH servers usually offer their services on port 22, and every Linux servers has a <code>root</code> account. Based on this information it&rsquo;s easy for an attacker to try to log in as <code>root</code> by guessing the password if the password has limited complexity and no additional security measures are in place. Sooner or later the intruder will be able to connect.</p>
<p>We can protect ourselves against these kind of attacks by:</p>
<ul>
<li>Disabling root login.</li>
<li>Disabling password login and using key-based authentication.</li>
<li>Configuring a non default port for SSH to listen on.</li>
<li>Allowing only specific users to log in on SSH.</li>
</ul>
<h2 id="limiting-root-access">Limiting Root access</h2>
<p>SSH servers have <code>root</code> login enabled by default, which is a big security concern. Disabling <code>root</code> login is easy: Modify the <code>PermitRootLogin</code> parameter in <code>/etc/ssh/sshd_config</code> and reload or restart the service:</p>
<pre><code># Authentication:

#LoginGraceTime 2m
PermitRootLogin no
#StrictModes yes
#MaxAuthTries 6
#MaxSessions 10
</code></pre><h2 id="configuring-alternative-ports">Configuring Alternative Ports</h2>
<p>Security problems on Linux servers start with a port scan issued by an attacker. There are 65,535 ports that can potentially be listening, and scanning all those ports takes a lot of time so most port scans focus on well known ports only. Port 22 is always among these ports.</p>
<p>To protect against port scans we can configure the SSH server to listen on another port. You can choose a completely random port, as long as the port is not already in use by another service.</p>
<pre><code># If you want to change the port on a SELinux system, you have to tell
# SELinux about this change.
# semanage port -a -t ssh_port_t -p tcp #PORTNUMBER
#
Port 39860
#AddressFamily any
#ListenAddress 0.0.0.0
#ListenAddress ::
</code></pre><blockquote>
<p>To avoid being locked out of the server after making changes to the SSH listing port, it&rsquo;s a good idea to open two sessions. Use one session to apply the port change and test, use the other sessions to keep your current connection open. Active sessions will not be disconnected after restarting the SSH server (unless the restart fails), so if something is wrong with the configuration and you&rsquo;re not longer able to connect you still have the second session to fix the problem.</p>
</blockquote>
<h2 id="modifying-selinux-to-allow-for-port-changes">Modifying SELinux to Allow for Port Changes</h2>
<p>After changing the SSH port you also need to configure SELinux to allow this change. Network ports are labeled with SELinux security labels to prevent services from accessing ports they shouldn&rsquo;t.</p>
<p>Use the <em><strong>semanage port</strong></em> command to change the label on the target port. Before doing so, it&rsquo;s a good idea to check if the port already has a label: <em><strong>semanage port -l</strong></em>, e.g. <code>semanage port -l | grep ssh</code></p>
<p>If the port doesn&rsquo;t have a label, use the <em><strong>-a</strong></em> option to add a label, if it does have a label use <em><strong>-m</strong></em> to modify the current security label.</p>
<p><code>semanage port -a -t ssh_port_t -p tcp 39860</code><br>
<code>semanage port -m -t ssh_port_t -p tcp 443</code></p>
<h2 id="limiting-user-access">Limiting User Access</h2>
<p>The <code>AllowUsers</code> option takes a space separated list of usernames that will be allowed to login through SSH. If the user <code>root</code> still needs to be able to log in you&rsquo;ll have to include it as well in the list.</p>
<blockquote>
<p>This option does <em>not</em> appear anywhere in the <code>/etc/ssh/sshd_config</code> file by default.</p>
</blockquote>
<p>Another interesting option is <code>MaxAuthTries</code>. It specifies the maximum number of authentication attempts permitted per connection.<code>MaxAuthTries</code> is also useful for analyzing security events, it logs failed login attempts once the number of failures reaches half this value. The higher the number of attempts, the more likely it is an intruder is trying to get in.<br>
SSH writes log information about failed login attempts to the AUTHPRIV syslog facility. This facility is by default configured to write information to <code>/var/log/secure</code>.</p>
<h1 id="other-useful-sshd-options">Other Useful sshd Options</h1>
<p>Apart from security-related options, there are some useful miscellaneous options you can use to streamline performance.</p>
<h2 id="session-options">Session Options</h2>
<p>On RHEL8, <code>GSSAPIAuthentication</code> option is set to <em><strong>yes</strong></em> by default. This option is only useful in an environment where Kerberos authentication is used. Having this feature on slows down the authentication procedure.</p>
<p>The <code>UseDNS</code> option is also enabled by default and instructs the SSH server to lookup the remote hostname and check with DNS that the hostname maps back to the same IP address (Reverse DNS Lookup). Although this option has some security benefits, it also involves a significant performance penalty. Set this to <code>no</code> if client connections are slow.</p>
<blockquote>
<p>To give an example on Reverse DNS lookups, assume you&rsquo;re connecting from a client with the <code>8.8.8.8</code> ip address. The SSH server will lookup the PTR record for the <code>8.8.8.8.in-addr.arpa</code> domain which would result in <code>dns.google</code>. In turn, this result resolves back to <code>8.8.8.8</code>.  The reverse DNS database of the Internet is rooted in the <code>.arpa</code> top-level domain.</p>
</blockquote>
<pre><code>$ dig -x 8.8.8.8
;; ANSWER SECTION:
8.8.8.8.in-addr.arpa.	76082	IN	PTR	dns.google.

$ dig dns.google
;; ANSWER SECTION:
dns.google.		824	IN	A	8.8.4.4
dns.google.		824	IN	A	8.8.8.8 

</code></pre><p>The <code>MaxSessions</code> option specifies the maximum number of sessions that can be opened from one IP address simultaneously. You might need to increase this option beyond the default value of 10.</p>
<h2 id="connection-keepalive-options">Connection Keepalive Options</h2>
<p>The <code>TCPKeepAlive</code> option is used to monitor whether the client is still available.
This option is by default enabled and sends a keepalive probe packet with the ACK flag to the client after a certain amount of time. If a reply is received, the SSH server can assume that the connection is still up and running.</p>
<p>The <code>ClientAliveInterval</code> option sets an interval in seconds after which the server sends a packet to the client if no activity has been detected. The <code>ClientAliveCountMax</code> parameter specifies how many of these should be sent. So if the <code>ClientAliveInterval</code> is set to <code>30</code> and the <code>ClientAliveCountMax</code> to <code>10</code>, inactive connections are kept alive for about 5 minutes.</p>
<blockquote>
<p>The equivalent client side options are <code>ServerAliveInterval</code> and <code>ServerAliveCountMax</code>, useful if you cannot change the configuration of the SSH server.</p>
</blockquote>
<h1 id="configuring-key-based-authentication-with-passphrases">Configuring Key-Based Authentication with Passphrases</h1>
<p>By default, password authentication is allowed on RHEL 8 SSH servers. You can disable password authentication and allow public/private key-based authentication only by setting the <code>PasswordAuthentication</code> option to <code>no</code>.</p>
<p>When using key-based authentication you can set a passphrase which makes the key pair stronger. In case an intruder has access to the private key he would also need to know the passphrase before being able to use the key.</p>
<p>Without further configuration the use of passphrases would mean that users have to enter the passphrase every time before a connection can be created, which is inconvenient. To work around this we can cache the passphrase for a session:</p>
<ul>
<li>Execute the <code>ssh-agent /bin/bash</code> command to start the agent for the current (Bash) shell.</li>
<li>Execute <code>ssh-add</code> to add the passphrase for the current user&rsquo;s private key. The key is now cached.</li>
<li>Connect to the remote server, you&rsquo;ll notice you do not need to enter the passphrase.</li>
</ul>
<h1 id="copying-and-synchronizing-files-securely-over-ssh">Copying and synchronizing files securely over SSH</h1>
<p><code>scp</code> is a program for copying files securely between computers using the SSH protocol.<br>
The basic usage is as follows:</p>
<ul>
<li>To copy a local file to a remote host:  <code>scp localfile remote_host:remote_path</code></li>
<li>To copy a remote file to a local path: <code>scp remote_host:remote_file localpath</code></li>
<li>To copy entire directory trees, add the <code>-r</code> option: <code>scp -r remote_host:path/directory .</code></li>
</ul>
<p>Rsync, which stands for “remote sync”, is a remote and local file synchronization tool. It uses an algorithm that minimizes the amount of data copied by only moving the portions of files that have changed. The basic syntax is similar to that of <code>scp</code>: <code>rsync source destination</code>.</p>
<ul>
<li><code>rsync -anvzP --progress remote_host:/path/to/directory/ /some/local/path</code></li>
</ul>
<p>The <code>-a</code> option is a combination flag, it stands for &ldquo;archive&rdquo; and syncs recursively and preserves symbolic links, special and device files, modification times, group, owner, and permissions. You could use <code>-r</code> to only sync recursively instead. <br>
The <code>-n</code> flag is the same as the <code>--dry-run</code> option and allows you to check results before actually running the synchronization. You need the <code>-v</code> flag (verbose) to get the appropriate output to verify.<br>
The <code>-z</code> option can reduce network transfer by adding compression.<br>
The <code>-P</code> flag combines the <code>--progress</code> and <code>--partial</code> options, it gives you a progress bar and allows you to resume interrupted transfers.<br>
Finally, you can use the <code>-A</code> flag to preserve Access Control Lists, and the <code>-X</code> flag to preserve SELinux context labels.</p>
<blockquote>
<p>Notice the traling slash <code>/</code> at the end of the first argument in the example command. This is necessary to include the contents of the source path. Without the trailing slash, <code>directory</code> would be created inside <code>/some/local/path</code>.</p>
</blockquote>
]]></content>
        </item>
        
        <item>
            <title>Introduction to Bash Shell Scripting</title>
            <link>https://joerismissaert.dev/introduction-to-bash-shell-scripting/</link>
            <pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/introduction-to-bash-shell-scripting/</guid>
            <description>Core Elements A shell script is a list of sequentially executed commands with optional scripting logic to allow code to be executed under specific conditions only. Starting a script from the parent shell opens a subshell from where the commands in the script are executed. These commands can be interpreted in different ways, to make it clear how they should be interpreted the shebang is used on the first line of the script: #!</description>
            <content type="html"><![CDATA[<p>
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />



    <img src="/img/bash_logo.png"  alt="Bash logo"  class="center"  style="width:150px"  />

</p>
<h1 id="core-elements">Core Elements</h1>
<p>A shell script is a list of sequentially executed commands with optional scripting logic to allow code to be executed under specific conditions only. Starting a script from the parent shell opens a subshell from where the commands in the script are executed. These commands can be interpreted in different ways, to make it clear how they should be interpreted the <em>shebang</em> is used on the first line of the script: <code>#!/bin/bash</code>, which would call and execute the script in a bash subshell.</p>
<p>The below script asks you for a path and stores the path in the <code>DIR</code> variable, then changes directory to the <code>DIR</code> value and prints the current working directory.</p>
<pre><code>#!/bin/bash
# MyComment

echo Provide a path to a directory:
read DIR
cd $DIR
pwd
exit 0
</code></pre><p>When you execute this script, notice how your current working directory hasn&rsquo;t changed after the script has executed. This is because the script executes in a subshell of the parent shell from where you invoked the script.</p>
<p>At the end of the above script an <code>exit 0</code> statement is included. An exit statement tells the parent script whether the scipt was successful, a <code>0</code> means it was successfull, while anything else means a problem was encountered.</p>
<p>A script needs to be executable. The most common way to make a script executable is by applying the execute permission to it. The script can also be executed as an argument to the <code>bash</code> command, e.g. <code>bash myscript.sh</code>.</p>
<p>You can store a script anywhere you like, but if it&rsquo;s stored outide of the <code>$PATH</code> you need to execute it with a <code>./</code> in front: <code>./myscript.sh</code>.</p>
<h1 id="variables-and-input">Variables and Input</h1>
<p>Scripts typically aren&rsquo;t a list of sequential commands, they can work with variables and input to be more flexible.</p>
<h2 id="positional-parameters">Positional Parameters</h2>
<p>When starting a script, an argument can be used. Arguments are anything you put behind the command while starting the script, e.g. <code>useradd lisa</code> where the command is <code>useradd</code> and the argument is <code>lisa</code>. In a script, the first variable is referred to as <code>$1</code>, the second as <code>$2</code> and so on.</p>
<pre><code>#!/bin/bash
# Run this script with a few arguments

echo The first argument is $1
echo The 2nd argument is $2
echo The 3rd argument is $3
</code></pre><p>Run the above script with a few arguments, and it will make sense:
<code>./script 1 2 3 4</code>
You&rsquo;ll notice the 4th argument, being <code>4</code> isn&rsquo;t echoed. We can work around that by making the script more flexible using a conditional <code>for</code> loop instead of echoeing each argument one after the other:</p>
<pre><code>#!/bin/bash
# Run this script with a few arguments

echo You have entered $# arguments.

for i in &quot;$@&quot;
  do echo $i
done

exit 0
</code></pre><p><code>$#</code> is a counter that shows how many arguments were used when starting the script.<br>
<code>$@</code> refers to all arguments used when starting the script.
In the above script, the condition is <code>for i in &quot;$@&quot;</code>, which means &ldquo;for each argument in the list of arguments&rdquo;. I&rsquo;ll cover more on <code>for</code> loops later, but what this script basically does is loop through the list of arguments (<code>$@</code>) and echo each one (<code>do echo $i</code>).</p>
<h2 id="variables">Variables</h2>
<p>Variables are labels that refer to a specific location in memory which contains a specific value. They can be defined statically or dynamically. Variables are defined by using the <code>=</code> sign directly after the uppercase name, followed by the value. You should never use spaces when defining variables:<br>
<code>MYVAR=value</code>, this would be a statically defined variable.</p>
<p>There are two solutions for defining variables dynamically:</p>
<ul>
<li>Using <code>read</code> in the script to ask the user for input. IT stops the script so input can be processed and stored in a variable:</li>
</ul>
<pre><code>[joeri@Ryzen7 ~]$ read NAME
joeri
[joeri@Ryzen7 ~]$ echo $NAME
joeri
</code></pre><ul>
<li>Using command substitution where you assign the result of a specific command to a variable. For example: <code>TODAY=$(date +%d-%m-%y)</code>.<br>
You enclose the command whose result you want to use between parentheses and preceed that with a <code>$</code> sign.</li>
</ul>
<pre><code>[joeri@Ryzen7 ~]$ TODAY=$(date +%d-%m-%y)
[joeri@Ryzen7 ~]$ echo $TODAY
31-10-20
</code></pre><h1 id="conditional-loops">Conditional Loops</h1>
<p>Conditional loops are executed only if a certain condition is true. I&rsquo;ll cover the most often used conditional loops in this section.</p>
<h2 id="if--then--else">if &hellip; then &hellip; else</h2>
<p>This construction is common to evaluate specific conditions and are often used together with the <code>test</code> command. Have a look at the man page of <code>test</code> for a complete overview of all the functionality.</p>
<p>Let&rsquo;s look at an example:</p>
<pre><code>#!/bin/bash
# MyComment

if [ -z $1 ]
then
  echo No value provided
fi
</code></pre><p>The  <code>-z</code> test command checks if the length of a string is zero (<code>man test</code>). If that is true, then &ldquo;No value provided&rdquo; will be echoed to the screen.
The above script will only provide output if you run it without any argument.</p>
<p>Below is another example using multiple test commands:</p>
<pre><code>#!/bin/bash
# Run this script with one argument.
# Find out if the argument is a file or a directory

if [ -f $1 ]
then
  echo &quot;$1 is a file&quot;
elif [ -d $1 ]
then
  echo &quot;$1 is a directory&quot;
else
  echo &quot;Not sure what $1 is....&quot;
fi

exit 0
</code></pre><h2 id="-and-">|| and &amp;&amp;</h2>
<p>Instead of writing full <code>if ... then</code> statements we can use logical operators. <code>||</code> is a logical OR and will execute the second part of the statement only if the first part is <em>not</em> true. <code>&amp;&amp;</code> is a logical AND, and will execute the second part of the statement only if the first part <em>is</em> true.
&ldquo;true&rdquo; is the state where a command exits with a <code>0</code>.</p>
<pre><code>[ -z $1 ] &amp;&amp; echo no argument provided
ping -c 1 192.168.1.256 || echo node does not exist
</code></pre><h2 id="for--do--done">For &hellip; do &hellip; done</h2>
<p>The <code>for</code> conditional loop provides a solution for processing ranges of data. It always starts with <code>for</code> followed by the condition, then <code>do</code> followed by the commands to be executed when the condition is true, and finally closed with <code>done</code>.</p>
<p>In the below example the COUNTER variable is initialized with a value of 10, if the value is greater than or equal to 0 we substract 1. As long as this condition is true we then echo the value of COUNTER:</p>
<pre><code>#!/bin/bash
#

for (( COUNTER=10; COUNTER&gt;=0; COUNTER--))
do
  echo $COUNTER
done
exit 0

</code></pre><p>We can also define a range by specifying the first number followed by two dots and closing with the last number in the range:</p>
<pre><code>[joeri@Ryzen7 ~]$ for i in {85..90}; do ping -c 1 192.168.100.$i &gt;/dev/null &amp;&amp; echo 192.168.100.$i is UP; done
192.168.100.88 is UP
</code></pre><p>With <code>for i in</code> each of the numbers in the range is assigned to the variable <code>i</code>. For each of those values the <code>ping -c 1</code> command is executed, and output is redirected to <code>/dev/null</code> since we don&rsquo;t need it. Based on the exit status of the <code>ping</code> command, <code>exit 0</code> or <code>true</code>, the part behind the logical operator <code>&amp;&amp;</code> is executed.</p>
<h2 id="while-and-until">While and until</h2>
<p>The <code>while</code> statement is useful if you want to do something as long as a condition is true. Its counterpart is <code>until</code> which keeps the iteration open as long as the condition is false, or until the condition is true.</p>
<p>The below script initializes the COUNTER value with a value of 0 and <em>while</em> the value is <em>less than</em> 11 we echo the value and increase the value with 1:</p>
<pre><code>#!/bin/bash
#

COUNTER=0

while [ $COUNTER -lt 11 ]; do
  echo The counter is $COUNTER
  (( COUNTER=COUNTER+1 ))
done
</code></pre><p>Below we echo the value of COUNTER and increase its value with 1 <em>until</em> the value is equal to 11. At that point we break out of the loop:</p>
<pre><code>#!/bin/bash
#

COUNTER=0

until [ $COUNTER = 11 ]; do
  echo The counter is $COUNTER
  (( COUNTER=COUNTER+1 ))
done
</code></pre><h2 id="case">Case</h2>
<p>The <code>case</code> statement is used to evaluate a number of expected values, you define very specific argument that you expect followed by the command that needs to be executed if that argument was used.</p>
<p>The generic syntax is <code>case item-to-evaluate in</code>, followed by a list of all possible values that need to be evaluated. Each item is closed with a <code>)</code>. Then follows a list of commands that are executed if the specific argument was used, the commands are closed with a double semicolon, <code>;;</code>.</p>
<p>The evaluations in <code>case</code> are performed in order. Then the first match is made, the <code>case</code> statement will not evaluate anything else. Whitin the evaluaten, wildcard-like patterns can be used. For example <code>*)</code>, which is a &ldquo;catchall&rdquo; statement.</p>
<pre><code>#!/bin/bash

echo -n &quot;Enter the name of a country: &quot;
read COUNTRY

echo -n &quot;The official language of $COUNTRY is &quot;

case $COUNTRY in

  Lithuania)
    echo -n &quot;Lithuanian&quot;
    ;;

  Romania | Moldova)
    echo -n &quot;Romanian&quot;
    ;;

  Italy | &quot;San Marino&quot; | Switzerland | &quot;Vatican City&quot;)
    echo -n &quot;Italian&quot;
    ;;

  *)
    echo -n &quot;unknown&quot;
    ;;
esac
</code></pre><h1 id="script-debugging">Script debugging</h1>
<p>If a script does not do what you expect it to do, try starting it as an argument to the <code>bash -x</code> command. This will show you line by line what the script is trying to do and will show specific errors if it does not work.</p>
<pre><code>[joeri@Ryzen7 ~]$ bash -x lang.sh 
+ echo -n 'Enter the name of a country: '
Enter the name of a country: + read COUNTRY
Germany
+ echo -n 'The official language of Germany is '
The official language of Germany is + case $COUNTRY in
+ echo -n unknown
unknown
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Troubleshooting Boot Issues</title>
            <link>https://joerismissaert.dev/troubleshooting-boot-issues/</link>
            <pubDate>Fri, 18 Sep 2020 00:00:00 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/troubleshooting-boot-issues/</guid>
            <description>The RHEL8 Boot Procedure In order to fix boot issues we need to be able to judge in which phase of the boot procedure the issue occurs so we can apply appropriate means to fix it. The following steps summarize the boot procedure:
 POST - The machine is powered on, the Power-On-Self-Test executes and hardware required to start the system is initialized. Boot device selection - From UEFI or BIOS, a bootable device is located.</description>
            <content type="html"><![CDATA[
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />


<h2 id="the-rhel8-boot-procedure">The RHEL8 Boot Procedure</h2>
<p>In order to fix boot issues we need to be able to judge in which phase of the boot procedure the issue occurs so we can apply appropriate means to fix it.
The following steps summarize the boot procedure:</p>
<ul>
<li><strong>POST</strong> - The machine is powered on, the Power-On-Self-Test executes and hardware required to start the system is initialized.</li>
<li><strong>Boot device selection</strong> - From UEFI or BIOS, a bootable device is located.</li>
<li><strong>Loading the boot loader</strong> - From the bootable device a boot loader is located.</li>
<li><strong>Loading the kernel</strong> - The kernel is loaded together with the initramfs. The initramfs contains kernel modules required to boot as well as initials scripts to proceed to the next stage of booting.</li>
<li><strong>Starting /sbin/init</strong> - The first process is loaded, <code>/sbin/init</code>, which is a symlink to Systemd. The udev daemon is loaded to take care of further hardware initialization. This all happens from initramfs.</li>
<li><strong>Process initrd.target</strong> - The Systemd process executes all units from the initrd.target, preparing a minimal operating environment from where the root file system on disk is mounted onto the <code>/sysroot</code> directory.</li>
<li><strong>Switch to root file system</strong> - The system switches to the root file system on disk and loads the Systemd process from disk.</li>
<li><strong>Running the default target</strong> - Systemd looks for the default target to execute and runs all of its units.</li>
</ul>
<p>The below table summarizes where a specific phase is configured and what you can do to troubleshoot if something goes wrong.</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Phase</th>
<th>Configuration</th>
<th>Fix</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>POST</strong></td>
<td>Hardware Configuration, BIOS, UEFI</td>
<td>Replace Hardware</td>
</tr>
<tr>
<td><strong>Boot Device</strong></td>
<td>BIOS/UEFI configuration or boot menu</td>
<td>Replace hardware or use rescue system</td>
</tr>
<tr>
<td><strong>Boot Loader</strong></td>
<td><code>grub2-install</code> and edits to <code>/etc/defaults/grub</code></td>
<td>GRUB Boot menu, edits to <code>/etc/defaults/grub</code> followed by <code>grub2-mkconfig</code></td>
</tr>
<tr>
<td><strong>Kernel</strong></td>
<td>Edits to GRUB config and <code>/etc/dracut.conf</code></td>
<td>GRUB Boot menu, edits to <code>/etc/defaults/grub</code> followed by <code>grub2-mkconfig</code></td>
</tr>
<tr>
<td><strong>/sbin/init</strong></td>
<td>Compiled into initramfs</td>
<td><strong>init=</strong> kernel boot argument, <strong>rd.break</strong> kernel boot argument, recreate initramfs</td>
</tr>
<tr>
<td><strong>initrd.target</strong></td>
<td>Compiled into initramfs</td>
<td>recreate initramfs</td>
</tr>
<tr>
<td><strong>Root file system</strong></td>
<td>Edits to <code>/etc/fstab</code></td>
<td>Edits to <code>/etc/fstab</code></td>
</tr>
<tr>
<td><strong>Default Target</strong></td>
<td><code>systemctl set-default</code></td>
<td>Start rescue.target as a kernel boot argument</td>
</tr>
</tbody>
</table>

<h2 id="passing-kernel-boot-arguments">Passing Kernel Boot Arguments</h2>
<p>The GRUB boot prompt offers a way to stop the boot procedure and pass specific options to the kernel.
When you see the GRUB2 menu, type <strong>e</strong> to enter a mode where you can edit commands and scroll down to the section that begins with <code>linux ($root)/vmlinuz</code>. This line tells GRUB how to start a kernel and looks similar to this:</p>
<pre><code>linux ($root)/vmlinuz-4.18.0-193.19.1.el8_2.x86_64 root=/dev/mapper/cl-root ro crash kernel-auto resume=/dev/mapper/cl-swap rd.lvm.lv=cl/root rd.lvm.lv=cl/swap rhgb quiet
</code></pre><p>Additional boot arguments need to be added to the end of this line.</p>
<p>The <strong>rhgb</strong> and <strong>quiet</strong> boot options hide boot messages, we can remove these in order to see what&rsquo;s happening when we boot the machine.
Once you made the necessary changes, press <code>CTRL+X</code> to start the kernel. Note that this change is not persistent, to make them persistent we must modify the content of <code>/etc/default/grub</code> and use <strong>grub2-mkconfig -o /boot/grub2/grub.cf</strong> to apply the change.</p>
<h2 id="starting-a-troubleshooting-target">Starting a Troubleshooting Target</h2>
<p>In the GRUB boot prompt we can use several options to allow us to fix our issue:</p>
<ul>
<li><strong>rd.break</strong> - Stops the boot procedure in the initramfs phase. This option is useful if you don&rsquo;t have the root password.</li>
<li><strong>init=/sbin/bash</strong> - A shell will be started immediately after loading the kernel and initrd.target.</li>
<li><strong>systemd.unit=emergency.target</strong> - Enters a mode that loads the bare minimum of required Systemd units, it requires a root password.</li>
<li><strong>systemd.unit=rescue.target</strong> - Starts more Systemd units to bring up a more complete operational mode.</li>
</ul>
<h2 id="using-a-rescue-disk">Using a Rescue Disk</h2>
<p>The default rescue image for RHEL is on the installation disk. When booting from the installation disk you&rsquo;ll see a <code>Troubleshooting</code> menu item which presents you with the following options:</p>
<ul>
<li><strong>Install RHEL in Basic Graphics Mode</strong> - This option reinstalls the machine. You should not use it unless a normal installation does not work and you need basic graphics mode.</li>
<li><strong>Rescue a RHEL System</strong> - This options prompts you to press Enter to start the installation, but only loads a rescue system. It does not overwrite the current configuration. The Rescue System will try to find an installed Linux system and mount it on <code>/mnt/sysimage</code>. If a valid installation was found and mounted you can press Enter twice to access the rescue shell. At this point we can switch to the root file system on disk to access all tools we need to repair the system: <code>chroot /mnt/sysimage</code></li>
<li><strong>Run a Memory Test</strong> - If you encounter memory errors this tool allows you to mark bad memory chips so you can boot your machine normally.</li>
<li><strong>Boot from Local Drive</strong> - If you cannot boot from GRUB on your usual boot device try this option. It offers a boot loader that will try to load the OS from your hard disk.</li>
</ul>
<h3 id="reinstalling-grub-using-a-rescue-disk">Reinstalling Grub Using a Rescue Disk</h3>
<p>One of the most common reasons to start a rescue disk is if the GRUB2 boot loader breaks. Once you have access to your machine using the rescue disk, reinstalling GRUB2 is a two step process:</p>
<ul>
<li>Make sure you switch to the root file system on disk: <code>chroot /mnt/sysroot</code></li>
<li>Use <strong>grub2-install</strong> followed by the name of the device on which you want to reinstall GRUB2, i.e. <code>grub2-install /dev/sda</code></li>
</ul>
<h3 id="recreating-initramfs-using-a-rescue-disk">Recreating Initramfs Using a Rescue Disk</h3>
<p>You know there is a problem with initramfs when you never see the root file system getting mounted on the root directory and don&rsquo;t see any Systemd unit files being started when analyzing the boot procedure.</p>
<p>To repair the initramfs image after booting into the rescue environment you can use the <strong>dracut</strong> command. <strong>dracut &ndash;force</strong> overwrites the existing initramfs and creates a new initramfs image for the currently loaded kernel. There is also the <code>/etc/dracut.conf</code> configuration file you can use to include specific options while re-creating initramfs. The <strong>dracut</strong> configuration itself is dispersed over several locations:</p>
<ul>
<li><code>/usr/lib/dracut/dracut.conf.d/</code> - Contains the system default configuration files</li>
<li><code>/etc/dracut.conf.d/</code> - Contains custom dracut configuration files</li>
<li><code>/etc/dracut.conf</code> - The master configuration file</li>
</ul>
<h2 id="recovering-from-file-system-issues">Recovering from File System Issues</h2>
<p>When there is a misconfiguration in the file system mounts the boot procedure may end with the &ldquo;Give root password for maintenance&rdquo; message. If a device does not exist or there&rsquo;s an error in the UUID, for example, Systemd waits to see if the device comes back online by itself. When that doesn&rsquo;t happen, the &ldquo;Give root password for maintenance&rdquo; message appears.</p>
<p>After entering the root password, issue the <strong>journalctl -xb</strong> command to see if relevant messages providing information about what is wrong are written to the journal. If the problem is indeed file system oriented we need to make sure the root file system is mountend with read/write rights, analyze what&rsquo;s wrong in <code>/etc/fstab</code> and fix that: <code>mount -o remount,rw /</code></p>
<h2 id="resetting-the-root-password">Resetting the Root Password</h2>
<p>When the root password is lost, the only way to reset it is to boot into minimal mode which allows you to login without using a password:</p>
<ul>
<li>Pass the <strong>rd.break</strong> boot argument to the kernel</li>
<li>Boot the system</li>
<li>The boot procedure stops after loading initramfs and before mounting the root file system.</li>
<li>Re-mount the root file system on disk to get read/write access to the system image: <code>mount -o remount,rw /sysroot</code></li>
<li>Make the contents of the <code>/sysroot</code> directory the new root directory: <code>chroot /sysroot</code></li>
<li>Use the <strong>passwd</strong>  command to set the new password.</li>
<li>Load the SELinux policy: <code>load_policy -i</code></li>
<li>Set the correct SELinux contect type to <code>/etc/shadow</code>: <code>chcon -t shadow_t /etc/shadow</code></li>
<li>Reboot by issuing the <strong>exit</strong> command twice. Use the new root password at the next boot.</li>
</ul>
<blockquote>
<p>An alternative to applying the SELinux context to <code>/etc/shadow</code> is to create the <code>/.autorelabel</code> file which forces SELinux to restore labels set on the entire file system the next time the system is booted.</p>
</blockquote>
]]></content>
        </item>
        
        <item>
            <title>Managing Systemd Targets and Working with GRUB2</title>
            <link>https://joerismissaert.dev/managing-systemd-targets-working-with-grub2/</link>
            <pubDate>Fri, 11 Sep 2020 00:00:00 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/managing-systemd-targets-working-with-grub2/</guid>
            <description>Managing Systemd Targets A Systemd target is a group of units belonging together, some of these targets can be used to define the state a system should boot in. These targets can be isolated and have the AllowIsolate property in their [Unit] section.
Four targets can be used to boot into:
 emergency.target : A minimal number of units are started. rescue.target : A fully operation Linux system without nonessential services.</description>
            <content type="html"><![CDATA[
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />


<h2 id="managing-systemd-targets">Managing Systemd Targets</h2>
<p>A Systemd target is a group of units belonging together, some of these targets can be used to define the state a system should boot in. These targets can be isolated and have the <code>AllowIsolate</code> property in their <code>[Unit]</code> section.<br>
Four targets can be used to boot into:</p>
<ul>
<li><strong>emergency.target</strong> : A minimal number of units are started.</li>
<li><strong>rescue.target</strong> : A fully operation Linux system without nonessential services.</li>
<li><strong>multi-user.target</strong> : The default target commonly used on servers, starts everything needed for full system functionality.</li>
<li><strong>graphical.target</strong> : Starts all units needed for full system functionality as well as a graphical interface.</li>
</ul>
<p>A target configuration consists of two parts, the target unit file and the &ldquo;wants&rdquo; directory that contains references to all unit files that need to be loaded when entering that specific target. They can also have other targets as dependencies, specified in the target unit file.</p>
<pre><code>[root@server1 ~]# systemctl cat multi-user.target 
# /usr/lib/systemd/system/multi-user.target
#  SPDX-License-Identifier: LGPL-2.1+
#
#  This file is part of systemd.
#
#  systemd is free software; you can redistribute it and/or modify it
#  under the terms of the GNU Lesser General Public License as published by
#  the Free Software Foundation; either version 2.1 of the License, or
#  (at your option) any later version.

[Unit]
Description=Multi-User System
Documentation=man:systemd.special(7)
Requires=basic.target
Conflicts=rescue.service rescue.target
After=basic.target rescue.service rescue.target
AllowIsolate=yes
</code></pre><p>The target unit doesn&rsquo;t contain much, it defines what it requires and which services and targets it can&rsquo;t coexist with. The <code>After</code> statement in the <code>[Unit]</code> sections also defines load ordering. It does not contain any information about units that it &ldquo;wants&rdquo;.</p>
<h3 id="understanding-wants">Understanding Wants</h3>
<p>Wants define which units should start when booting or starting a specific target.
Wants are created when enabling units using <code>systemd enable</code>, this happens by creating a symbolic link in the <code>/etc/systemd/system</code> directory. This directory contains a subdirectory for every target, which in turn contains &ldquo;wants&rdquo; as symbolic links to specific services that should be started:</p>
<pre><code>[root@server1 ~]# ls -l /etc/systemd/system/multi-user.target.wants/
total 0
lrwxrwxrwx. 1 root root 35 Sep 26 17:46 atd.service -&gt; /usr/lib/systemd/system/atd.service
lrwxrwxrwx. 1 root root 38 Sep 26 17:44 auditd.service -&gt; /usr/lib/systemd/system/auditd.service
lrwxrwxrwx. 1 root root 44 Sep 26 17:46 avahi-daemon.service -&gt; /usr/lib/systemd/system/avahi-daemon.service
lrwxrwxrwx. 1 root root 39 Sep 26 17:45 chronyd.service -&gt; /usr/lib/systemd/system/chronyd.service
lrwxrwxrwx. 1 root root 37 Sep 26 17:44 crond.service -&gt; /usr/lib/systemd/system/crond.service
...
</code></pre><p>The <code>[Install]</code> section in a service unit file specifies the target it is &ldquo;wanted&rdquo; by. Enabling the service creates a symbolic link in that targets' &ldquo;wants&rdquo; directory, making sure it starts when that target is booted into or started.</p>
<pre><code>[root@server1 ~]# systemctl cat httpd.service 
...
[Install]
WantedBy=multi-user.target

[root@server1 ~]# systemctl enable httpd
Created symlink /etc/systemd/system/multi-user.target.wants/httpd.service → /usr/lib/systemd/system/httpd.service.
</code></pre><h3 id="isolating-targets">Isolating Targets</h3>
<p>To get a list of all targets that are currently loaded, we can use the <code>systemctl --type=target</code> command. This shows all currently active targets. The <code>systemctl --type=target --all</code> command also shows inactivate targets.</p>
<pre><code>[root@server1 ~]# systemctl --type=target
UNIT                   LOAD   ACTIVE SUB    DESCRIPTION                
basic.target           loaded active active Basic System               
cryptsetup.target      loaded active active Local Encrypted Volumes    
getty.target           loaded active active Login Prompts              
graphical.target       loaded active active Graphical Interface        
local-fs-pre.target    loaded active active Local File Systems (Pre)   
local-fs.target        loaded active active Local File Systems         
multi-user.target      loaded active active Multi-User System          
network-online.target  loaded active active Network is Online          
network.target         loaded active active Network                    
nfs-client.target      loaded active active NFS client services        
nss-user-lookup.target loaded active active User and Group Name Lookups
paths.target           loaded active active Paths                      
remote-fs-pre.target   loaded active active Remote File Systems (Pre)  
remote-fs.target       loaded active active Remote File Systems        
rpc_pipefs.target      loaded active active rpc_pipefs.target          
rpcbind.target         loaded active active RPC Port Mapper            
slices.target          loaded active active Slices                     
sockets.target         loaded active active Sockets                    
sound.target           loaded active active Sound Card                 
sshd-keygen.target     loaded active active sshd-keygen.target         
swap.target            loaded active active Swap                       
sysinit.target         loaded active active System Initialization      
timers.target          loaded active active Timers                     

LOAD   = Reflects whether the unit definition was properly loaded.
ACTIVE = The high-level unit activation state, i.e. generalization of SUB.
SUB    = The low-level unit activation state, values depend on unit type.

23 loaded units listed. Pass --all to see loaded but inactive units, too.
To show all installed unit files use 'systemctl list-unit-files'.
</code></pre><p>Some of these targets can be isolated, they can be started to define the state of the machine and these are also the targets that can be set as the default target. They roughly correspond to the following System V runlevels:</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Target</th>
<th>Runlevel</th>
</tr>
</thead>
<tbody>
<tr>
<td>poweroff.target</td>
<td>runlevel 0</td>
</tr>
<tr>
<td>rescue.target</td>
<td>runlevel 1</td>
</tr>
<tr>
<td>multi-user.target</td>
<td>runlevel 3</td>
</tr>
<tr>
<td>graphical.target</td>
<td>runlevel 5</td>
</tr>
<tr>
<td>reboot.target</td>
<td>runlevel 6</td>
</tr>
</tbody>
</table>

<p>As mentioned earlier, targets that can be isolated have the <code>AllowIsolate</code> property in their <code>[Unit]</code> section:</p>
<pre><code>[root@server1 system]# grep Isolate *.target
anaconda.target:AllowIsolate=yes
ctrl-alt-del.target:AllowIsolate=yes
default.target:AllowIsolate=yes
emergency.target:AllowIsolate=yes
exit.target:AllowIsolate=yes
graphical.target:AllowIsolate=yes
halt.target:AllowIsolate=yes
initrd-switch-root.target:AllowIsolate=yes
initrd.target:AllowIsolate=yes
kexec.target:AllowIsolate=yes
multi-user.target:AllowIsolate=yes
poweroff.target:AllowIsolate=yes
reboot.target:AllowIsolate=yes
rescue.target:AllowIsolate=yes
runlevel0.target:AllowIsolate=yes
runlevel1.target:AllowIsolate=yes
runlevel2.target:AllowIsolate=yes
runlevel3.target:AllowIsolate=yes
runlevel4.target:AllowIsolate=yes
runlevel5.target:AllowIsolate=yes
runlevel6.target:AllowIsolate=yes
system-update.target:AllowIsolate=yes
</code></pre><p>To switch the current state of your machine to either one of these targets, use the <code>systemctl isolate</code> command:
<code>systemctl isolate rescue.target</code>
<code>systemctl isolate reboot.target</code></p>
<p>We can set a default ttarget using the <code>systemctl set-default</code> command, or check the current default target using the <code>systemctl get-default</code> command. Notice how the existing symlink is removed and a new one is created for <code>default.target</code>:</p>
<pre><code>[root@server1 system]# systemctl get-default 
graphical.target
 
[root@server1 system]# systemctl set-default multi-user.target 
Removed /etc/systemd/system/default.target.
Created symlink /etc/systemd/system/default.target → /usr/lib/systemd/system/multi-user.target.
</code></pre><h2 id="working-with-grub2">Working with GRUB2</h2>
<p>The GRUB2 bootloader makes sure we can boot Linux, it&rsquo;s installed in the boot sector of the hard drive and loads a Linux kernel and initramfs.
The initramfs contains a mini file system, mounted during boot, from where kernel modules load that are needed during the rest of the boot process, e.g. LVM modules.</p>
<p>We apply changes to GRUB2 by editing the <code>/etc/default/grub</code> file and we pass boot arguments to the kernel by editing the <code>GRUB_CMDLINE_LINUX</code> line:</p>
<pre><code>[root@server1 system]# cat /etc/default/grub 
GRUB_TIMEOUT=5
GRUB_DISTRIBUTOR=&quot;$(sed 's, release .*$,,g' /etc/system-release)&quot;
GRUB_DEFAULT=saved
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL_OUTPUT=&quot;console&quot;
GRUB_CMDLINE_LINUX=&quot;crashkernel=auto resume=/dev/mapper/cl-swap rd.lvm.lv=cl/root rd.lvm.lv=cl/swap rhgb quiet&quot;
GRUB_DISABLE_RECOVERY=&quot;true&quot;
GRUB_ENABLE_BLSCFG=true
</code></pre><p>The <code>GRUB_TIMEOUT</code> parameter defines how long GRUB2 waits before proceeding with the boot procedure. During this time you can press <code>e</code> to make changes to the configuration, just as you would by editing the <code>/etc/default/grub</code> file.</p>
<p>Removing the <code>rhgb</code> and <code>quiet</code> boot options would allow you to see the output of the boot procedure on screen.</p>
<p>After making changes to <code>/etc/default/grub</code> the relevant GRUB file on the <code>/boot</code> partition needs to be regenerated. On a BIOS system this file is located in <code>/boot/grub2/grub.cfg</code>, while on a UEFI system the file is located in <code>/boot/efi/EFI/redhat/grub.cfg</code>. To regenerate these files, we issue the <code>grub2-mkconfig</code> command and redirect its output to either one of these files:
<code>grub2-mkconfig -o /boot/grub2/grub.cfg</code>
<code>grub2-mkconfig -o /boot/efi/EFI/redhat/grub.cfg</code></p>
]]></content>
        </item>
        
        <item>
            <title>Basic Kernel Management</title>
            <link>https://joerismissaert.dev/basic-kernel-management/</link>
            <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/basic-kernel-management/</guid>
            <description>The Role of the Linux Kernel The Linux kernel is the layer between the user who works with Linux from a shell environment and the available hardware. It manages the I/O instructions received from software and translates it to CPU instructions. The kernel also handles essential operating system tasks like the scheduler to make sure that any processes started on the OS are handled by the CPU.
OS tasks that are handled by the kernel are implemented by using different kernel threads.</description>
            <content type="html"><![CDATA[
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />


<h2 id="the-role-of-the-linux-kernel">The Role of the Linux Kernel</h2>
<p>The Linux kernel is the layer between the user who works with Linux from a shell environment and the available hardware. It manages the I/O instructions received from software and translates it to CPU instructions. The kernel also handles essential operating system tasks like the scheduler to make sure that any processes started on the OS are handled by the CPU.</p>
<p>OS tasks that are handled by the kernel are implemented by using different kernel threads. You can easily indentify them with a command like <em>ps aux</em>, the kernel threads are listed between square brackets:</p>
<pre><code>[root@server1 ~]# ps aux
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root           1  0.0  0.5 180072 10364 ?        Ss   09:50   0:01 /usr/lib/syst
root           2  0.0  0.0      0     0 ?        S    09:50   0:00 [kthreadd]
root           3  0.0  0.0      0     0 ?        I&lt;   09:50   0:00 [rcu_gp]
root           4  0.0  0.0      0     0 ?        I&lt;   09:50   0:00 [rcu_par_gp]
root           6  0.0  0.0      0     0 ?        I&lt;   09:50   0:00 [kworker/0:0H
root           8  0.0  0.0      0     0 ?        I&lt;   09:50   0:00 [mm_percpu_wq
root           9  0.0  0.0      0     0 ?        S    09:50   0:00 [ksoftirqd/0]
root          10  0.0  0.0      0     0 ?        I    09:50   0:00 [rcu_sched]
</code></pre><p>The kernel also handles hardware initialization, making sure hardware can be used. To do so, drivers must be loaded and since the kernel is modular these drivers are loaded as kernel modules.</p>
<p>Hardware manufacturers do not always provide open source drivers, in this case the alternative would be to use closed source drivers. This is not always ideal, a badly functioning driver can crash the entire kernel. If this happens on an open source driver, the Linux community would jump in to debug and fix the problem which cannot be done on a closed source driver. A closed source or proprietary driver may however provide additional functionality not available in the open source equivalent. A kernel that is using closed source drivers is known as a <em>tainted kernel</em>.</p>
<h2 id="analyzing-what-the-kernel-is-doing">Analyzing What the Kernel is Doing</h2>
<p>A few different tools are provided by the Linux operating system to help check what the kernel is doing:</p>
<ul>
<li><strong>dmesg</strong></li>
<li>The <code>/proc</code> pseudo file system</li>
<li>The <strong>uname</strong> and <strong>hostnamectl</strong> utility</li>
</ul>
<p>When you require detailed information about kernel activity, you can use the <strong>dmesg</strong> command. This prints the content of the kernel ring buffer, an area of memory where the kernel keeps the recent log messages. Each entry in the output starts with a time indicator that shows the specific second the event was logged, relative to the start of the kernel.</p>
<p>An alternative to <strong>dmesg</strong> is <strong>journalctl - -dmesg</strong> or <strong>journalctl -k</strong>. These commands show a clock time indicator.</p>
<pre><code>[root@server1 ~]# dmesg | head
[    0.000000] Linux version 4.18.0-193.19.1.el8_2.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 8.3.1 20191121 (Red Hat 8.3.1-5) (GCC)) #1 SMP Mon Sep 14 14:37:00 UTC 2020
[    0.000000] Command line: BOOT_IMAGE=(hd0,msdos1)/vmlinuz-4.18.0-193.19.1.el8_2.x86_64 root=/dev/mapper/cl-root ro crashkernel=auto resume=/dev/mapper/cl-swap rd.lvm.lv=cl/root rd.lvm.lv=cl/swap rhgb quiet
[    0.000000] x86/fpu: x87 FPU will use FXSAVE
[    0.000000] BIOS-provided physical RAM map:
[    0.000000] BIOS-e820: [mem 0x0000000000000000-0x000000000009fbff] usable
[    0.000000] BIOS-e820: [mem 0x000000000009fc00-0x000000000009ffff] reserved
[    0.000000] BIOS-e820: [mem 0x00000000000f0000-0x00000000000fffff] reserved
[    0.000000] BIOS-e820: [mem 0x0000000000100000-0x000000007ffdcfff] usable
[    0.000000] BIOS-e820: [mem 0x000000007ffdd000-0x000000007fffffff] reserved
[    0.000000] BIOS-e820: [mem 0x00000000b0000000-0x00000000bfffffff] reserved


[root@server1 ~]# journalctl -k | head
-- Logs begin at Fri 2020-10-02 09:50:11 +04, end at Fri 2020-10-02 10:53:20 +04. --
Oct 02 09:50:11 server1.example.local kernel: Linux version 4.18.0-193.19.1.el8_2.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 8.3.1 20191121 (Red Hat 8.3.1-5) (GCC)) #1 SMP Mon Sep 14 14:37:00 UTC 2020
Oct 02 09:50:11 server1.example.local kernel: Command line: BOOT_IMAGE=(hd0,msdos1)/vmlinuz-4.18.0-193.19.1.el8_2.x86_64 root=/dev/mapper/cl-root ro crashkernel=auto resume=/dev/mapper/cl-swap rd.lvm.lv=cl/root rd.lvm.lv=cl/swap rhgb quiet
Oct 02 09:50:11 server1.example.local kernel: x86/fpu: x87 FPU will use FXSAVE
Oct 02 09:50:11 server1.example.local kernel: BIOS-provided physical RAM map:
Oct 02 09:50:11 server1.example.local kernel: BIOS-e820: [mem 0x0000000000000000-0x000000000009fbff] usable
Oct 02 09:50:11 server1.example.local kernel: BIOS-e820: [mem 0x000000000009fc00-0x000000000009ffff] reserved
Oct 02 09:50:11 server1.example.local kernel: BIOS-e820: [mem 0x00000000000f0000-0x00000000000fffff] reserved
Oct 02 09:50:11 server1.example.local kernel: BIOS-e820: [mem 0x0000000000100000-0x000000007ffdcfff] usable
Oct 02 09:50:11 server1.example.local kernel: BIOS-e820: [mem 0x000000007ffdd000-0x000000007fffffff] reserved
</code></pre><p>Many of the performance related commands or tools we use grab their information from the <code>/proc</code> file system. It contains detailed status information about what is happening on the machine. The <code>/proc</code> directory contains Process ID subdirectories which contain information about the particular process. The directory also contains status files, i.e. <code>/proc/partitions</code> or <code>/proc/meminfo</code>:</p>
<pre><code>[root@server1 ~]# cat /proc/1146/status
Name:	kvdo0:cpuQ0
Umask:	0000
State:	S (sleeping)
Tgid:	1146
Ngid:	0
Pid:	1146
PPid:	2
...

[root@server1 ~]# cat /proc/partitions 
major minor  #blocks  name

  11        0    8038400 sr0
   8       64    5242880 sde
   8       48    5242880 sdd
   8        0   26214400 sda
   8        1    1048576 sda1
   8        2   25164800 sda2
   8       32    5242880 sdc
   8       16    5242880 sdb
   8       17     102400 sdb1
   8       18     921600 sdb2
   8       19    2097152 sdb3
   8       20    2120687 sdb4
...



[root@server1 ~]# cat /proc/meminfo 
MemTotal:        1870616 kB
MemFree:           87464 kB
MemAvailable:     184724 kB
...
</code></pre><p>We can change kernel performance parameters during run time by writing values to the <code>/proc/sys</code> pseudo file system. You can apply the changes permanently by writing the parameters to <code>/etc/sysctl.conf</code>. To see what parameters are currently in use, issue the <strong>systctl -a</strong> command.</p>
<pre><code>[root@server1 ~]# sysctl -a | grep ip_forward
net.ipv4.ip_forward = 0

[root@server1 ~]# echo &quot;1&quot; &gt; /proc/sys/net/ipv4/ip_forward
[root@server1 ~]# sysctl -a | grep ip_forward
net.ipv4.ip_forward = 1


[root@server1 ~]# echo &quot;net.ipv4.ip_forward = 1&quot; &gt;&gt; /etc/sysctl.conf
[root@server1 ~]# reboot


[root@server1 ~]# sysctl -a | grep ip_forward
net.ipv4.ip_forward = 1
</code></pre><p>Another useful command would be <strong>uname</strong> and <strong>hostnamectl</strong>, it gives different kinds of information about the OS:</p>
<pre><code>[root@server1 ~]# uname -a
Linux server1.example.local 4.18.0-193.19.1.el8_2.x86_64 #1 SMP Mon Sep 14 14:37:00 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux

[root@server1 ~]# uname -r
4.18.0-193.19.1.el8_2.x86_64

[root@server1 ~]# hostnamectl status
   Static hostname: server1.example.local
         Icon name: computer-vm
           Chassis: vm
        Machine ID: e40db12b26ec4e9bb6a6f295f6d4d83e
           Boot ID: 5441210211394c5098724e9b89426cb2
    Virtualization: kvm
  Operating System: CentOS Linux 8 (Core)
       CPE OS Name: cpe:/o:centos:centos:8
            Kernel: Linux 4.18.0-193.19.1.el8_2.x86_64
      Architecture: x86-64
</code></pre><p>Lastly, you can <em>cat</em> the distribution release verion:</p>
<pre><code>[root@server1 ~]# cat /etc/redhat-release 
CentOS Linux release 8.2.2004 (Core) 
</code></pre><h2 id="working-with-kernel-modules">Working with Kernel Modules</h2>
<p>Since the release of Linux kernel 2.0 kernels are no longer compiled but modular. A modular kernel consists of a relatively small kernel core and provides driver support through modules that are loaded when they are required. Modules implement specific kernel functionality, they are not limited to loading hardware drivers alone. For example, file system support is also loaded as kernel modules.</p>
<h3 id="understanding-hardware-initialization">Understanding Hardware Initialization</h3>
<p>The loading of drivers is an automated process:</p>
<ul>
<li>The kernel probes available hardware during boot.</li>
<li>When a hardware component is detected, the <strong>systemd-udevd</strong> process loads the appropriate driver and makes the device available.</li>
<li><strong>systemd-udevd</strong> reads the rules in <code>/usr/lib/udev/rules.d/</code>. These are system-provided rules that should not be modified.</li>
<li><strong>systemd-udevd</strong> reads custom rules from the <code>/etc/udev/rules.d</code> directory, if available.</li>
<li>Required kernel modules have been loaded and the status of associated hardware is written to the sysfs file system on <code>/sys</code>. This pseudo file system tracks hardware-related settings.</li>
</ul>
<p>The <strong>systemd-udevd</strong> process continuously monitors for plugging and unplugging of hardware devices. You can see this in action when plugging/unplugging an usb or other block device while the <strong>udevadm monitor</strong> command is running:</p>
<pre><code>[root@server1 ~]# udevadm monitor
monitor will print the received events for:
UDEV - the event which udev sends out after rule processing
KERNEL - the kernel uevent

KERNEL[7080.543250] change   /devices/pci0000:00/0000:00:1f.2/ata1/host0/target0:0:0/0:0:0:0/block/sr0 (block)
UDEV  [7080.558849] change   /devices/pci0000:00/0000:00:1f.2/ata1/host0/target0:0:0/0:0:0:0/block/sr0 (block)
KERNEL[7080.578292] change   /devices/pci0000:00/0000:00:1f.2/ata1/host0/target0:0:0/0:0:0:0/block/sr0 (block)
UDEV  [7080.746283] change   /devices/pci0000:00/0000:00:1f.2/ata1/host0/target0:0:0/0:0:0:0/block/sr0 (block)
</code></pre><h3 id="managing-kernel-modules">Managing Kernel Modules</h3>
<p>Although loading of drivers happens automatically when they are required, there might be occasions where you need to manually load the appropriate kernel module.</p>
<p>To list all currently used kernel modules we use the <strong>lsmod</strong> command:</p>
<pre><code>[root@server1 ~]# lsmod | head
Module                  Size  Used by
binfmt_misc            20480  1
nls_utf8               16384  1
isofs                  45056  1
fuse                  131072  3
uinput                 20480  1
xt_CHECKSUM            16384  1
ipt_MASQUERADE         16384  3
xt_conntrack           16384  1
ipt_REJECT             16384  2
</code></pre><p><strong>modinfo</strong> provides more information about a specific kernel module, including two interesting sections: the alias and parms.
The alias refers to an alternative name that can be used to address the module and, the parms section refer to parameters that can be set while loading the module.</p>
<pre><code>[root@server1 ~]# modinfo e1000
filename:       /lib/modules/4.18.0-193.19.1.el8_2.x86_64/kernel/drivers/net/ethernet/intel/e1000/e1000.ko.xz
version:        7.3.21-k8-NAPI
license:        GPL
description:    Intel(R) PRO/1000 Network Driver
author:         Intel Corporation, &lt;linux.nics@intel.com&gt;
rhelversion:    8.2
srcversion:     9DFB28D9833DABBB7757EDD
alias:          pci:v00008086d00002E6Esv*sd*bc*sc*i*
...
depends:        
intree:         Y
name:           e1000
vermagic:       4.18.0-193.19.1.el8_2.x86_64 SMP mod_unload modversions 
sig_id:         PKCS#7
signer:         CentOS Linux kernel signing key
sig_key:        4C:02:86:8D:9E:A5:E0:4D:A9:C5:DF:8B:D7:28:EA:05:AF:C6:2A:6D
sig_hashalgo:   sha256
signature:      65:B3:87:34:C5:6F:E5:26:A7:41:90:2C:BB:20:04:54:6E:93:44:2A:
		86:73:D7:FF:FD:12:D3:17:74:EB:4B:9B:9C:FB:19:3F:D8:6A:16:10:
		0D:72:69:CA:63:B2:2E:63:A9:B4:84:94:0D:4B:C4:94:FC:E6:48:CC:
		95:DB:99:65:BC:6F:57:1C:F2:C5:CF:F0:BE:F2:8B:63:11:8F:43:C1:
		8C:1C:D3:03:6B:BC:76:0E:18:06:76:F1:C1:CF:72:84:04:92:07:A7:
		C4:59:4B:7B:72:86:CD:EB:A8:C5:EF:D9:39:FD:B0:38:1A:E3:49:18:
		04:88:39:8D:B9:98:D3:5E:EA:0C:CA:B7:44:51:64:F8:7F:CA:01:75:
		9A:48:DD:E9:2E:E1:38:60:C6:33:37:1A:81:79:B1:22:63:16:5B:42:
		DF:E2:08:9B:B4:47:47:9E:9A:69:5D:62:E9:9E:72:A3:7D:D0:E0:B0:
		51:24:EA:AD:B1:0B:08:67:63:89:17:19:9A:DF:13:82:FB:C2:DA:32:
		97:AA:07:C4:75:A5:6A:A1:E4:AF:D3:64:04:45:24:3F:40:81:21:12:
		99:11:54:2C:04:0C:86:98:56:79:C9:34:EC:B9:96:4F:52:BE:A4:CC:
		0A:3D:0F:78:5B:0E:1A:E3:7A:57:45:FA:B3:80:EF:B0:2E:75:8F:8B:
		FE:71:A1:74:63:DC:B2:7E:29:AD:87:4B:6E:AF:66:F7:81:34:1E:0B:
		7D:02:71:93:20:01:A7:9B:08:5F:AD:8C:EA:F5:E4:1E:4A:D1:AF:90:
		CE:23:9A:65:5B:F7:DE:94:3C:DF:6F:5C:15:51:62:D1:64:05:B3:8A:
		9A:F4:83:3C:C4:31:E4:EE:A5:6C:0D:56:96:DC:F1:00:53:91:78:BD:
		D4:20:03:A1:59:07:58:16:B0:8D:7B:19:E6:6A:A3:31:81:7E:31:ED:
		77:66:58:B0:F5:68:4E:A0:FA:5C:8B:56:40:4A:BB:77:E3:E3:13:62:
		1B:E5:5C:13
parm:           TxDescriptors:Number of transmit descriptors (array of int)
parm:           RxDescriptors:Number of receive descriptors (array of int)
parm:           Speed:Speed setting (array of int)
parm:           Duplex:Duplex setting (array of int)
parm:           AutoNeg:Advertised auto-negotiation setting (array of int)
parm:           FlowControl:Flow Control setting (array of int)
parm:           XsumRX:Disable or enable Receive Checksum offload (array of int)
parm:           TxIntDelay:Transmit Interrupt Delay (array of int)
parm:           TxAbsIntDelay:Transmit Absolute Interrupt Delay (array of int)
parm:           RxIntDelay:Receive Interrupt Delay (array of int)
parm:           RxAbsIntDelay:Receive Absolute Interrupt Delay (array of int)
parm:           InterruptThrottleRate:Interrupt Throttling Rate (array of int)
parm:           SmartPowerDownEnable:Enable PHY smart power down (array of int)
parm:           copybreak:Maximum size of packet that is copied to a new buffer on receive (uint)
parm:           debug:Debug level (0=none,...,16=all) (int)
</code></pre><p>To manually load and unload modules we use the <strong>modprobe</strong> and <strong>modprobe -r</strong> commands.
The <strong>modprobe</strong> command automatically loads any dependencies.</p>
<h3 id="checking-driver-availability-for-hardware-devices">Checking Driver Availability for Hardware Devices</h3>
<p>To check if a particular device is supported and thus has a module loaded you can use the <strong>lspci -k</strong> command.
If there are any devices for which no kernel module was loaded you&rsquo;re likely dealing with an unsupported device.</p>
<pre><code>[root@server1 ~]# lspci -k
00:00.0 Host bridge: Intel Corporation 82G33/G31/P35/P31 Express DRAM Controller
	Subsystem: Red Hat, Inc. QEMU Virtual Machine
00:01.0 VGA compatible controller: Red Hat, Inc. Virtio GPU (rev 01)
	Subsystem: Red Hat, Inc. Device 1100
	Kernel driver in use: virtio-pci
...
</code></pre><h3 id="managing-kernel-module-parameters">Managing Kernel Module Parameters</h3>
<p>You may want to load kernel modules with specific parameters you&rsquo;ve discovered using the <strong>modinfo</strong> command. To do so, specify the name of the parameter and its value in the <strong>modprobe</strong> command:</p>
<pre><code>[root@server1 ~]# modprobe cdrom debug=1
[root@server1 ~]# 
</code></pre><p>To make this persistent, you can add an entry to <code>/etc/modprobe.conf</code> or create a file in the <code>/etc/modprobe.d/</code> directory where the name of the file matches the module name and the content specifies the parameters you want to set:</p>
<pre><code>[root@server1 modprobe.d]# pwd
/etc/modprobe.d
[root@server1 modprobe.d]# cat cdrom.conf 
options cdrom debug=1
</code></pre><h2 id="upgrading-the-linux-kernel">Upgrading the Linux Kernel</h2>
<p>When upgrading the Linux kernel a new version of the kernel is installed next to the current version and will be used by default.
The kernel files for the last four kernels installed will be kept in <code>/boot</code>. The GRUB2 boot loader automatically picks up all kernels found in this directory, allowing you to select an older kernel at boot time in case the newly installed kernel doesn&rsquo;t boot correctly.</p>
<p>To install a new version of the kernel, issue the <strong>yum upgrade kernel</strong> or <strong>yum install kernel</strong> command.</p>
]]></content>
        </item>
        
        <item>
            <title>Advanced Storage: Virtual Data Optimizer</title>
            <link>https://joerismissaert.dev/advanced-storage-virtual-data-optimizer/</link>
            <pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/advanced-storage-virtual-data-optimizer/</guid>
            <description>Virtual Data Optimizer is a storage solution developed to reduce disk space usage on block devices by applying deduplication features. VDO creates volumes on top of any existing block device from where you either create an XFS file system, or use the volume as a Physical Volume in an LVM setup.
VDO uses three common technologies:
 Zero-block elimination to filter out data blocks that contain only zeros. Deduplication of redundant data blocks.</description>
            <content type="html"><![CDATA[
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />


<p>Virtual Data Optimizer is a storage solution developed to reduce disk space usage on block devices by applying deduplication features. VDO creates volumes on top of any existing block device from where you either create an XFS file system, or use the volume as a Physical Volume in an LVM setup.</p>
<p>VDO uses three common technologies:</p>
<ul>
<li><strong>Zero-block elimination</strong> to filter out data blocks that contain only zeros.</li>
<li><strong>Deduplication</strong> of redundant data blocks.</li>
<li><strong>Compression</strong> when the kvdo module compresses data blocks.</li>
</ul>
<p>Typical usage cases for VDO are host platforms for containers and virtual machines or cloud block storage.
Commonly, a logical size of up to 10 times the physical size is used for these types of environments.</p>
<h2 id="setting-up-vdo">Setting up VDO</h2>
<p>To use VDO the underlying block devices must have a minimal size of 4GiB and the <strong>vdo</strong> and <strong>kmod-kvdo</strong> packages must be installed.</p>
<p>We create the VDO device using the <code>vdo create</code> command, specify a name using the <code>--name=</code> option, and we <em>can</em> specify the logical size using the <code>--vdoLogicalSize=</code> option. e.g. <code>vdo create --name=myvdo1 --vdoLogicalSize=1T /dev/sdb</code></p>
<p>Once the device is created, we can put an XFS file system on top of it:
<code>mkfs.xfs -K /dev/mapper/myvdo1</code>
The <code>-K</code> option prevents unused blocks from being discarded immediately, making the command much faster.</p>
<p>At this point we issue the <code>udevadm settle</code> command to ensure device nodes have been created succesfully.</p>
<p>To persistently mount the VDO file system using the <code>/etc/fstab</code> file we must include the following mount options:
<code>x-systemd.requires=vdo.service,discard</code>
This makes sure the <strong>vdo</strong> service is loaded before trying to mount the file system.</p>
<p>An alternative method to persistently mount the VDO file system is to use the example systemd mount unit found in <code>/usr/share/doc/vdo/examples/systemd</code>.
Copy it to <code>/etc/systemc/system/mountpointname.mount</code> and edit the following lines:</p>
<pre><code>name = 
What = 
Where = 

</code></pre><p>The Unit file name must correspond to the <em>name</em>, <em>What</em> and <em>Where</em> values.
Make sure to enable and start the moutn at boot: <code>systemctl enable --now mountpointname.mount</code></p>
<h3 id="example">Example</h3>
<pre><code>[root@server1 ~]# vdo create --name=vdo1 --device=/dev/sdb --vdoLogicalSize=1T
Creating VDO vdo1
      The VDO volume can address 2 GB in 1 data slab.
      It can grow to address at most 16 TB of physical storage in 8192 slabs.
      If a larger maximum size might be needed, use bigger slabs.
Starting VDO vdo1
Starting compression on VDO vdo1
VDO instance 0 volume is ready at /dev/mapper/vdo1


[root@server1 ~]# lsblk
NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda           8:0    0   25G  0 disk 
├─sda1        8:1    0    1G  0 part /boot
└─sda2        8:2    0   24G  0 part 
  ├─cl-root 253:0    0   22G  0 lvm  /
  └─cl-swap 253:1    0  2.1G  0 lvm  [SWAP]
sdb           8:16   0    5G  0 disk 
└─vdo1      253:2    0    1T  0 vdo  
sdc           8:32   0    5G  0 disk 

[root@server1 ~]# mkfs.xfs -K /dev/mapper/vdo1 
meta-data=/dev/mapper/vdo1       isize=512    agcount=4, agsize=67108864 blks
         =                       sectsz=4096  attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=268435456, imaxpct=5
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=131072, version=2
         =                       sectsz=4096  sunit=1 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0

[root@server1 ~]# udevadm settle

[root@server1 ~]# cp /usr/share/doc/vdo/examples/systemd/VDO.mount.example /etc/systemd/system/vdo1.mount
[root@server1 ~]# vim /etc/systemd/system/vdo1.mount 
....

[root@server1 ~]# cat /etc/systemd/system/vdo1.mount 
[Unit]
Description = Mount filesystem that lives on VDO
name = vdo1.mount
Requires = vdo.service systemd-remount-fs.service
After = multi-user.target
Conflicts = umount.target
 
[Mount]
What = /dev/mapper/vdo1
Where = /vdo1
Type = xfs
Options = discard

[Install]
WantedBy = multi-user.target


[root@server1 ~]# systemctl enable --now vdo1.mount

[root@server1 ~]# vdostats --human-readable 
Device                    Size      Used Available Use% Space saving%
/dev/mapper/vdo1          5.0G      3.0G      2.0G  60%           99%

[root@server1 ~]# df -h /vdo1/
Filesystem        Size  Used Avail Use% Mounted on
/dev/mapper/vdo1  1.0T  7.2G 1017G   1% /vdo1

[root@server1 ~]# reboot

</code></pre>]]></content>
        </item>
        
        <item>
            <title>Advanced Storage: Configuring Stratis</title>
            <link>https://joerismissaert.dev/advanced-storage-configuring-stratis/</link>
            <pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/advanced-storage-configuring-stratis/</guid>
            <description>Stratis, created as an answer to Btrfs and ZFS by Red Hat, is a volume-managing file system that introduces advanced storage features like:
 Thin-provisioning: The file system presents itself to users as much bigger than it really is. Useful in virtualized environments. Snapshots: Allows users to backup the current state of the file system and makes it easy to revert to a previous state. Cache tier: A Ceph storage feature that ensures data is stored physically closer to the Ceph client, making data access faster.</description>
            <content type="html"><![CDATA[
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />


<p>Stratis, created as an answer to Btrfs and ZFS by Red Hat, is a volume-managing file system that introduces advanced storage features like:</p>
<ul>
<li><strong>Thin-provisioning</strong>: The file system presents itself to users as much bigger than it really is. Useful in virtualized environments.</li>
<li><strong>Snapshots</strong>: Allows users to backup the current state of the file system and makes it easy to revert to a previous state.</li>
<li><strong>Cache tier</strong>: A Ceph storage feature that ensures data is stored physically closer to the Ceph client, making data access faster.</li>
<li><strong>Programmatic API</strong>: Storage can be configured and modified through API access, particularly useful in cloud environments.</li>
<li><strong>Monitoring and repair</strong>: Stratis has built-in features to monitor and repair the file system, compared to traditional file systems which would rely on tools like <strong>fsck</strong>.</li>
</ul>
<h2 id="stratis-architecture">Stratis Architecture</h2>
<p>The lowest layer in the Stratis architecture is the pool, which is comparable to an LVM volume group. The pool represents all available storage and consists of one or more storage devices (referred to as <em>blockdev</em>). These block devices can be of any type, including LVM devices but <em>not</em> partitions, but cannot be thin-provisioned as Stratis creates volumes that are thing provisioned themselves. Stratis creates a <code>/stratis/poolname</code> directory for each pool, this directory contains links to devices that represent the file systems in the pool.</p>
<p>From the Stratis pool file systems are created which live in a volume on top of the pool. A pool can contain one or more file systems. Stratis only works with XFS file systems and these are integrated within the Stratis volume: You should not reformat or reconfigure XFS file systems that are managed by Stratis.</p>
<p>The file systems are thin-provisioned, they don&rsquo;t have a fixed size and grow automatically as more data is added to the file system.</p>
<h2 id="creating-and-mounting-stratis-storage">Creating and Mounting Stratis Storage</h2>
<p>To create Stratis storage, we need to create a pool from a block device and add a file system on top of the pool. Block devices need to be 1GiB at a minimum. Note that a Stratis file systems occupies a minimum of 527MiB even if no data has been added.</p>
<p>Let&rsquo;s make sure we have the <code>stratis-cli</code> and <code>stratisd</code> packages installed, then start and enable the <code>stratisd</code> daemon:</p>
<pre><code>[root@server1 ~]# yum install stratis-cli stratisd
...
[root@server1 ~]# systemctl enable --now stratisd
[root@server1 ~]# systemctl status stratisd
● stratisd.service - A daemon that manages a pool of block devices to create flexible file systems
   Loaded: loaded (/usr/lib/systemd/system/stratisd.service; enabled; vendor preset: enabled)
   Active: active (running) since Wed 2020-08-19 12:25:50 EDT; 4s ago
</code></pre><p>We create a pool from one of the available block devices. Make sure the block device does not contain any file system (use <code>blkid -p /dev/sdx</code>) or partition table, if so we wipe them with the <strong>wipefs</strong> command, e.g <code>wipefs --all /dev/sdx</code>.</p>
<pre><code>[root@server1 ~]# stratis pool create mypool1 /dev/sda
[root@server1 ~]# stratis pool list
Name     Total Physical Size  Total Physical Used
mypool1               10 GiB               52 MiB
</code></pre><p>Next, we create the <code>myfs1</code> file system on top of the <code>mypool1</code> pool:</p>
<pre><code>[root@server1 ~]# stratis fs create mypool1 myfs1
[root@server1 ~]# stratis fs list
Pool Name  Name   Used     Created            Device                  UUID                            
mypool1    myfs1  545 MiB  Aug 19 2020 12:28  /stratis/mypool1/myfs1  ffdbb3a131f6421c990f69aa8d87c6aa
[root@server1 ~]#
</code></pre><p>To peristently mount a Stratis file system, the UUID must be used in the <code>/etc/fstab</code> file and the mount option <code>x-systemd.requires=stratisd.service</code> must be specified to ensure that Systemd waits to activate this device until the stratisd service is loaded:</p>
<pre><code>[root@server1 ~]# blkid -p /stratis/mypool1/myfs1
/stratis/mypool1/myfs1: UUID=&quot;ffdbb3a1-31f6-421c-990f-69aa8d87c6aa&quot; TYPE=&quot;xfs&quot; USAGE=&quot;filesystem&quot;

[root@server1 ~]# mkdir /mnt/myfs1
[root@server1 ~]# vim /etc/fstab
...
UUID=ffdbb3a1-31f6-421c-990f-69aa8d87c6aa       /mnt/myfs1      xfs     defaults,x-systemd.requires=stratisd.service    0 0
...
[root@server1 ~]# 
[root@server1 ~]# mount -a
[root@server1 ~]# reboot

</code></pre><h2 id="managing-stratis">Managing Stratis</h2>
<p>Traditional Linux tools cannot handle thin-provisioned volumes, we need to use the Stratis specific tools:</p>
<ul>
<li><strong>stratis blockdev</strong>: Shows information about all block devices.</li>
<li><strong>stratis pool</strong>: Shows information about Stratis pools.</li>
<li><strong>stratis fs</strong>: Shows information about file systems.</li>
</ul>
<p>You can use tab-completion on the above commands to reveal specific options.</p>
<h3 id="expanding-and-renaming-a-pool-and-file-system">Expanding and Renaming a Pool and File System</h3>
<p>We can add a block device to a pool to expand the storage capacity of the pool using the <strong>stratis pool add-data poolname blockdevice</strong> command:</p>
<pre><code>[root@server1 ~]# stratis pool list
Name     Total Physical Size  Total Physical Used
mypool1               10 GiB              597 MiB
  
[root@server1 ~]# stratis pool add-data mypool1 /dev/sdb

[root@server1 ~]# stratis pool list
Name     Total Physical Size  Total Physical Used
mypool1               15 GiB              601 MiB
</code></pre><h3 id="destroying-a-pool-and-file-system">Destroying a Pool and File System</h3>
<p>To destroy a pool and file system, we need to unmount the file system first. Then use the <strong>stratis fs destroy poolname fsname</strong> command, followed by the <strong>stratis pool destroy poolname</strong> command:</p>
<pre><code>[root@server1 ~]# stratis fs list
Pool Name  Name   Used     Created            Device                  UUID                            
mypool1    myfs1  545 MiB  Aug 19 2020 12:28  /stratis/mypool1/myfs1  ffdbb3a131f6421c990f69aa8d87c6aa

[root@server1 ~]# umount /stratis/mypool1/myfs1
[root@server1 ~]# stratis fs destroy mypool1 myfs1

[root@server1 ~]# stratis fs list
Pool Name  Name  Used  Created  Device  UUID

[root@server1 ~]# stratis pool list
Name     Total Physical Size  Total Physical Used
mypool1               15 GiB               56 MiB

[root@server1 ~]# stratis pool destroy mypool1 

[root@server1 ~]# stratis pool list
Name  Total Physical Size  Total Physical Used
[root@server1 ~]#
</code></pre><h3 id="creating-and-accessing-a-stratis-snapshot">Creating and Accessing a Stratis Snapshot</h3>
<p>In Stratis, a snapshot is a regular Stratis file system created as a copy of another Stratis file system. The snapshot initially contains the same file content as the original file system, but can change as the snapshot is modified. Whatever changes you make to the snapshot will not be reflected in the original file system.</p>
<p>To create a Stratis snapshot, use <strong>stratis fs snapshot poolname fsname snapshotname</strong>.
To access the snapshot, mount it as a regular file system from the /stratis/my-pool/ directory: <strong>mount /stratis/poolname/snapshotname mount-point</strong></p>
<pre><code>[root@server1 ~]# stratis pool create mypool1 /dev/sda
[root@server1 ~]# stratis pool add-data mypool1 /dev/sdb
[root@server1 ~]# stratis pool list
Name     Total Physical Size  Total Physical Used
mypool1               15 GiB               56 MiB

[root@server1 ~]# stratis fs create mypool1 myfs1
[root@server1 ~]# stratis fs list
Pool Name  Name   Used     Created            Device                  UUID                            
mypool1    myfs1  545 MiB  Aug 19 2020 13:16  /stratis/mypool1/myfs1  d9e0c47f26e44e0b8990a6aa7546d0f7

[root@server1 ~]# stratis fs snapshot mypool1 myfs1 myfs1snapshot
[root@server1 ~]# stratis fs list
Pool Name  Name           Used     Created            Device                          UUID                            
mypool1    myfs1          545 MiB  Aug 19 2020 13:16  /stratis/mypool1/myfs1          d9e0c47f26e44e0b8990a6aa7546d0f7
mypool1    myfs1snapshot  545 MiB  Aug 19 2020 13:17  /stratis/mypool1/myfs1snapshot  b2fb662124a4424c9d21429012fcfdc4

[root@server1 ~]# mkdir -p /mnt/myfs1snapshot
[root@server1 ~]# mount /stratis/mypool1/myfs1snapshot /mnt/myfs1snapshot/
[root@server1 ~]# umount /mnt/myfs1snapshot
[root@server1 ~]# mount /stratis/mypool1/myfs1 /mnt/myfs1
[root@server1 ~]#

</code></pre><h3 id="reverting-a-stratis-file-system-to-a-previous-snapshot">Reverting a Stratis File System to a Previous Snapshot</h3>
<p>It&rsquo;s a good idea to backup the current file system before reverting to a previous snapshot:</p>
<pre><code>[root@server1 ~]# stratis fs snapshot mypool1 myfs1 myfs1snapshot2
[root@server1 ~]#
</code></pre><p>Next, we unmount and remove the original file system:</p>
<pre><code>[root@server1 ~]# umount /mnt/myfs1
[root@server1 ~]# stratis fs destroy mypool1 myfs1
[root@server1 ~]#

</code></pre><p>We create a copy of a previous snapshot which we wish to restore, under the name of the original file system:</p>
<pre><code>[root@server1 ~]# stratis fs list
Pool Name  Name            Used     Created            Device                           UUID                            
mypool1    myfs1snapshot2  545 MiB  Aug 19 2020 13:23  /stratis/mypool1/myfs1snapshot2  f54d88f686d64acd94c3a7d73dac92f5
mypool1    myfs1snapshot   545 MiB  Aug 19 2020 13:17  /stratis/mypool1/myfs1snapshot   b2fb662124a4424c9d21429012fcfdc4

[root@server1 ~]# stratis fs snapshot mypool1 myfs1snapshot myfs1
[root@server1 ~]# stratis fs list
Pool Name  Name            Used     Created            Device                           UUID                            
mypool1    myfs1snapshot2  545 MiB  Aug 19 2020 13:23  /stratis/mypool1/myfs1snapshot2  f54d88f686d64acd94c3a7d73dac92f5
mypool1    myfs1           545 MiB  Aug 19 2020 13:31  /stratis/mypool1/myfs1           82f75da64c744079b1c2ae51792812a0
mypool1    myfs1snapshot   545 MiB  Aug 19 2020 13:17  /stratis/mypool1/myfs1snapshot   b2fb662124a4424c9d21429012fcfdc4
</code></pre><p>We mount the snapshot, now accessible with the same name as the original file system:</p>
<pre><code>[root@server1 ~]# mount /stratis/mypool1/myfs1 /mnt/myfs1
[root@server1 ~]#
</code></pre><h3 id="removing-a-stratis-snapshot">Removing a Stratis Snapshot</h3>
<p>We remove a Stratis snapshot by unmounting it first if required, then using the <strong>stratis fs destroy poolname snapshotname</strong> command.</p>
<pre><code>[root@server1 ~]# stratis fs destroy mypool1 myfs1snapshot2
[root@server1 ~]#
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Advanced Storage: Logical Volume Manager</title>
            <link>https://joerismissaert.dev/advanced-storage-logical-volume-manager/</link>
            <pubDate>Mon, 10 Aug 2020 00:00:00 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/advanced-storage-logical-volume-manager/</guid>
            <description>Understanding LVM The Logical Volume Manager was introduced to workaround some restrictions that come with standard partitions. The most important restriction would be inflexibility, with LVM you can dynamically grow a partition even if the disk itself is running out of space.
In the LVM architecture we can distinguish several layers. The lowest layer contains the storage devices, these can be anything from regular disks to partions and logical units (LUNs) on a storage-area network (SAN).</description>
            <content type="html"><![CDATA[
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />


<h2 id="understanding-lvm">Understanding LVM</h2>
<p>The Logical Volume Manager was introduced to workaround some restrictions that come with standard partitions.
The most important restriction would be inflexibility, with LVM you can dynamically grow a partition even if the disk itself is running out of space.</p>
<p>In the LVM architecture we can distinguish several layers. The lowest layer contains the storage devices, these can be anything from regular disks to partions and logical units (LUNs) on a storage-area network (SAN). Storage devices need to be flagged as physical volumes so that it can be used in an LVM setup. In turn, the physical volume is added to a volume group, which is the abstraction of all available storage space. The volume group can be resized when needed by adding more space (or physical volumes) to the volume group.</p>
<p>On top of the volume group we have the logical volumes, they get their disk space from the volume group. This means a logical volume can consist of storage space coming from multiple physical volumes.</p>
<p>The actual file systems are created on the logical volumes. The file system must support resizing if the logical volumes are resized.</p>
<blockquote>
<p>When running out of disk space on a logical volume, we take available disk space from the volume group. If there is no available disk space on the volume group, we add a physical volume to the volume group.</p>
</blockquote>

    <img src="/img/LVM.png"  alt="LVM Architecture"  class="center"  />


<p>The most important benefit for using LVM would be the added flexibility in managing storage, volumes are not bound to the restrictions of physical hard drives.</p>
<p>Another benefit would be the support for snapshots. A snapshot keeps the current state of a logical volume and it can be used to revert to a previous state.</p>
<p>LVM snapshots are created by copying the logical volume metadata (describing the current state) to a snapshot volume. As long as nothing changes, the original blocks on the volume are addressed. When blocks are modified, the blocks containing the previous state of file are copied to the snapshot volume.</p>
<p>A third advantage to using LVM would be the option to replace failing hardware easily. If a disk is failing, data can be moved within the volume group, the failing disk can be removed from the volume group and a new disk can be added. This without any downtime for the logical volume itself.</p>
<h2 id="creating-logical-volumes">Creating Logical Volumes</h2>
<p>In order to create logical volumes, we need to create the underlying layers in the LVM architecture: First we convert the physical devices into physical volumes, then we create the volume group and assign physical volumes to it, lastly we create the logical volume.</p>
<h3 id="creating-physical-volumes">Creating Physical Volumes</h3>
<p>To create a physical volume, we create a partition and mark it with the LVM partition type. For MBR disks that would be type <code>8e</code> and <code>8e00</code> for GUID disks. When using <strong>parted</strong> you need to use the <code>set n lvm on</code> command, where <code>n</code> is the partition number.</p>
<p>After creating the partition and flagging it as an LVM partition type, we use the <strong>pvcreate</strong> command to mark it as a physical volume. This writes metadata to the partition so a volume group can use it.</p>
<p>In the below example, I&rsquo;ll use an unpartitioned disk (<code>sda</code>) to create a physical volume on.</p>
<pre><code>[root@server1 ~]# lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda      8:0    0   10G  0 disk 
sr0     11:0    1 1024M  0 rom  
vda    253:0    0   20G  0 disk 
├─vda1 253:1    0    1G  0 part /boot
├─vda2 253:2    0    1G  0 part [SWAP]
└─vda3 253:3    0    8G  0 part /


[root@server1 ~]# gdisk /dev/sda
GPT fdisk (gdisk) version 1.0.3

Partition table scan:
  MBR: not present
  BSD: not present
  APM: not present
  GPT: not present

Creating new GPT entries.

Command (? for help): n
Partition number (1-128, default 1): 
First sector (34-20971486, default = 2048) or {+-}size{KMGTP}: 
Last sector (2048-20971486, default = 20971486) or {+-}size{KMGTP}: +2G
Current type is 'Linux filesystem'
Hex code or GUID (L to show codes, Enter = 8300): 8e00
Changed type of partition to 'Linux LVM'

Command (? for help): p
Disk /dev/sda: 20971520 sectors, 10.0 GiB
Model: QEMU HARDDISK   
Sector size (logical/physical): 512/512 bytes
Disk identifier (GUID): 7188B769-C4F4-46C6-8A81-B0E719E621EA
Partition table holds up to 128 entries
Main partition table begins at sector 2 and ends at sector 33
First usable sector is 34, last usable sector is 20971486
Partitions will be aligned on 2048-sector boundaries
Total free space is 16777149 sectors (8.0 GiB)

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048         4196351   2.0 GiB     8E00  Linux LVM

Command (? for help): w

Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTING
PARTITIONS!!

Do you want to proceed? (Y/N): y
OK; writing new GUID partition table (GPT) to /dev/sda.
The operation has completed successfully.


[root@server1 ~]# pvcreate /dev/sda1
  Physical volume &quot;/dev/sda1&quot; successfully created.
</code></pre><p>We can see a summmary of the physical volumes by using the <strong>pvs</strong> command, or see more details by using the <strong>pvdisplay</strong> command:</p>
<pre><code>[root@server1 ~]# pvs
  PV         VG Fmt  Attr PSize PFree
  /dev/sda1     lvm2 ---  2.00g 2.00g

[root@server1 ~]# pvdisplay
  &quot;/dev/sda1&quot; is a new physical volume of &quot;2.00 GiB&quot;
  --- NEW Physical volume ---
  PV Name               /dev/sda1
  VG Name               
  PV Size               2.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               0ata3q-CxXg-WFv6-p3GR-CWxc-wrG3-HxVM6N

</code></pre><h3 id="creating-volume-groups">Creating Volume Groups</h3>
<p>Now that we have a physical volume, we should assign it to a volume group.
In this case we&rsquo;ll create a new volume group, later on we&rsquo;ll discuss how to add a physical volume to an already existing volume group.</p>
<p>We need to issue the <strong>vgcreate</strong> command followed by the name of the volume group and the name of the physical device we want to add to it:</p>
<pre><code>[root@server1 ~]# vgcreate vgdata /dev/sda1 
  Volume group &quot;vgdata&quot; successfully created
</code></pre><p>Check the volume group with the <strong>vgs</strong> and <strong>vgdisplay</strong> commands:</p>
<pre><code>[root@server1 ~]# vgs
  VG     #PV #LV #SN Attr   VSize  VFree 
  vgdata   1   0   0 wz--n- &lt;2.00g &lt;2.00g

[root@server1 ~]# vgdisplay
  --- Volume group ---
  VG Name               vgdata
  System ID             
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               &lt;2.00 GiB
  PE Size               4.00 MiB
  Total PE              511
  Alloc PE / Size       0 / 0   
  Free  PE / Size       511 / &lt;2.00 GiB
  VG UUID               h50RrD-QmTD-yY3f-gDiy-TolW-GR1B-gFoaCe
</code></pre><blockquote>
<p>We could have created the physical and volume group in one step as long as the partition is marked as an LVM partition. When issuing the command <strong>vgcreate vgdata /dev/sda1</strong> without having created the physical volume, the <strong>vgcreate</strong> utility will automatically flag the partition as a physical volume.   This is useful for adding a complete disk device instead of a partition. A complete disk device does not need to be flagged for LVM use in a partion utility, e.g. <strong>vgcreate vgbackup /dev/sdb</strong>.</p>
</blockquote>
<p>When working with LVM there is the <em>physical extent size</em> to consider. This is the size of the basic building blocks used in the LVM configuration. The default extent size is 4.00MiB:</p>
<pre><code>[root@server1 ~]# vgdisplay | grep  'PE'
  PE Size               4.00 MiB
  Total PE              511
  Alloc PE / Size       0 / 0   
  Free  PE / Size       511 / &lt;2.00 GiB
</code></pre><p>The PE Size is always specified as multiples of 2MiB with a maximum of 128MiB.
The <strong>vgcreate -s</strong> option allows you to specify the PE Size you want to use. If you need to create huge logical volumes it is more efficient to use a big PE Size.</p>
<p>Above we can see that the PE Size is 4.00MiB and we have a total PE of 511 blocks.
511 multiplied by 4 would be 2044MiB.</p>
<h3 id="creating-the-logical-volumes-and-file-systems">Creating the Logical Volumes and File Systems</h3>
<p>When creating the logical volume, we have to specify a logical volume name and size.
We specify the name use the <strong>lvcreate -n</strong> option, an absolute size with the <strong>-L</strong> option or a relative size using the <strong>-l</strong> option:</p>
<ul>
<li><strong>lvcreate -n mylvol1 -L 2G vgdata</strong> - Creates a logical volume with the name <code>mylvol1</code> and an absolute size of 2GiB taken from the <code>vgdata</code> volume group.</li>
<li><strong>lvcreate -n mylvol1 -l 100%FREE vgdata</strong> - Creates a logical volume spanning all available space in the <code>vgdata</code> volume group.</li>
<li><strong>lvcreate -n mylvol1 -l 50%FREE vgdata</strong> - Creates a logical volume spanning 50% of the available space in the <code>vgdata</code> volume group.</li>
</ul>
<pre><code>[root@server1 ~]# lvcreate -n lvol1 -l 50%FREE vgdata
  Logical volume &quot;lvol1&quot; created.
[root@server1 ~]# lvcreate -n lvol2 -l 100%FREE vgdata
  Logical volume &quot;lvol2&quot; created.

[root@server1 ~]# lsblk
NAME             MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                8:0    0   10G  0 disk 
└─sda1             8:1    0    2G  0 part 
  ├─vgdata-lvol1 252:0    0 1020M  0 lvm  
  └─vgdata-lvol2 252:1    0    1G  0 lvm  


[root@server1 ~]# lvs
  LV    VG     Attr       LSize    Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  lvol1 vgdata -wi-a----- 1020.00m                                                    
  lvol2 vgdata -wi-a-----    1.00g    

[root@server1 ~]# vgs
  VG     #PV #LV #SN Attr   VSize  VFree
  vgdata   1   2   0 wz--n- &lt;2.00g    0 

</code></pre><p>Notice how I created one logical volume consisting of half of the available space in the volume group first, then created a second one with the <code>-l 100%FREE</code> option to take all remaining available space.</p>
<p>At this point we&rsquo;re ready to create a file system on both logical volumes:</p>
<pre><code>[root@server1 ~]# mkfs.xfs /dev/vgdata/lvol1
meta-data=/dev/vgdata/lvol1      isize=512    agcount=4, agsize=65280 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=261120, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=1566, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0

[root@server1 ~]# mkfs.xfs /dev/vgdata/lvol2
meta-data=/dev/vgdata/lvol2      isize=512    agcount=4, agsize=65536 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=262144, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
</code></pre><h3 id="understanding-device-naming">Understanding Device Naming</h3>
<p>Logical volumes can be addressed in different ways:</p>
<ul>
<li><code>/dev/[volume group]/[logical volume]</code> e.g. <code>/dev/vgdata/lvol1</code></li>
<li><code>/dev/mapper/[volumge group]-[logical volume]</code> e.g. <code>/dev/mapper/vgdata-lvol1</code></li>
</ul>
<p>The first method is basically a symbolic link to the device mapper (abbreviate as <code>dm</code>), which in turn is a generic interface the Linux kernel uses to address storage devices. Device mapper devices are generated on detection and use meaningless names like <code>/dev/dm-0</code> and <code>/dev/dm-1</code>:</p>
<pre><code>[root@server1 ~]# ls -l /dev/vgdata/
total 0
lrwxrwxrwx. 1 root root 7 Aug 14 00:10 lvol1 -&gt; ../dm-0
lrwxrwxrwx. 1 root root 7 Aug 14 00:10 lvol2 -&gt; ../dm-1
</code></pre><p>To provide easier access there are symbolic links to those same device mapper devices in <code>/dev/mapper</code>:</p>
<pre><code>[root@server1 ~]# ls -l /dev/mapper/
total 0
crw-------. 1 root root 10, 236 Aug 14 00:10 control
lrwxrwxrwx. 1 root root       7 Aug 14 00:10 vgdata-lvol1 -&gt; ../dm-0
lrwxrwxrwx. 1 root root       7 Aug 14 00:10 vgdata-lvol2 -&gt; ../dm-1
</code></pre><p>When working with LVM logical volumes, you can use either of these device names.</p>
<h3 id="essential-commands-for-lvm-management">Essential Commands for LVM Management</h3>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>pvcreate</td>
<td>Creates physical volumes</td>
</tr>
<tr>
<td>pvs</td>
<td>Summary of available physical volumes</td>
</tr>
<tr>
<td>pvdisplay</td>
<td>List physical volumes and their properties</td>
</tr>
<tr>
<td>pvremove</td>
<td>Removes the physical volume signature from a block device</td>
</tr>
<tr>
<td>vgcreate</td>
<td>Creates a volume group</td>
</tr>
<tr>
<td>vgs</td>
<td>Summary of available volume groups</td>
</tr>
<tr>
<td>vgdisplay</td>
<td>List volume groups and their properties</td>
</tr>
<tr>
<td>vgremove</td>
<td>Removes a volume group</td>
</tr>
<tr>
<td>lvcreate</td>
<td>Creates logical volumes</td>
</tr>
<tr>
<td>lvs</td>
<td>Shows a summary of all available logical volumes</td>
</tr>
<tr>
<td>lvdisplay</td>
<td>List available logical volumes and their properties</td>
</tr>
<tr>
<td>lvremove</td>
<td>Removes a logical volumes</td>
</tr>
</tbody>
</table>

<h2 id="resizing-lvm-logical-volumes">Resizing LVM Logical Volumes</h2>
<p>The ability to resize logical volumes is one of the major benefits of using LVM.</p>
<blockquote>
<p>When using the XFS file system, a volume can be increased in size, but <em>not</em> decreased.  Ext4 supports decreasing the file system size, but it must be done when the file system is offline. In other words, you must unmount it before you can resize it. To increase the size of a logical volume, we need to have disk space available in the volume group, so we need to address that first.</p>
</blockquote>
<h3 id="resizing-volume-groups">Resizing Volume Groups</h3>
<p>The <strong>vgextend</strong> command is used to add storage to a volume group, while the <strong>vgreduce</strong> command is used to take physical volumes out of a volume group:</p>
<ul>
<li>Make sure a physical volume or device is available to be added to a volume group</li>
<li>Use the <strong>vgextend</strong> command to extend the volume group with the space from the new physical volume or device.</li>
<li>Use the <strong>vgs</strong> or <strong>vgdisplay</strong> commands to verify that a physical volume has been added to the volume group. The number of physical volumes for a specific volume group are indicated in the <code>#PV</code> column.</li>
</ul>
<h3 id="resizing-logical-volumes-and-file-systems">Resizing Logical Volumes and File Systems</h3>
<p>Logical volumes can be extended using the <strong>lvextend</strong> or <strong>lvresize</strong> commands and it can automatically take care of extending the file system on top using the <strong>-r</strong> option.</p>
<p>Similarly to creating logical volumes, you can extend the logical volume size with the <code>-L</code> or <code>-l</code> options (absolute or relative sizes). If you specify an absolute size,  the <code>-L</code> option must be followed by a <strong>+</strong> sign and the amount of disk space you want to add, e.g. <code>lvextend -L +1G -r /dev/vgdata/lvol1</code>.</p>
<p>Relative sizes can be specified like so:</p>
<ul>
<li>
<p><code>lvextend -r -l 75%VG /dev/vgdata/lvol1</code> - This resizes the logical volume so that it takes <em>75% of the <strong>total</strong> disk space</em> in the volume group.</p>
</li>
<li>
<p><code>lvresize -r -l +75%VG /dev/vgdata/lvol1</code> - This <em><strong>adds</strong> 75% of the total size</em> of the volume group to the logical volume.</p>
</li>
<li>
<p><code>lvextend -r -l +75%FREE /dev/vgdata/lvol1</code> - <em><strong>Add</strong></em> 75% of all <em><strong>free</strong></em> disk space to the logical volume.</p>
</li>
<li>
<p><code>lvresize -r -l 75%FREE /dev/vgdata/lvol1</code> - This resizes the logical volume so that it takes <em>75% of the <strong>free</strong> disk space</em> in the volume group.</p>
</li>
</ul>
<p>Let&rsquo;s check the current block devices:</p>
<pre><code>[root@server1 ~]# lsblk
NAME             MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                8:0    0   10G  0 disk 
└─sda1             8:1    0    2G  0 part 
  ├─vgdata-lvol1 252:0    0 1020M  0 lvm  
  └─vgdata-lvol2 252:1    0    1G  0 lvm  
sr0               11:0    1 1024M  0 rom  
vda              253:0    0   20G  0 disk 
├─vda1           253:1    0    1G  0 part /boot
├─vda2           253:2    0    1G  0 part [SWAP]
└─vda3           253:3    0    8G  0 part /
</code></pre><p>We create a new partition and mark it as a physical volume:</p>
<pre><code>[root@server1 ~]# gdisk /dev/sda
GPT fdisk (gdisk) version 1.0.3

Partition table scan:
  MBR: protective
  BSD: not present
  APM: not present
  GPT: present

Found valid GPT with protective MBR; using GPT.

Command (? for help): p
Disk /dev/sda: 20971520 sectors, 10.0 GiB
Model: QEMU HARDDISK   
Sector size (logical/physical): 512/512 bytes
Disk identifier (GUID): 7188B769-C4F4-46C6-8A81-B0E719E621EA
Partition table holds up to 128 entries
Main partition table begins at sector 2 and ends at sector 33
First usable sector is 34, last usable sector is 20971486
Partitions will be aligned on 2048-sector boundaries
Total free space is 16777149 sectors (8.0 GiB)

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048         4196351   2.0 GiB     8E00  Linux LVM

Command (? for help): n
Partition number (2-128, default 2): 
First sector (34-20971486, default = 4196352) or {+-}size{KMGTP}: 
Last sector (4196352-20971486, default = 20971486) or {+-}size{KMGTP}: +3G
Current type is 'Linux filesystem'
Hex code or GUID (L to show codes, Enter = 8300): 8e00
Changed type of partition to 'Linux LVM'

Command (? for help): p
Disk /dev/sda: 20971520 sectors, 10.0 GiB
Model: QEMU HARDDISK   
Sector size (logical/physical): 512/512 bytes
Disk identifier (GUID): 7188B769-C4F4-46C6-8A81-B0E719E621EA
Partition table holds up to 128 entries
Main partition table begins at sector 2 and ends at sector 33
First usable sector is 34, last usable sector is 20971486
Partitions will be aligned on 2048-sector boundaries
Total free space is 10485693 sectors (5.0 GiB)

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048         4196351   2.0 GiB     8E00  Linux LVM
   2         4196352        10487807   3.0 GiB     8E00  Linux LVM

Command (? for help): w

Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTING
PARTITIONS!!

Do you want to proceed? (Y/N): y
OK; writing new GUID partition table (GPT) to /dev/sda.
Warning: The kernel is still using the old partition table.
The new table will be used at the next reboot or after you
run partprobe(8) or kpartx(8)
The operation has completed successfully.

[root@server1 ~]# partprobe
</code></pre><p>Check the output of <code>lsblk</code> for the new partition:</p>
<pre><code>[root@server1 ~]# lsblk
NAME             MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                8:0    0   10G  0 disk 
├─sda1             8:1    0    2G  0 part 
│ ├─vgdata-lvol1 252:0    0 1020M  0 lvm  
│ └─vgdata-lvol2 252:1    0    1G  0 lvm  
└─sda2             8:2    0    3G  0 part 
sr0               11:0    1 1024M  0 rom  
vda              253:0    0   20G  0 disk 
├─vda1           253:1    0    1G  0 part /boot
├─vda2           253:2    0    1G  0 part [SWAP]
└─vda3           253:3    0    8G  0 part /
</code></pre><p>Check the output of <code>vgs</code>, extend the volume group and check the changes:</p>
<pre><code>[root@server1 ~]# vgs
  VG     #PV #LV #SN Attr   VSize  VFree
  vgdata   1   2   0 wz--n- &lt;2.00g    0 

[root@server1 ~]# vgextend vgdata /dev/sda2
  Physical volume &quot;/dev/sda2&quot; successfully created.
  Volume group &quot;vgdata&quot; successfully extended

[root@server1 ~]# vgs
  VG     #PV #LV #SN Attr   VSize VFree 
  vgdata   2   2   0 wz--n- 4.99g &lt;3.00g
</code></pre><p>Extend the logical volume with all available disk space (3GiB) from the volume group:</p>
<pre><code>[root@server1 ~]# lvresize -r -l +100%FREE /dev/vgdata/lvol1
Phase 1 - find and verify superblock...
Phase 2 - using internal log
        - zero log...
        - scan filesystem freespace and inode maps...
        - found root inode chunk
Phase 3 - for each AG...
        - scan (but don't clear) agi unlinked lists...
        - process known inodes and perform inode discovery...
        - agno = 0
        - agno = 1
        - agno = 2
        - agno = 3
        - agno = 4
        - agno = 5
        - agno = 6
        - agno = 7
        - agno = 8
        - agno = 9
        - agno = 10
        - agno = 11
        - agno = 12
        - process newly discovered inodes...
Phase 4 - check for duplicate blocks...
        - setting up duplicate extent list...
        - check for inodes claiming duplicate blocks...
        - agno = 0
        - agno = 1
        - agno = 2
        - agno = 3
        - agno = 4
        - agno = 5
        - agno = 6
        - agno = 7
        - agno = 8
        - agno = 9
        - agno = 10
        - agno = 11
        - agno = 12
No modify flag set, skipping phase 5
Phase 6 - check inode connectivity...
        - traversing filesystem ...
        - traversal finished ...
        - moving disconnected inodes to lost+found ...
Phase 7 - verify link counts...
No modify flag set, skipping filesystem flush and exiting.
  Size of logical volume vgdata/lvol1 changed from &lt;3.00 GiB (767 extents) to 3.99 GiB (1022 extents).
  Logical volume vgdata/lvol1 successfully resized.
meta-data=/dev/mapper/vgdata-lvol1 isize=512    agcount=13, agsize=65280 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=785408, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=1566, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
data blocks changed from 785408 to 1046528
</code></pre><p>Notice that I was using the XFS file system and that the <code>lvresize -r</code> option resized the file system as well.</p>
<p>Verify the changes in <code>lbsblk</code>:</p>
<pre><code>[root@server1 ~]# lsblk
NAME             MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                8:0    0   10G  0 disk 
├─sda1             8:1    0    2G  0 part 
│ ├─vgdata-lvol1 252:0    0    4G  0 lvm  
│ └─vgdata-lvol2 252:1    0    1G  0 lvm  
└─sda2             8:2    0    3G  0 part 
  └─vgdata-lvol1 252:0    0    4G  0 lvm  
sr0               11:0    1 1024M  0 rom  
vda              253:0    0   20G  0 disk 
├─vda1           253:1    0    1G  0 part /boot
├─vda2           253:2    0    1G  0 part [SWAP]
└─vda3           253:3    0    8G  0 part /
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Managing Storage: Creating &amp; Mounting File Systems</title>
            <link>https://joerismissaert.dev/managing-storage-creating-mounting-file-systems/</link>
            <pubDate>Fri, 31 Jul 2020 00:00:00 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/managing-storage-creating-mounting-file-systems/</guid>
            <description>Partitions by themselves aren&amp;rsquo;t of any use if they don&amp;rsquo;t contain a file system. On RHEL 8 different file systems can be used, the default being xfs. To format a partition with one of the supported file systems we can use the mkfs command followed by the -t option to specify a specific file system. Alternatively, you can use a file system-specific tool like mkfs.xfs to format an xfs file system or mkfs.</description>
            <content type="html"><![CDATA[
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />


<p>Partitions by themselves aren&rsquo;t of any use if they don&rsquo;t contain a file system. On RHEL 8 different file systems can be used, the default being <strong>xfs</strong>. To format a partition with one of the supported file systems we can use the <code>mkfs</code> command followed by the <code>-t</code> option to specify a specific file system. Alternatively, you can use a file system-specific tool like <code>mkfs.xfs</code> to format an <strong>xfs</strong> file system or <code>mkfs.ext4</code> to format an <strong>Ext4</strong> file system.</p>
<blockquote>
<p>If you use <code>mkfs</code> without specifying what file system to format, the <strong>Ext2</strong> file system is used.</p>
</blockquote>
<p>Among the supported file systems, we can distuinguish Journaling, Non-Journaling and FAT File Systems:</p>
<ul>
<li>
<p><strong>Non-Journaling</strong>:</p>
<ul>
<li>Ext2 - Extended File System 2, Legacy Linux file system. There is no use for this on RHEL 8.</li>
</ul>
</li>
<li>
<p><strong>Journaling</strong>:  <em>Uses a journal to keep track of changes that have not been written to the file system. This provides some protection for file corruption during system crashes and unexpected shutdowns</em></p>
<ul>
<li>Ext3 - Previous version of Ext4. There is no use for this on RHEL 8.</li>
<li>Ext4 - The default file system in previous versions of RHEL. Still supported on RHEL 8.</li>
<li>XFS - The default RHEL 8 file system.</li>
<li>BTRFS:
<ul>
<li>Uses Copy on Write (CoW), a resource management technique.</li>
<li>Uses Subvolumes: Similar to a partition, but can be accessed like a directory.</li>
<li>Snapshots: Subvolumes that reference the original data&rsquo;s location, metadata and directory structure.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>File Allocation Table file systems</strong></p>
<ul>
<li>Linux can use VFAT (Virtual File Allocation TAble) which allows longer file names.</li>
<li>EFI Boot partitions <em>need</em> to use a FAT partition (VFAT on Linux)</li>
<li>ex-FAT - Extended FAT file system: Allows files larger than 2Gb.</li>
</ul>
</li>
</ul>
<pre><code>[root@server1 ~]# mkfs -t xfs /dev/vda3
meta-data=/dev/vda3              isize=512    agcount=4, agsize=131072 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=524288, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
</code></pre><p>Remember you can&rsquo;t format a file system onto an Extended partition, it contains the partition table for the Logical partitions.</p>
<pre><code>[root@server1 ~]# mkfs.xfs /dev/vda4
mkfs.xfs: /dev/vda4 appears to contain a partition table (dos).
mkfs.xfs: Use the -f option to force overwrite.
</code></pre><pre><code>[root@server1 ~]# mkfs.xfs /dev/vda5
meta-data=/dev/vda5              isize=512    agcount=4, agsize=262144 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=1048576, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
</code></pre><h2 id="changing-file-system-properties">Changing File System Properties</h2>
<p>File system properties can be managed using different tools specific for the file system you&rsquo;re using.</p>
<h3 id="managing-ext4-file-system-properties">Managing Ext4 File System Properties</h3>
<p>The generic tool for managing Ext4 file system properties is <strong>tune2fs</strong>. This tool was developped for Ext2 but is fully compatible with Ext3 and Ext4.</p>
<p><code>tune2fs -l</code> shows the different file system properties:</p>
<pre><code>[root@server1 ~]# tune2fs -l /dev/vdb1
tune2fs 1.44.6 (5-Mar-2019)
Filesystem volume name:   &lt;none&gt;
Last mounted on:          &lt;not available&gt;
Filesystem UUID:          492d646a-950d-49da-a3f3-62f0872a9f4b
Filesystem magic number:  0xEF53
Filesystem revision #:    1 (dynamic)
Filesystem features:      has_journal ext_attr resize_inode dir_index filetype extent 64bit flex_bg sparse_super large_file huge_file dir_nlink extra_isize metadata_csum
Filesystem flags:         signed_directory_hash 
Default mount options:    user_xattr acl
Filesystem state:         clean
Errors behavior:          Continue
Filesystem OS type:       Linux
Inode count:              655360
Block count:              2621179
Reserved block count:     131058
Free blocks:              2554426
Free inodes:              655349
First block:              0
Block size:               4096
Fragment size:            4096
Group descriptor size:    64
Reserved GDT blocks:      1024
Blocks per group:         32768
Fragments per group:      32768
Inodes per group:         8192
Inode blocks per group:   512
Flex block group size:    16
Filesystem created:       Fri Jul 31 20:51:26 2020
Last mount time:          n/a
Last write time:          Fri Jul 31 21:01:22 2020
Mount count:              0
Maximum mount count:      -1
Last checked:             Fri Jul 31 20:51:26 2020
Check interval:           0 (&lt;none&gt;)
Lifetime writes:          68 MB
Reserved blocks uid:      0 (user root)
Reserved blocks gid:      0 (group root)
First inode:              11
Inode size:           256
Required extra isize:     32
Desired extra isize:      32
Journal inode:            8
Default directory hash:   half_md4
Directory Hash Seed:      5613adcb-e707-4a6d-8559-53c1f238609d
Journal backup:           inode blocks
Checksum type:            crc32c
Checksum:                 0x4828a56a
</code></pre><p>Interesting properties are the file system label (showing as Filesystem volume name), File System features and Default mount option:</p>
<ul>
<li>Use <code>tune2fs -o</code> to set default file system mount option:
<ul>
<li><code>tune2fs -o acl,user_xatrr /dev/vdb1</code> to switch on access control lists and user extended attributes.</li>
<li><code>tune2fs -o ^acl,user_xattr /dev/vdb1</code> to switch off the same options.</li>
</ul>
</li>
<li>Use <code>tune2fs -O</code> to set or unset file system features:
<ul>
<li><code>tune2fs -O ^dir_index /dev/vdb1</code></li>
<li><code>tune2fs -O dir_index /dev/vdb1</code></li>
</ul>
</li>
<li>Use <code>tune2fs -L</code> or <code>e2label2</code> to set a file system label:
<ul>
<li><code>tune2fs -L MyData /dev/vdb1</code></li>
</ul>
</li>
<li>Use <code>tune2fs -i</code> to set file system checks intervals:
<ul>
<li><code>tune2fs -i 3w /dev/vdb1</code> sets the interval to 1814400 seconds, or 3 weeks.</li>
</ul>
</li>
</ul>
<blockquote>
<p>File system labels or volume names will come in handy when mounting file systems. We can use the label instead of the device name for consistent mounting, even if the underlying device name changes.</p>
</blockquote>
<h3 id="managing-xfs-file-system-properties">Managing XFS File System Properties</h3>
<p>The XFS file system is completely different, you can not set file system attributes within the file system metadata. You can however change some XFS properties, like the volume label, using the <code>xfs_admin</code> command:</p>
<pre><code>[root@server1 ~]# xfs_admin -L XFS_Disk /dev/vda3
writing all SBs
new label = &quot;XFS_Disk&quot;

[root@h ~]# blkid | grep vda3
/dev/vda3: LABEL=&quot;XFS_Disk&quot; UUID=&quot;1e235236-8dfa-42d3-9948-826df137780c&quot; TYPE=&quot;xfs&quot; PARTUUID=&quot;de6faae3-03&quot;

[root@server1 ~]# xfs_admin -l /dev/vda3
label = &quot;XFS_Disk&quot;
</code></pre><h2 id="adding-swap-files-and-partitions">Adding Swap Files and Partitions</h2>
<p>Using swap on Linux is a convenient way to improve kernel memory usage. If a shortage of physical RAM occurs, non-recently used memory pages can be moved to swap space to make more RAM available for other programs. However, intensive usage of swap space could indicate a potential problem, swap space should be closely monitored.</p>
<p>Swap space is either created by formatting a partition with the swap partition type, or creating a swap file and formatting the file as swap space. From a performance point of view it doesn&rsquo;t make much difference if a swap partition or a swap file is being used.
A swap file could be helpful if you need to increase swap space, but you don&rsquo;t have free disk space to create a partition.</p>
<p>For a swap partition, use <code>fdisk</code> or <code>gdisk</code> depending on the partition table:</p>
<pre><code>[root@server1 ~]# fdisk /dev/vda

Welcome to fdisk (util-linux 2.32.1).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.


Command (m for help): p
Disk /dev/vda: 30 GiB, 32212254720 bytes, 62914560 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xde6faae3

Device     Boot    Start      End  Sectors Size Id Type
/dev/vda1  *        2048 37750783 37748736  18G 83 Linux
/dev/vda2       37750784 41945087  4194304   2G 82 Linux swap / Solaris

Command (m for help): n
Partition type
   p   primary (2 primary, 0 extended, 2 free)
   e   extended (container for logical partitions)
Select (default p): p
Partition number (3,4, default 3): 
First sector (41945088-62914559, default 41945088): 
Last sector, +sectors or +size{K,M,G,T,P} (41945088-62914559, default 62914559): +2G

Created a new partition 3 of type 'Linux' and of size 2 GiB.

Command (m for help): t
Partition number (1-3, default 3): 
Hex code (type L to list all codes): 82

Changed type of partition 'Linux' to 'Linux swap / Solaris'.

Command (m for help): w
The partition table has been altered.
Syncing disks.

[root@server1 ~]# mkswap /dev/vda3
Setting up swapspace version 1, size = 2 GiB (2147479552 bytes)
no label, UUID=61fc53d7-f385-4c79-94bd-69b33010c09a

[root@server1 ~]# free -m
              total        used        free      shared  buff/cache   available
Mem:           1829        1015         191          21         622         642
Swap:          2047           0        2047

[root@server1 ~]# swapon /dev/vda3
[root@server1 ~]# free -m
              total        used        free      shared  buff/cache   available
Mem:           1829        1017         189          21         622         640
Swap:          4095           0        4095
[root@server1 ~]#

</code></pre><p>To add a swap file, create the file first. The below <code>dd</code> command creates a file containing 512 blocks of 1MiB containing all zeroes.</p>
<pre><code>[root@server1 ~]# dd if=/dev/zero of=/swapfile bs=1M count=512
512+0 records in
512+0 records out
536870912 bytes (537 MB, 512 MiB) copied, 6.28219 s, 85.5 MB/s

[root@server1 ~]# chmod 0600 /swapfile 
[root@server1 ~]# mkswap /swapfile
Setting up swapspace version 1, size = 512 MiB (536866816 bytes)
no label, UUID=0f2316f3-d713-4752-be6b-74584092c193

[root@server1 ~]# swapon /swapfile
[root@server1 ~]# free -m
              total        used        free      shared  buff/cache   available
Mem:           1829        1002         170          12         655         664
Swap:          4607          35        4572
</code></pre><h1 id="mounting-file-systems">Mounting File Systems</h1>
<p>To use a partition we have to mount it to make its content available through a specific directory.
In order to do so, we need specific information:</p>
<ul>
<li><strong>What</strong> - Mandatory information that specifies what device we want to mount.</li>
<li><strong>Where</strong> - Mandatory information that specifies the directory on wich we want to mount our device.</li>
<li><strong>File System</strong> - Optional. Typically the <strong>mount</strong> command will detect the proper file system.</li>
<li><strong>Options</strong> - Optional but depends on the needs you have for the file system.</li>
</ul>
<p>To mount a file system, the <strong>mount</strong> command is used, to unmount a file system the <strong>umount</strong> command is used:</p>
<pre><code>[root@server1 ~]# mkdir /ext4dir
[root@server1 ~]# mount /dev/vdb1 /ext4dir

[root@server1 ~]# lsblk
NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
sr0     11:0    1   7G  0 rom  
vda    253:0    0  30G  0 disk 
├─vda1 253:1    0  18G  0 part /
├─vda2 253:2    0   2G  0 part [SWAP]
└─vda3 253:3    0   2G  0 part [SWAP]
vdb    253:16   0  10G  0 disk 
└─vdb1 253:17   0  10G  0 part /ext4dir

[root@server1 ~]# umount /ext4dir 
[root@server1 ~]# lsblk
NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
sr0     11:0    1   7G  0 rom  
vda    253:0    0  30G  0 disk 
├─vda1 253:1    0  18G  0 part /
├─vda2 253:2    0   2G  0 part [SWAP]
└─vda3 253:3    0   2G  0 part [SWAP]
vdb    253:16   0  10G  0 disk 
└─vdb1 253:17   0  10G  0 part 
[root@server1 ~]#
</code></pre><p>Note that for unmounting, both <code>umount /dev/vdb1</code> or <code>umount /ext4dir</code> would work.</p>
<h2 id="device-names-uuids-or-disk-labels">Device Names, UUIDs, or Disk Labels</h2>
<p>Typically we use device names like <code>/dev/vdb1</code> to mount devices. If you&rsquo;re in a environment where a dynamic storage topology is being used this is not a good approach, since device names may change after e.g. a server reboot.</p>
<p>On a default RHEL 8 installation, Universally Unique Identifiers are used for every file system. This may not be handy when manually mounting, but does make sense when automating file system mounts. Before the use of UUIDs was common, file systems were often mounted via their Volume Name or Disk Label. Both the UUID and the Disk Label can be seen using the <strong>blkid</strong> command:</p>
<pre><code>[root@server1 ~]# blkid
/dev/vda1: UUID=&quot;658aed1c-058c-42a6-bce1-ffcf41426b53&quot; TYPE=&quot;xfs&quot; PARTUUID=&quot;de6faae3-01&quot;
/dev/vda2: UUID=&quot;c8ad4b20-2469-4caf-aa30-1a99d16870d9&quot; TYPE=&quot;swap&quot; PARTUUID=&quot;de6faae3-02&quot;
/dev/vda3: UUID=&quot;61fc53d7-f385-4c79-94bd-69b33010c09a&quot; TYPE=&quot;swap&quot; PARTUUID=&quot;de6faae3-03&quot;
/dev/vdb1: LABEL=&quot;ext4disk&quot; UUID=&quot;492d646a-950d-49da-a3f3-62f0872a9f4b&quot; TYPE=&quot;ext4&quot; PARTLABEL=&quot;Linux filesystem&quot; PARTUUID=&quot;e8063de5-2db0-455b-a50b-a3b2f69e857c&quot;

[root@server1 ~]# mount UUID=&quot;492d646a-950d-49da-a3f3-62f0872a9f4b&quot; /ext4dir
[root@server1 ~]# umount /ext4dir
[root@server1 ~]# mount LABEL=&quot;ext4disk&quot; /ext4dir
[root@server1 ~]#
</code></pre><h2 id="automating-file-system-mounts-through-etcfstab">Automating File System Mounts Through /etc/fstab</h2>
<p>Usually you would want to mount file systems automatically, the classical way of doing this is through the <code>/etc/fstab</code> file. This file specifies everything needed to mount file systems:</p>
<pre><code>[root@server1 ~]# cat /etc/fstab

#
# /etc/fstab
# Created by anaconda on Mon Jun 22 03:30:44 2020
#
# Accessible filesystems, by reference, are maintained under '/dev/disk/'.
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info.
#
# After editing this file, run 'systemctl daemon-reload' to update systemd
# units generated from this file.
#
UUID=658aed1c-058c-42a6-bce1-ffcf41426b53 /                       xfs     defaults        0 0
UUID=c8ad4b20-2469-4caf-aa30-1a99d16870d9 swap                    swap    defaults        0 0
</code></pre><p>Each line in the file constains six fields:</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Device</td>
<td>A device name, UUID or label.</td>
</tr>
<tr>
<td>Mount Point</td>
<td>A directory or kernel interface where the device needs to be mounted.</td>
</tr>
<tr>
<td>File System</td>
<td>The file system type.</td>
</tr>
<tr>
<td>Mount Options</td>
<td>The mount options applied to the file system.</td>
</tr>
<tr>
<td>Dump Support</td>
<td>Set to 1 to enable support using the <strong>dump</strong> utility. Necessary for some backup solutions.</td>
</tr>
<tr>
<td>Automatic Check</td>
<td>File system integrity check. <strong>0</strong> to disable automated checks, <strong>1</strong> if this is the root file system to be checked automatically, <strong>2</strong> for all other file systems that need automatic checking while booting. Network file systems should have this option set to <strong>0</strong>.</td>
</tr>
</tbody>
</table>

<blockquote>
<p>The <strong>xfs</strong> file system does not support file system checks, so in this case the Automatic Check field should be set to <code>0</code>.</p>
</blockquote>
<p>Not all file systems use a directory as a mount point, e.g. system devices like swap use a kernel interface. You can easily recognize kernel interfaces, their name doesn&rsquo;t start with a <code>/</code> like a directory (and it doesn&rsquo;t exist in the file system).</p>
<p>The Mount Options field defines specific moutn options, if no options are required then this field will read &ldquo;defaults&rdquo;. The following common mount options can be used:</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Option</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>auto/noauto</td>
<td>The file system will (not) be mounted automatically.</td>
</tr>
<tr>
<td>acl</td>
<td>Adds support for Access Control Lists.</td>
</tr>
<tr>
<td>user_xattr</td>
<td>Adds support for user-extended attributes.</td>
</tr>
<tr>
<td>ro</td>
<td>Mounts the file system in read only mode.</td>
</tr>
<tr>
<td>atime/noatime</td>
<td>Enables or disables access time modifications.</td>
</tr>
<tr>
<td>exec/noexec</td>
<td>Allows or denies execution of program files from the file system.</td>
</tr>
<tr>
<td>_netdev</td>
<td>To mount a network file system. This tells fstab to wait until the network is available before mounting.</td>
</tr>
</tbody>
</table>

<pre><code>[root@localhost ~]# cat /etc/fstab 

#
# /etc/fstab
# Created by anaconda on Mon Jun 22 03:30:44 2020
#
# Accessible filesystems, by reference, are maintained under '/dev/disk/'.
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info.
#
# After editing this file, run 'systemctl daemon-reload' to update systemd
# units generated from this file.
#
UUID=658aed1c-058c-42a6-bce1-ffcf41426b53 /                       xfs     defaults        0 0
UUID=c8ad4b20-2469-4caf-aa30-1a99d16870d9 swap                    swap    defaults        0 0
UUID=492d646a-950d-49da-a3f3-62f0872a9f4b /ext4dir  ext4  defaults  0 0
/swapfile swap  swap  defaults  0 0
</code></pre><pre><code>[root@localhost ~]# mount -a
[root@localhost ~]# lsblk
NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
sr0     11:0    1   7G  0 rom  /run/media/student/CentOS-8-1-1911-x86_64-dvd
vda    253:0    0  30G  0 disk 
├─vda1 253:1    0  18G  0 part /
├─vda2 253:2    0   2G  0 part [SWAP]
└─vda3 253:3    0   2G  0 part [SWAP]
vdb    253:16   0  10G  0 disk 
└─vdb1 253:17   0  10G  0 part /ext4dir
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Managing Storage: Creating Partitions</title>
            <link>https://joerismissaert.dev/managing-storage-creating-partitions/</link>
            <pubDate>Wed, 29 Jul 2020 16:05:02 +0400</pubDate>
            
            <guid>https://joerismissaert.dev/managing-storage-creating-partitions/</guid>
            <description>To match the different partition types we use different partitioning utilities. The fdisk utlity is used to create MBR partitions while the gdisk utility is used to create GPT paritions.
Apart from fdisk and gdisk, there is the parted command which can create both MBR and GPT partitions but has less advanced features.
Each command takes a disk device name as an argument. The device names are usually /dev/sda, /dev/sdb, &amp;hellip; in the order the device is recognized by the kernel.</description>
            <content type="html"><![CDATA[
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />


<p>To match the different partition types we use different partitioning utilities.
The <code>fdisk</code> utlity is used to create MBR partitions while the <code>gdisk</code> utility is used to create GPT paritions.</p>
<p>Apart from <code>fdisk</code> and <code>gdisk</code>, there is the <code>parted</code> command which can create both MBR and GPT partitions but has less advanced features.</p>
<p>Each command takes a disk device name as an argument. The device names are usually <code>/dev/sda</code>, <code>/dev/sdb</code>, &hellip; in the order the device is recognized by the kernel. You can have disks up to <code>/dev/sdz</code> and beyond: <code>/dev/sdaa</code>, <code>/dev/sdab</code>, &hellip;</p>
<p>Partitions are numbered, the <code>/dev/sda</code> device contains partitions like <code>/dev/sda1</code> , <code>/dev/sda2</code>, &hellip;</p>
<p>The name of the device also depends on the type of driver that&rsquo;s used:</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Device Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>/dev/sda</td>
<td>Devices that use the SCSI driver, used for both SCSI and SATA devices. This is common on physical machines but also on VMware virtual machines.</td>
</tr>
<tr>
<td>/dev/nvme0n1</td>
<td>The first hard disk on an NVME Express interface. Note that the first drive is referred to as <code>n1</code> instead of <code>a</code>.</td>
</tr>
<tr>
<td>/dev/hda</td>
<td>Legacy IDE disk devices.</td>
</tr>
<tr>
<td>/dev/vda</td>
<td>Common on KVM virtual machines using the virtio disk driver.</td>
</tr>
<tr>
<td>/dev/xvda</td>
<td>A disk in a Xen virtual machine that uses the Xen virtual disk driver.</td>
</tr>
</tbody>
</table>

<h2 id="creating-mbr-partitions-with-fdisk">Creating MBR Partitions with fdisk</h2>
<p>Next, I will show you how to use <code>fdisk</code> to create a partition on nonpartitioned disk space. If you don&rsquo;t have nonpartitioned disk space the same principle can be applied to a separate virtual disk. Make sure to create a snapshot of your virtual machine so you can easily revert back.</p>
<p>The <code>lsblk</code> command shows me I have two disks: <code>vda</code> and <code>vdb</code>, where <code>vda</code> contains two partitions:</p>
<pre><code>[root@server1 ~]# lsblk
NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
sr0     11:0    1   7G  0 rom  
vda    253:0    0  30G  0 disk 
├─vda1 253:1    0  18G  0 part /
└─vda2 253:2    0   2G  0 part [SWAP]
vdb    253:16   0  10G  0 disk 
</code></pre><p><code>df -h</code> shows me the available disk space on each disk, for <code>/dev/vda</code> that would be 7.2Gb. There&rsquo;s no file system yet on <code>/dev/vdb</code> so it&rsquo;s not showing anything for this disk.</p>
<pre><code>[root@server1 ~]# df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        899M     0  899M   0% /dev
tmpfs           915M     0  915M   0% /dev/shm
tmpfs           915M  1.6M  914M   1% /run
tmpfs           915M     0  915M   0% /sys/fs/cgroup
/dev/vda1        18G   11G  7.2G  61% /
tmpfs           183M   12K  183M   1% /run/user/42
tmpfs           183M   28K  183M   1% /run/user/1000
</code></pre><p>Let&rsquo;s run the <code>fdisk</code> command against the <code>/dev/vda</code> disk and print out the current disk allocation:</p>
<pre><code>[root@server1 ~]# fdisk /dev/vda

Welcome to fdisk (util-linux 2.32.1).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.


Command (m for help): p
Disk /dev/vda: 30 GiB, 32212254720 bytes, 62914560 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xde6faae3

Device     Boot    Start      End  Sectors Size Id Type
/dev/vda1  *        2048 37750783 37748736  18G 83 Linux
/dev/vda2       37750784 41945087  4194304   2G 82 Linux swap / Solaris

Command (m for help): 
</code></pre><p>We can see that this disk has 62914560 sectors and that the last partition does not end on the last sector
This confirms again we have free disk space available to create a new partition.</p>
<p>Type <code>n</code> to create a new partition, followed by <code>p</code> to create a primary partition. You can choose the new parition number or hit enter to accept the default.</p>
<pre><code>Command (m for help): n
Partition type
   p   primary (2 primary, 0 extended, 2 free)
   e   extended (container for logical partitions)
Select (default p): p
Partition number (3,4, default 3): 3
</code></pre><p><code>fdisk</code> will suggest the first available sector, you can accept that by hitting enter.
For the last sector (which will ultimately determine the size), you can choose to accept the default to create a partition on all remaining disk space, or you can specify the last sector (using <code>+numberofsectors</code>) or specify a size using <code>+number(K,M,G)</code>, i.e. <code>+1G</code> will make the partition 1GiB in size.</p>
<pre><code>First sector (41945088-62914559, default 41945088): 
Last sector, +sectors or +size{K,M,G,T,P} (41945088-62914559, default 62914559): +1G

Created a new partition 3 of type 'Linux' and of size 1 GiB.
</code></pre><p>By default a Linux parition type is used. You can change the type using the <code>t</code> command. In this case we&rsquo;ll continue to use the Linux type.</p>
<p>Use the <code>p</code> command to print the disk allocation again:</p>
<pre><code>Command (m for help): p
Disk /dev/vda: 30 GiB, 32212254720 bytes, 62914560 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xde6faae3

Device     Boot    Start      End  Sectors Size Id Type
/dev/vda1  *        2048 37750783 37748736  18G 83 Linux
/dev/vda2       37750784 41945087  4194304   2G 82 Linux swap / Solaris
/dev/vda3       41945088 44042239  2097152   1G 83 Linux
</code></pre><p>Once we&rsquo;re happy with the modifications, we use <code>w</code> to write them to disk and exit <code>fdisk</code>.<br>
(Type <code>q</code> at anytime if you need to quit <code>fdisk</code> without writing your changes.)</p>
<pre><code>Command (m for help): w
The partition table has been altered.
Syncing disks.
</code></pre><p>It&rsquo;s possible you&rsquo;ll receive the following warning message:</p>
<pre><code>WARNING: Re-reading the partition table failed with error 16: Device or resource busy.
The kernel still uses the old table. The new table will be used at the next reboot or after you run partprobe(8) or kpartx(8).
</code></pre><p>This means the partition table was written succesfully, but the in-memory kernel partition could not be updated.
You can check this by comparing the output of <code>fdisk -l /dev/vda</code> with <code>cat /proc/partitions</code>.<br>
Run <code>partprobe /dev/vda</code> to write the changes to the in-memory kernel parition table.
Typically this happens when you add partitions to a disk that already has mounted partitions.</p>
<h3 id="using-extended-and-logical-partitions-on-mbr">Using Extended and Logical Partitions on MBR</h3>
<p>If three partitions have been created already, there is room for 1 more primary partition. If you need to go beyond 4 partitions on an MBR disk you will have to create an extended partition and create logical partitions within it.</p>
<p>If something goes wrong with your extended partition you will have a problem with all logical partitions as well. With this in mind you might be better off using LVM, which I&rsquo;ll discuss later on in a different post.</p>
<blockquote>
<p>An extended partition is used only for the purpose of creating logical partitions. Consider it as a container for the logical partitions, you can&rsquo;t create a filesystem directly on the extended partition.</p>
</blockquote>
<pre><code>[root@server1 ~]# fdisk /dev/vda

Welcome to fdisk (util-linux 2.32.1).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.


Command (m for help): p
Disk /dev/vda: 30 GiB, 32212254720 bytes, 62914560 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xde6faae3

Device     Boot    Start      End  Sectors Size Id Type
/dev/vda1  *        2048 37750783 37748736  18G 83 Linux
/dev/vda2       37750784 41945087  4194304   2G 82 Linux swap / Solaris
/dev/vda3       41945088 44042239  2097152   1G 83 Linux

Command (m for help): n
Partition type
   p   primary (3 primary, 0 extended, 1 free)
   e   extended (container for logical partitions)
Select (default e): e

Selected partition 4
First sector (44042240-62914559, default 44042240): 
Last sector, +sectors or +size{K,M,G,T,P} (44042240-62914559, default 62914559): 

Created a new partition 4 of type 'Extended' and of size 9 GiB.

Command (m for help): p
Disk /dev/vda: 30 GiB, 32212254720 bytes, 62914560 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xde6faae3

Device     Boot    Start      End  Sectors Size Id Type
/dev/vda1  *        2048 37750783 37748736  18G 83 Linux
/dev/vda2       37750784 41945087  4194304   2G 82 Linux swap / Solaris
/dev/vda3       41945088 44042239  2097152   1G 83 Linux
/dev/vda4       44042240 62914559 18872320   9G  5 Extended

Command (m for help): n
All primary partitions are in use.
Adding logical partition 5
First sector (44044288-62914559, default 44044288): 
Last sector, +sectors or +size{K,M,G,T,P} (44044288-62914559, default 62914559): +5G

Created a new partition 5 of type 'Linux' and of size 5 GiB.

Command (m for help): n
All primary partitions are in use.
Adding logical partition 6
First sector (54532096-62914559, default 54532096): 
Last sector, +sectors or +size{K,M,G,T,P} (54532096-62914559, default 62914559): 

Created a new partition 6 of type 'Linux' and of size 4 GiB.

Command (m for help): p
Disk /dev/vda: 30 GiB, 32212254720 bytes, 62914560 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xde6faae3

Device     Boot    Start      End  Sectors Size Id Type
/dev/vda1  *        2048 37750783 37748736  18G 83 Linux
/dev/vda2       37750784 41945087  4194304   2G 82 Linux swap / Solaris
/dev/vda3       41945088 44042239  2097152   1G 83 Linux
/dev/vda4       44042240 62914559 18872320   9G  5 Extended
/dev/vda5       44044288 54530047 10485760   5G 83 Linux
/dev/vda6       54532096 62914559  8382464   4G 83 Linux

Command (m for help): w
The partition table has been altered.
Failed to add partition 5 to system: Device or resource busy
Failed to add partition 6 to system: Device or resource busy

The kernel still uses the old partitions. The new table will be used at the next reboot. 
Syncing disks.

[root@server1 ~]# fdisk -l /dev/vda
Disk /dev/vda: 30 GiB, 32212254720 bytes, 62914560 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xde6faae3

Device     Boot    Start      End  Sectors Size Id Type
/dev/vda1  *        2048 37750783 37748736  18G 83 Linux
/dev/vda2       37750784 41945087  4194304   2G 82 Linux swap / Solaris
/dev/vda3       41945088 44042239  2097152   1G 83 Linux
/dev/vda4       44042240 62914559 18872320   9G  5 Extended
/dev/vda5       44044288 54530047 10485760   5G 83 Linux
/dev/vda6       54532096 62914559  8382464   4G 83 Linux


[root@server1 ~]# cat /proc/partitions
major minor  #blocks  name

 253        0   31457280 vda
 253        1   18874368 vda1
 253        2    2097152 vda2
 253        3    1048576 vda3
 253        4    9436160 vda4
 253       16   10485760 vdb
  11        0    7377920 sr0
[root@server1 ~]# partprobe /dev/vda
Error: Partition(s) 5, 6 on /dev/vda have been written, but we have been unable to inform the kernel of the change, probably because it/they are in use.  As a result, the old partition(s) will remain in use.  You should reboot now before making further changes.
[root@server1 ~]# reboot
</code></pre><blockquote>
<p>Since we&rsquo;re getting an error using partprobe, we <strong>should</strong> reboot and not continue modifying or managing partitions.</p>
</blockquote>
<h2 id="creating-gpt-partitions-with-gdisk">Creating GPT Partitions with gdisk</h2>
<p>If a disk is configured with a GUID Partition Table or if the disk has a size that goes beyond 2TiB you need to manage partitions with the <code>gdisk</code> utility.</p>
<blockquote>
<p>Never use <code>gdisk</code> on a disk that has been formatted with <code>fdisk</code> and already contains <code>fdisk</code> partitions. <code>gdisk</code> will detect MBR is present and convert this to GPT after which your computer will likely not boot anymore.</p>
</blockquote>
<pre><code>[root@server1 ~]# gdisk /dev/vda
GPT fdisk (gdisk) version 1.0.3

Partition table scan:
  MBR: MBR only
  BSD: not present
  APM: not present
  GPT: not present


***************************************************************
Found invalid GPT and valid MBR; converting MBR to GPT format
in memory. THIS OPERATION IS POTENTIALLY DESTRUCTIVE! Exit by
typing 'q' if you don't want to convert your MBR partitions
to GPT format!
***************************************************************


Warning! Secondary partition table overlaps the last partition by
33 blocks!
You will need to delete this partition or resize it in another utility.

Command (? for help): q
</code></pre><p>In the following example, I&rsquo;ll create a GPT layout on a new disk, <code>/dev/vdb</code>:</p>
<pre><code>[root@server1 ~]# gdisk /dev/vdb
GPT fdisk (gdisk) version 1.0.3

Partition table scan:
  MBR: not present
  BSD: not present
  APM: not present
  GPT: not present

Creating new GPT entries.
</code></pre><p>Type <code>p</code> to print the current disk allocation:</p>
<pre><code>Command (? for help): p
Disk /dev/vdb: 20971520 sectors, 10.0 GiB
Sector size (logical/physical): 512/512 bytes
Disk identifier (GUID): 32689440-34F8-4D19-8FCC-0FA78AFFCAF5
Partition table holds up to 128 entries
Main partition table begins at sector 2 and ends at sector 33
First usable sector is 34, last usable sector is 20971486
Partitions will be aligned on 2048-sector boundaries
Total free space is 20971453 sectors (10.0 GiB)

Number  Start (sector)    End (sector)  Size       Code  Name

</code></pre><p>Type <code>n</code> to create a new partition, accept the default partition number that is suggested.
Accept the default suggested First sector.
The last sector should be set at 1GiB.</p>
<p>By default the Linux partition type is selected. Accept or type <code>l</code> to show other parition types.
Relevant partition types are as follows:</p>
<ul>
<li><strong>8200</strong> Linux Swap</li>
<li><strong>8300</strong> Linux File System</li>
<li><strong>8e00</strong> Linux LVM</li>
</ul>
<p>These are the same types as the ones used in MBR, except that two 0s are added to the ID.</p>
<pre><code>Command (? for help): n
Partition number (1-128, default 1): 
First sector (34-20971486, default = 2048) or {+-}size{KMGTP}: 
Last sector (2048-20971486, default = 20971486) or {+-}size{KMGTP}: +1G
Current type is 'Linux filesystem'
Hex code or GUID (L to show codes, Enter = 8300): 
Changed type of partition to 'Linux filesystem'
</code></pre><p>Check the partition by printing the disk allocation:</p>
<pre><code>Command (? for help): p
Disk /dev/vdb: 20971520 sectors, 10.0 GiB
Sector size (logical/physical): 512/512 bytes
Disk identifier (GUID): 32689440-34F8-4D19-8FCC-0FA78AFFCAF5
Partition table holds up to 128 entries
Main partition table begins at sector 2 and ends at sector 33
First usable sector is 34, last usable sector is 20971486
Partitions will be aligned on 2048-sector boundaries
Total free space is 18874301 sectors (9.0 GiB)

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048         2099199   1024.0 MiB  8300  Linux filesystem
</code></pre><p>Type <code>w</code> to write your changes.</p>
<pre><code>Command (? for help): w

Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTING
PARTITIONS!!

Do you want to proceed? (Y/N): y
OK; writing new GUID partition table (GPT) to /dev/vdb.
The operation has completed successfully.
</code></pre><p>Again, if you get an error message indicating that the partition table is in use, type <code>partprobe</code> to update the kernel partition table and reboot if required.</p>
<pre><code>[root@server1 ~]# lsblk
NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
sr0     11:0    1   7G  0 rom  
vda    253:0    0  30G  0 disk 
├─vda1 253:1    0  18G  0 part /
├─vda2 253:2    0   2G  0 part [SWAP]
├─vda3 253:3    0   1G  0 part 
├─vda4 253:4    0   1K  0 part 
├─vda5 253:5    0   5G  0 part 
└─vda6 253:6    0   4G  0 part 
vdb    253:16   0  10G  0 disk 
└─vdb1 253:17   0   1G  0 part 
</code></pre><pre><code>[root@server1 ~]# cat /proc/partitions 
major minor  #blocks  name

 253        0   31457280 vda
 253        1   18874368 vda1
 253        2    2097152 vda2
 253        3    1048576 vda3
 253        4          1 vda4
 253        5    5242880 vda5
 253        6    4191232 vda6
 253       16   10485760 vdb
 253       17    1048576 vdb1
  11        0    7377920 sr0
</code></pre><h2 id="creating-gpt-partitions-with-parted">Creating GPT Partitions with parted</h2>
<p>The <code>parted</code> utility uses an interactive shell, it&rsquo;s considered to be the default utility on RHEL 8 but it lacks advanced features.</p>
<p>I&rsquo;ve deleted the partion I created previously with <code>gdisk</code> and I&rsquo;ll recreate it using <code>parted</code>:</p>
<pre><code>[root@server1 ~]# parted /dev/vdb
GNU Parted 3.2
Using /dev/vdb
Welcome to GNU Parted! Type 'help' to view a list of commands.
</code></pre><p><code>print</code> the current disk allocation table:</p>
<pre><code>(parted) print                                                                
Model: Virtio Block Device (virtblk)
Disk /dev/vdb: 10.7GB
Sector size (logical/physical): 512B/512B
Partition Table: gpt
Disk Flags: 

Number  Start  End  Size  File system  Name  Flags


</code></pre><p>Type <code>mklabel</code> and press Enter. You&rsquo;ll be prompted for a disk label type, press the Tab key twice to see a list of available disk label types. Choose the <code>gpt</code> type and press Enter.</p>
<pre><code>(parted) mklabel                                                          
New disk label type?                                                      
aix    amiga  atari  bsd    dvh    gpt    loop   mac    msdos  pc98   sun    
New disk label type? gpt
Warning: The existing disk label on /dev/vdb will be destroyed and all data on this disk will be lost. Do you want to continue?
Yes/No? yes                                                               
</code></pre><p>Type <code>mkpart</code>, the utility prompts for a partition name. I&rsquo;ve named by partition <em>backup</em>.</p>
<pre><code>(parted) mkpart                                                           
Partition name?  []? backup
</code></pre><p>Notice you&rsquo;re prompted for a file system type, it suggest we&rsquo;re applying a file system here but this is <em><strong>not</strong></em> the case.
Documentation suggest this setting isn&rsquo;t used, you could accept the default, but it&rsquo;s suggested to use Tab completion and choose a file system type comes close to what you&rsquo;re going to use on the partition later when actually creating the file system.</p>
<pre><code>File system type?  [ext2]?                                                
affs0            affs3            affs6            amufs0           amufs3           apfs1            btrfs            ext4             hfs              hp-ufs           linux-swap(new)  linux-swap(v1)   reiserfs         xfs              
affs1            affs4            affs7            amufs1           amufs4           apfs2            ext2             fat16            hfs+             jfs              linux-swap(old)  nilfs2           sun-ufs          
affs2            affs5            amufs            amufs2           amufs5           asfs             ext3             fat32            hfsx             linux-swap       linux-swap(v0)   ntfs             swsusp           
File system type?  [ext2]? xfs
</code></pre><p>We can specify the start location as a number of blocks or an offset from the start of the device like <code>1MiB</code>, not <code>+1MiB</code>.
We&rsquo;ll make the partition 5GiB in size, so we specify the <code>5Gib</code> offset for the end value.</p>
<pre><code>Start? 1MiB
End? 5GiB
</code></pre><p>Type <code>print</code> to see the modifications and <code>quit</code> to quit the utility and commit changes.</p>
<pre><code>(parted) print                                                            
Model: Virtio Block Device (virtblk)
Disk /dev/vdb: 10.7GB
Sector size (logical/physical): 512B/512B
Partition Table: gpt
Disk Flags: 

Number  Start   End     Size    File system  Name    Flags
 1      1049kB  5369MB  5368MB  xfs          backup

(parted) quit                                                             
Information: You may need to update /etc/fstab.
</code></pre><pre><code>[root@server1 ~]# lsblk
NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
sr0     11:0    1   7G  0 rom  
vda    253:0    0  30G  0 disk 
├─vda1 253:1    0  18G  0 part /
├─vda2 253:2    0   2G  0 part [SWAP]
├─vda3 253:3    0   1G  0 part 
├─vda4 253:4    0   1K  0 part 
├─vda5 253:5    0   5G  0 part 
└─vda6 253:6    0   4G  0 part 
vdb    253:16   0  10G  0 disk 
└─vdb1 253:17   0   5G  0 part 
[root@server1 ~]#
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Managing Storage: Understanding MBR and GPT Partitions</title>
            <link>https://joerismissaert.dev/managing-storage-understanding-mbr-and-gpt-partitions/</link>
            <pubDate>Tue, 28 Jul 2020 17:45:50 +0400</pubDate>
            
            <guid>https://joerismissaert.dev/managing-storage-understanding-mbr-and-gpt-partitions/</guid>
            <description>If a mass storage device is connected to a Linux computer, the Linux kernel tries to locate any partitions. So to use a hard drive we need to partition it. On RHEL 8 two different partitioning schemes are available: the Master Boot Record and GUID Partition Table. Linux typically has multiple partitions on one hard disk, this makes sense for different reasons:
 Distinguish different types of data. Mount options to enhance security or performance.</description>
            <content type="html"><![CDATA[
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />


<p>If a mass storage device is connected to a Linux computer, the Linux kernel tries to locate any partitions. So to use a hard drive we need to partition it. On RHEL 8 two different partitioning schemes are available: the Master Boot Record and GUID Partition Table. Linux typically has multiple partitions on one hard disk, this makes sense for different reasons:</p>
<ul>
<li>Distinguish different types of data.</li>
<li>Mount options to enhance security or performance.</li>
<li>Create backup strategies where only relevant portions of the OS are backed up.</li>
<li>If one partition accidentally fills up completely other partitions are still usable and your system might not crash immediately.</li>
</ul>
<h2 id="the-mbr-partitioning-scheme">The MBR Partitioning Scheme</h2>
<p>In the early 1980s the Master Boot Record partitioning scheme was invented to define hard disk layout.
On BIOS-based systems the BIOS searches for an operating system on the bootable disk device where the first 512 bytes of the boot medium is read. The MBR is defined as these first 512 bytes which contains an operating system boot loader (446 bytes) and a partition table (64 bytes). The size used for the partition table, means no more than 4 partions can be created with a maximum size of 2 TiB per partition. The last 2 bytes (0x55, 0xaa) of the first 512 byte sector signifies the device is bootable. These two bytes are also call the &ldquo;magic number&rdquo;.</p>
<p>To go beyond the limit of 4 partitions, one partition could be created as an extended partition (as opposed to a primary partition) where multiple logical partitions can be created within to reach a total of 15 partitions addressable by the Linux kernel.</p>

    <img src="/img/MBR.png"  alt="Master Boot Record"  class="center"  />


<h2 id="the-gpt-partitioning-scheme">The GPT Partitioning Scheme</h2>
<p>Current hard drives have become too big to be addressed by MBR partitions, the GUID Partition Table counters this. On computers that use the new Unified Extensible Firware Interface (UEFI) instead of BIOS, GPT partitions are the only way to address disks. Systems using BIOS can use GUID partitions and must do so if a disk bigger than 2 TiB needs to be addressed. Although not being used by UEFI devices, the MBR is present at the beginning of the disk, in block LBA0, for protective and compatibility purposes. Strictly speaking, the GPT starts up from the Partition Table Header.</p>
<p>The benefits of using GUID:</p>
<ul>
<li>a 8 ZiB partition size (1024^4).</li>
<li>Maximum 128 partitions.</li>
<li>No distinguishment between primary, extended and logical partitions.</li>
<li>Uses 128-bit global unique identifier (GUID) to identify partitions.</li>
<li>A backup of the GUID partition table is created at the end of the disk (Secondary GPT Header), eliminating the single point of failure of MBR partition tables.</li>
</ul>

    <img src="/img/GPT.png"  alt="GUID Partition Table Scheme"  class="center"  />


<h2 id="storage-measurement-units">Storage Measurement Units</h2>
<p>Different measurement units can be used, we typically differentiate between binary (power of 2) and decimal (power of 10) measurement units. Decimal units use a multiple of 1,000 bytes while binary units use a multiple of 1,024 bytes.</p>
<p>When speaking of partitions using binary units, the starting point of the partition is aligned to the exact sector specified by size and the ending point is aligned to the specified size minus 1 sector.
If using decimal units, the starting and ending point is aligned within one half of the specified unit: for example, ±500KB when using the MB suffix.</p>
<p>In computers it makes sense to use binary units because that&rsquo;s how computers address items, the binary unit has become the standard on current Linux distribution although some utilities may provide their output in decimal units.</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Symbol</th>
<th>Name</th>
<th>Value</th>
<th>Symbol</th>
<th>Name</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>KB</td>
<td>Kilobyte</td>
<td>1000^1</td>
<td>KiB</td>
<td>Kibibyte</td>
<td>1024^1</td>
</tr>
<tr>
<td>MB</td>
<td>Megabyte</td>
<td>1000^2</td>
<td>KiB</td>
<td>Mebibyte</td>
<td>1024^2</td>
</tr>
<tr>
<td>GB</td>
<td>Gigabyte</td>
<td>1000^3</td>
<td>KiB</td>
<td>Gibibyte</td>
<td>1024^4</td>
</tr>
<tr>
<td>TB</td>
<td>Terabyte</td>
<td>1000^4</td>
<td>KiB</td>
<td>Tebibyte</td>
<td>1024^5</td>
</tr>
<tr>
<td>PB</td>
<td>Petabyte</td>
<td>1000^5</td>
<td>KiB</td>
<td>Pebibyte</td>
<td>1024^6</td>
</tr>
<tr>
<td>EB</td>
<td>Exabyte</td>
<td>1000^6</td>
<td>KiB</td>
<td>Exbibyte</td>
<td>1024^7</td>
</tr>
<tr>
<td>ZB</td>
<td>Zettabyte</td>
<td>1000^7</td>
<td>KiB</td>
<td>Zebibyte</td>
<td>1024^8</td>
</tr>
<tr>
<td>YB</td>
<td>Yottabyte</td>
<td>1000^8</td>
<td>KiB</td>
<td>Yobibyte</td>
<td>1024^9</td>
</tr>
</tbody>
</table>

]]></content>
        </item>
        
        <item>
            <title>Understanding and Configuring System Logging</title>
            <link>https://joerismissaert.dev/understanding-and-configuring-system-logging/</link>
            <pubDate>Wed, 15 Jul 2020 20:06:32 +0400</pubDate>
            
            <guid>https://joerismissaert.dev/understanding-and-configuring-system-logging/</guid>
            <description>Understanding System Logging Services on a Linux server write information to log files in different destinations using different approaches, and there are multiple ways to find relevant information in those log files.
Services can write log information via:
 Direct Write - information is written directly to a log file by a service. rsyslogd - a service passes log information to the rsyslogd service that in turn takes care of managing centralized log files.</description>
            <content type="html"><![CDATA[
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />


<h2 id="understanding-system-logging">Understanding System Logging</h2>
<p>Services on a Linux server write information to log files in different destinations using different approaches, and there are multiple ways to find relevant information in those log files.</p>
<p>Services can write log information via:</p>
<ul>
<li><strong>Direct Write</strong> - information is written directly to a log file by a service.</li>
<li><strong>rsyslogd</strong> - a service passes log information to the <code>rsyslogd</code> service that in turn takes care of managing centralized log files.</li>
<li><strong>journald</strong> - this service is tightly integrated with Systemd.</li>
</ul>
<p><code>journald</code> receives all messages generated by Systemd Units. It also collects messages from the kernel, the boot procedure and services, then writes these message to an event journal which is stored in binary format. This journal can be queried with the <code>journalctl</code> command.</p>
<p><code>journald</code> is not persistent between reboots, so messages are also forwarded to <code>rsyslogd</code> which in turn writes the log information to different files in the <code>/var/log</code> directory and thus making them persistent.</p>
<p><code>rsyslogd</code> adds some services to <code>journald</code>, it allows you to configure remote logging and log servers, and makes logging persistent.</p>
<p>We also have the <code>auditd</code> service which offers an in-depth trace of what specific services, processes and users have been doing.</p>
<p>We the above in mind, we have three approaches to get more information on what has been happening on a machine:</p>
<ul>
<li>Monitor the files in <code>/var/log</code> written by <code>rsyslogd</code> or by services directly.</li>
<li>Use the <code>journalctl</code> command.</li>
<li>Use the <code>systemctl status</code> command.</li>
</ul>
<h3 id="persistent-log-files">Persistent Log Files</h3>
<p>As mentioned earlier, persistent log files are located in the <code>/var/log</code> directory. Most of the files are managed by <code>rsyslogd</code> but some of them are created directly by services. The most convenient way of reading these files would be by using a pager like <code>less</code>.</p>
<p>The below table provides an overview of some of the standard files that are created in this directory and what content you may expect.</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Log File</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td>/var/log/messages</td>
<td>A generic log file where most messages are written to.</td>
</tr>
<tr>
<td>/var/log/dmesg</td>
<td>Contains kernel log messages.</td>
</tr>
<tr>
<td>/var/log/secure</td>
<td>Contains authentication-related messages.</td>
</tr>
<tr>
<td>/var/log/boot.log</td>
<td>Messages related to system startup</td>
</tr>
<tr>
<td>/var/log/audit/audit.log</td>
<td>Contains audit messages. SELinux writes to this file.</td>
</tr>
<tr>
<td>/var/log/maillog</td>
<td>Mail-related messages</td>
</tr>
<tr>
<td>/var/log/samba</td>
<td>The Samba service writes directly to this file, it&rsquo;s not managed through <code>rsyslogd</code>.</td>
</tr>
<tr>
<td>/var/log/sssd</td>
<td>Messages written by the <code>sssd</code> service, which plays an important role in the authentication process.</td>
</tr>
<tr>
<td>/var/log/cups</td>
<td>Messages generated the the CUPS print service.</td>
</tr>
<tr>
<td>/var/log/httpd</td>
<td>Directory that contains log files written directly by the Apache web server.</td>
</tr>
</tbody>
</table>

<p>It might be useful to see what is happening in real time. The <code>tail -f &lt;logfile&gt;</code> command would allow you to live monitor a specific log file, you&rsquo;ll see lines being added to the output of the command as system events happen.</p>
<h3 id="using-logger">Using logger</h3>
<p>While services write information to log files by themselves or through <code>rsyslogd</code>, users can write messages to <code>rsyslogd</code> from the command line or a script using the <code>logger</code> command. Simply type <code>logger</code> followed by the message you want to write to the logs.</p>
<p>You can also specify the priority and facility to log to. For example, <code>logger -p kern.err hello</code> would write <code>hello</code> to the kernel facility using the error priority.</p>
<h2 id="configuring-rsyslogd">Configuring rsyslogd</h2>
<p>The <code>rsyslogd</code> service makes sure that information that needs to be logged is written to the location where you would want to find it.
This is configured inside the <code>/etc/rsyslog.conf</code> file that acts as the central location from where <code>rsyslogd</code> is configured. From this file the <code>/etc/rsyslog.d/</code> directory is included, RPM packages can put specific configuration files inside this directory.</p>
<p>If you need to pass specific options to the rsyslogd service <em>on startup</em> you need to do that via the <code>/etc/sysconfig/ryslog</code> file. This file contains one line by default where you can specify startup parameters: <code>SYSLOGD_OPTIONS=&quot;&quot;</code>
This variable is included in the Systemd configuration file that starts rsyslogd.</p>
<h3 id="understanding-rsyslogd-configuration-files">Understanding rsyslogd Configuration Files</h3>
<p>The <code>rsyslogd.conf</code> file contains different sections to specify what should be logged and where:</p>
<ul>
<li>
<p><strong>### MODULES ###</strong>: rsyslogd is modular and you can expand features using modules.</p>
</li>
<li>
<p><strong>### GLOBAL DIRECTIVES ###</strong>: Specified global parameters like the default timestamp format or the location where files are being written to.</p>
</li>
<li>
<p><strong>### RULES ###</strong>: This is the most important part, it contains rules that specify which information should be logged to which destination.</p>
</li>
</ul>
<pre><code>#### RULES ####

# Log all kernel messages to the console.
# Logging much else clutters up the screen.
#kern.*                                                 /dev/console

# Log anything (except mail) of level info or higher.
# Don't log private authentication messages!
*.info;mail.none;authpriv.none;cron.none                /var/log/messages

# The authpriv file has restricted access.
authpriv.*                                              /var/log/secure

# Log all the mail messages in one place.
mail.*                                                  -/var/log/maillog


# Log cron stuff
cron.*                                                  /var/log/cron

# Everybody gets emergency messages
*.emerg                                                 :omusrmsg:*

# Save news errors of level crit and higher in a special file.
uucp,news.crit                                          /var/log/spooler

# Save boot messages also to boot.log
local7.*                                                /var/log/boot.log
</code></pre><h4 id="facilities-priorities-and-destinations">Facilities, Priorities and Destinations</h4>
<p>The Rules section we discussed above uses facilities, priorities and destinations to specify what should  be logged and where.</p>
<ul>
<li>A <em>facility</em> specifies a category of information. There is a fixed list of facilities that cannot be extended to ensure backwards compatibility. As a result some facilities exist that serve no purpose anymore and some services that became relevant at a later stage do not have their own facility. To circumvent that you can configure the service to use the generic <code>daemon</code> and <code>local0</code> to <code>local7</code> facilities.</li>
</ul>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Facility</th>
<th>Used by</th>
</tr>
</thead>
<tbody>
<tr>
<td>auth/authpriv</td>
<td>Messages related to authentication</td>
</tr>
<tr>
<td>cron</td>
<td>Messages generated by the crond service</td>
</tr>
<tr>
<td>daemon</td>
<td>Generic facility to be used for nonspecified daemons</td>
</tr>
<tr>
<td>kern</td>
<td>Kernel messages</td>
</tr>
<tr>
<td>lpr</td>
<td>Messages generated through the legacy lpd print system</td>
</tr>
<tr>
<td>mail</td>
<td>Email related messages</td>
</tr>
<tr>
<td>mark</td>
<td>Special facility that can be used to periodically write a marker</td>
</tr>
<tr>
<td>news</td>
<td>Messages generated by the NNTP news system</td>
</tr>
<tr>
<td>security</td>
<td>Deprecated, same as auth/authpriv</td>
</tr>
<tr>
<td>syslog</td>
<td>Messages generated by the syslog system</td>
</tr>
<tr>
<td>user</td>
<td>Messages generated in user space</td>
</tr>
<tr>
<td>uucp</td>
<td>Messages generated by the legacy UUCP system</td>
</tr>
<tr>
<td>local0-7</td>
<td>Messages generated by services configured to use one of the local0 to local7 facilities</td>
</tr>
</tbody>
</table>

<ul>
<li><em>Priorities</em> define the severity of the message that needs to be logged. When specifying a priority, all messages with that specific priority and higher will be logged.<br>
If you need to configure logging where messages with different priorities are sent to different files you can specify the priority with a <code>=</code> sign:
<code>cron.=debug -/var/log/cron.debug</code>
This will write all cron message with only the debug priority to the <code>/var/log/cron.debug</code> log.</li>
</ul>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Priority</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>emerg/panic</td>
<td>Message generated when the availability of the service is discontinued</td>
</tr>
<tr>
<td>alert</td>
<td>Used when the availability of the service is about to be discontinued</td>
</tr>
<tr>
<td>crit</td>
<td>A critical error has occurred</td>
</tr>
<tr>
<td>error/err</td>
<td>A noncritical error has occurred</td>
</tr>
<tr>
<td>warning/warn</td>
<td>Something is suboptimal, but no real error yet</td>
</tr>
<tr>
<td>notice</td>
<td>Messages about items that might become an issue later</td>
</tr>
<tr>
<td>info</td>
<td>Informational messages about normal service operation</td>
</tr>
<tr>
<td>debug</td>
<td>Debug messages that will give as much information as possible about service operation</td>
</tr>
</tbody>
</table>

<ul>
<li><em>Destinations</em> defines where the message should be written to. Typically these are files, but they can be devices or ryslog modules as well. If the destination file name starts with a hypen, e.g. <code>-/var/log/maillog</code>, the message will not be commited immediately to the file. Instead, it will be buffered to make writes more efficient.</li>
</ul>
<p>See <strong>man 5 rsyslog.conf</strong></p>
<h2 id="rotating-log-files">Rotating Log Files</h2>
<p>When a certain treshold has been reached the old log file is closed and a new log file is opened by the logrotate utility which is started periodically by the crond service.</p>
<p>When a log file is rotated, the old log file is typically copied to a new file that has the rotation date in it. By default, four old log files are kept on the system, files older than that period are removed from the system automatically.</p>
<blockquote>
<p>You should back up log files or configure a centralized log server where logrotate keeps rotated messages for a significanlty longer period.</p>
</blockquote>
<p>The default settings for log rotation are kep in the <code>/etc/logrotate.conf</code> file, but you can create configuration files inside the <code>/etc/logrotate.d/</code> directory to be applied to specific log files.</p>
<pre><code># see &quot;man logrotate&quot; for details
# rotate log files weekly
weekly

# keep 4 weeks worth of backlogs
rotate 4

# create new (empty) log files after rotating old ones
create

# use date as a suffix of the rotated file
dateext

# uncomment this if you want your log files compressed
#compress

# packages drop log rotation information into this directory
include /etc/logrotate.d

# system-specific logs may be also be configured here.
</code></pre><h2 id="working-with-journald">Working with journald</h2>
<p>The systemd-journald service stores log messages in a binary file, the journal, <code>/run/log/journal</code>. Since this file is in the <code>/run</code> directory, it is not persistent.</p>
<p>We can examine the journal using the <code>journalctl</code> command, the output is shown in a <code>less</code> pager and by default you&rsquo;ll see the beginning of the journal. You can live monitor the journal using the <code>journcalctl -f</code> command.</p>
<p><code>journalctl</code> has many filtering options, you can find some of the most useful in the table below.
You can also use tab completion to see specific options.</p>
<p>




<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>journalctl --no-pager</code></td>
<td>Shows the content without using a pager</td>
</tr>
<tr>
<td><code>journalctl -f</code></td>
<td>Live monitor the journal as events happen</td>
</tr>
<tr>
<td><code>journalctl _UID=1000</code></td>
<td>Show messages that have been logged for a specific user</td>
</tr>
<tr>
<td><code>journalctl -p err</code></td>
<td>Show messages with the <em>error</em> priority</td>
</tr>
<tr>
<td><code>journalctl --since yesterday --until today</code></td>
<td>Shows messages in a specific time frame. Accepts keywords like <em>yesterday</em>, <em>today</em>, <em>tomorrow</em> or you can specify a date/time in the <code>YYYY-MM-DD hh:mm:ss</code> format.</td>
</tr>
<tr>
<td><code>journalctl _SYSTEMD_UNIT=sshd.service</code></td>
<td>Shows information on the sshd Systemd Unit.</td>
</tr>
<tr>
<td><code>journalctl --dmesg</code></td>
<td>Shows kernel related messages</td>
</tr>
<tr>
<td><code>journalctl -o verbose</code></td>
<td>Shows detailed information on logged items</td>
</tr>
</tbody>
</table>
<br>
The above options can all be combined, e.g. <code>journalctl --since yesterday -p err _UID=1000</code> or <code>journalctl _SYSTEMD_UNIT=sshd.service -o verbose</code>.</p>
<h3 id="persistently-storing-the-systemd-journal">Persistently Storing the Systemd Journal</h3>
<p>Storing the journal in a persistent way so that it survives reboots requires setting the <code>Storage=</code> parameter inside <code>/etc/systemd/journal.conf</code>:</p>
<ul>
<li><strong>Storage=auto</strong> The journal will be written to disk if the <code>/var/log/journal</code> <em>directory</em> exists.</li>
<li><strong>Storage=volatile</strong> The journal will only be stored in <code>/run/log/journal</code> in a non persistent way.</li>
<li><strong>Storage=persistent</strong> The journal will be written to disk in the <code>/var/log/journal</code> directory, if this directory does not exist it will be created.</li>
<li><strong>Storage=none</strong> No data will be stored, but forwarding to other targets like the kernel log buffer or syslog will still work.</li>
</ul>
<p>If you manually created the <code>/var/log/journal</code> directory you need to set ownership and permissions, then either reboot or kill the <code>systemd-journald</code> service. Restarting the <code>systemd-journald</code> service is not enough:</p>
<pre><code># chown root:systemd-journal /var/log/journal
# chmod 2755 /var/log/journal
# killall -USR1 systemd-journald
</code></pre><p>The journal has built-in log rotation, so even if the journal is stored persistently the data is not kept forever.
The journal is limited to a maximum size of 10% of the file system it&rsquo;s on and it will stop growing when less than 15% of file system is free. When this happens the oldest messages from the journal are dropped to make room for new messages.
This can be changed in the <code>/etc/systemd/journal.conf</code> file.</p>
]]></content>
        </item>
        
        <item>
            <title>Scheduling Tasks with Cron, At and Anacron</title>
            <link>https://joerismissaert.dev/scheduling-tasks-with-cron-at-anacron/</link>
            <pubDate>Tue, 07 Jul 2020 20:57:41 +0400</pubDate>
            
            <guid>https://joerismissaert.dev/scheduling-tasks-with-cron-at-anacron/</guid>
            <description>Configuring Cron to Automate Recurring Tasks. On Linux, the cron service is used to run processes automatically at specific times as a way to automate tasks that have to occur regularly.
The cron service consists of two major components:
 The cron daemon checks every minute to see if there are any jobs to run. The cron configuration files work together to provide the right information to the right service at the right time.</description>
            <content type="html"><![CDATA[
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />


<h2 id="configuring-cron-to-automate-recurring-tasks">Configuring Cron to Automate Recurring Tasks.</h2>
<p>On Linux, the cron service is used to run processes automatically at specific times as a way to automate tasks that have to occur regularly.</p>
<p>The cron service consists of two major components:</p>
<ul>
<li><em>The cron daemon</em> checks every minute to see if there are any jobs to run.</li>
<li><em>The cron configuration files</em> work together to provide the right information to the right service at the right time.</li>
</ul>
<h3 id="managing-the-crond-service">Managing the crond Service</h3>
<p>Because some system tasks run through the <code>crond</code> service, this service is started  by default. The <code>crond</code> service itself doesn&rsquo;t need much management, it does not need to be reloaded or activated, the <code>crond</code> daemon wakes up every minute to see if anything needs to be run. You can monitor the current status of the service using the <code>systemctl status crond -l</code> command:</p>
<pre><code>[root@server1 ~]# systemctl status crond -l
● crond.service - Command Scheduler
   Loaded: loaded (/usr/lib/systemd/system/crond.service; enabled; vendor preset: enabled)
   Active: active (running) since Sat 2020-07-04 14:22:31 +04; 3 days ago
 Main PID: 9134 (crond)
    Tasks: 1 (limit: 49648)
   Memory: 3.4M
   CGroup: /system.slice/crond.service
           └─9134 /usr/sbin/crond -n

Jul 07 12:01:01 server1 CROND[56917]: (root) CMD (run-parts /etc/cron.hourly)
Jul 07 13:01:01 server1 CROND[57001]: (root) CMD (run-parts /etc/cron.hourly)
Jul 07 14:01:01 server1 CROND[57303]: (root) CMD (run-parts /etc/cron.hourly)
Jul 07 15:01:01 server1 CROND[57551]: (root) CMD (run-parts /etc/cron.hourly)
Jul 07 16:01:01 server1 CROND[57582]: (root) CMD (run-parts /etc/cron.hourly)
Jul 07 17:01:01 server1 CROND[57621]: (root) CMD (run-parts /etc/cron.hourly)
Jul 07 18:01:01 server1 CROND[57715]: (root) CMD (run-parts /etc/cron.hourly)
Jul 07 19:01:01 server1 CROND[57807]: (root) CMD (run-parts /etc/cron.hourly)
Jul 07 19:01:01 server1 run-parts[57816]: (/etc/cron.hourly) finished 0anacron
Jul 07 20:01:01 server1 CROND[57845]: (root) CMD (run-parts /etc/cron.hourly)

</code></pre><p>The <code>systemctl</code> command uses the <code>journald</code> service to find out what is happening with the crond service.</p>
<h3 id="cron-timing">Cron Timing</h3>
<p>A time string is used to specify when exactly a specific job should be run.
The following cron time and date fields can be used for this time string:</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Field</th>
<th>Values</th>
</tr>
</thead>
<tbody>
<tr>
<td>minute</td>
<td>0 - 59</td>
</tr>
<tr>
<td>hour</td>
<td>0 - 23</td>
</tr>
<tr>
<td>day of month</td>
<td>1 - 31</td>
</tr>
<tr>
<td>month</td>
<td>1 - 12</td>
</tr>
<tr>
<td>day of week</td>
<td>0 - 7 (Sunday is 0 or 7)</td>
</tr>
</tbody>
</table>

<p>In any of the above fields you can use an <code>*</code> as a wildcard to refer to any value, ranges of numbers, lists and patterns are also allowed:</p>
<ul>
<li><code>* 11 * * *</code> - Every minute between 11:00 and 11:59</li>
<li><code>0 11 * * 1-5</code> - Every weekday at 11:00</li>
<li><code>0 7-18 * * 1-5</code> - Every weekday between 7a.m. and 6p.m. at the top of the hour</li>
<li><code>0 */2 2 12 5</code> - Every 2 hours at the top of the hour, on the 2nd of December and every Friday in December.</li>
</ul>
<pre><code># For details see man 4 crontabs

# Example of job definition:
# .---------------- minute (0 - 59)
# |  .------------- hour (0 - 23)
# |  |  .---------- day of month (1 - 31)
# |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...
# |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
# |  |  |  |  |
# *  *  *  *  * user-name  command to be executed
</code></pre><h3 id="cron-configuration-files">Cron Configuration Files</h3>
<p>The main configuration file for cron is <code>/etc/crontab</code>, but you will not change this file directly. Different Cron configuration files are used:</p>
<ul>
<li>Cron files in <code>/etc/cron.d</code></li>
<li>Scripts in <code>/etc/cron.hourly</code>, <code>cron.daily</code>, <code>cron.weekly</code> and <code>cron.monthly</code></li>
<li>User-specific files created with <code>crontab -e</code></li>
</ul>
<p>Cron jobs can be created for specific users by either logging in as that user and executing <code>crontab -e</code> or executing <code>crontab -e -u USERNAME</code> as root.
<code>crontab -e</code> opens the <code>vi</code> editor and creates a temporary file. After saving your changes, the temporary file is moved to <code>/var/spool/cron</code> where a file is created for each user. These files should not be edited directly.</p>
<p>You can also add Cron jobs that are not tied to a specific user account, these will be executed by default as root unless you specify otherwise.
To do so you add a file where the content meets the syntax of a typical cron job inside the <code>/etc/cron.d</code> directory, the file name does not matter.</p>
<p>The last way to schedule Cron jobs is through the following directories:</p>
<ul>
<li><code>/etc/cron.hourly</code></li>
<li><code>/etc/cron.daily</code></li>
<li><code>/etc/cron.weekly</code></li>
<li><code>/etc/cron.monthly</code></li>
</ul>
<p>Scripts added to these directories should not contain any information on when it should be executed. You would only add scripts there if the exact time of execution does not really matter, the only thing that would matter is if the job needs to be launched once an hour, day, week or month.</p>
<h3 id="the-purpose-of-anacron">The Purpose of Anacron</h3>
<p>Anacron is the service that takes care of starting the hourly, daily, weekly and monthly jobs regardless of the exact time.
Anacron uses the <code>/etc/anacrontab</code> file for this:</p>
<pre><code>[root@server1 cron.d]# cat /etc/anacrontab 
# /etc/anacrontab: configuration file for anacron

# See anacron(8) and anacrontab(5) for details.

SHELL=/bin/sh
PATH=/sbin:/bin:/usr/sbin:/usr/bin
MAILTO=root
# the maximal random delay added to the base delay of the jobs
RANDOM_DELAY=45
# the jobs will be started during the following hours only
START_HOURS_RANGE=3-22

#period in days   delay in minutes   job-identifier   command
1	5	cron.daily		nice run-parts /etc/cron.daily
7	25	cron.weekly		nice run-parts /etc/cron.weekly
@monthly 45	cron.monthly		nice run-parts /etc/cron.monthly
</code></pre><p>From the above we can see that Anacron will only run jobs between 3a.m. and 10p.m.<br>
The <em>period in days</em> specifies the job execution frequency, <em>delay in minutes</em> specifies how long Anacron waits before executing the job. Then we have the job identifier <code>cron.daily</code> and the command that&rsquo;s being executed (<em>nice run-parts /etc/cron.daily</em>)</p>
<blockquote>
<p>The need to configure jobs through Anacron is taken away by the <code>/etc/cron.hourly</code>, <code>cron.daily</code>, <code>cron.weekly</code> and <code>cron.monthly</code> directories.</p>
</blockquote>
<p>Note that there is no single command that would show all currently scheduled jobs. The <code>crontab -l</code> command does list cron jobs, but only for the current user.</p>
<h3 id="cron-security">Cron Security</h3>
<p>By default, all users are allowed to create Cron jobs.<br>
To limit which user is allowed or not allowed to create Cron jobs we use the <code>/etc/cron.allow</code> and <code>/etc/cron.deny</code> files:</p>
<ul>
<li>If the <code>cron.allow</code> file exists, a user must be listed in the file to be allowed to use Cron.</li>
<li>If the <code>cron.deny</code> file exists, a user must <strong>not</strong> be listed in the file to be allowed to use cron.</li>
</ul>
<blockquote>
<p>Both files should not exist at the same time.<br>
If neither file exists, only root can use Cron.</p>
</blockquote>
<h2 id="configuring-at-to-schedule-future-tasks">Configuring At to Schedule Future Tasks</h2>
<p>The <code>atd</code> service is used for jobs that need to be executed only once and are thus not recurring.
You can use the <code>at</code> command followed by the time the job needs the be executed, either a specific time as in <code>at 14:00</code> or a time indication like <code>at noon</code> or <code>at teatime</code>.</p>
<p>After typing the <code>at</code> command a shell opens where you can type several commands that will be executed on at the time you specified, press <code>CTRL-D</code> to leave the shell.</p>
<p>The <code>atq</code> command lists an overview of all currently scheduled jobs. You can remove a specific job using the <code>atrm</code> command followed by the job number.</p>
<blockquote>
<p>A load value can be specified when starting the <code>atd</code> service using the <code>-l</code> option.<br>
e.g. <code>atd -l 3.0</code> will make sure no job is started when the system load is higher than 3.0</p>
</blockquote>
<pre><code>[student@server1 ~]$ at noon
warning: commands will be executed using /bin/sh
at&gt; echo &quot;It's noon&quot;
at&gt; &lt;EOT&gt;
job 41 at Wed Jul 08 12:00:00 2020
[student@server1 ~]$ atq
41	Wed Jul 08 12:00:00 2020 a student
[student@server1 ~]$ atrm 41
[student@server1 ~]$ atq
[student@server1 ~]$ 

</code></pre><p>Instead of specifying commands to be executed interactively from the prompt, we can instruct <code>at</code> to execute an existing script or program simply by passing it as an argument to the -f flag (or by using input redirection):</p>
<pre><code>[student@server1 ~]$ at now + 1 minute -f script.sh
warning: commands will be executed using /bin/sh
job 42 at Wed Jul 08 12:02:00 2020
[student@server1 ~]$ 

[student@server1 ~]$ at now + 1 minute &lt; at_jobs.txt
warning: commands will be executed using /bin/sh
job 43 at Wed Jul 08 12:05:00 2020
[student@server1 ~]$ 
</code></pre>]]></content>
        </item>
        
        <item>
            <title>An Introduction to Systemd and Systemd Unit files</title>
            <link>https://joerismissaert.dev/an-introduction-to-systemd-and-systemd-unit-files/</link>
            <pubDate>Thu, 25 Jun 2020 12:04:28 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/an-introduction-to-systemd-and-systemd-unit-files/</guid>
            <description>Systemd is considered to be the future standard init system by all mainstream Linux distributions. The Systemd System and Service Manager uses &amp;ldquo;units&amp;rdquo; as an abstraction for parts of the system to be managed: typically services, sockets, mounts and targets. It provides a uniform interface for managing units.
The systemctl -t help command displays a list of available units that can be managed with Systemd.
[student@server1 ~]$ systemctl -t help Available unit types: service mount swap socket target device automount timer path slice scope Each unit type can be recognized by the file extension.</description>
            <content type="html"><![CDATA[
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />


<p>Systemd is considered to be the future standard init system by all mainstream Linux distributions. The Systemd System and Service Manager uses &ldquo;units&rdquo; as an abstraction for parts of the system to be managed: typically services, sockets, mounts and targets. It provides a uniform interface for managing units.</p>
<p>The <strong>systemctl -t help</strong> command displays a list of available units that can be managed with Systemd.</p>
<pre><code>[student@server1 ~]$ systemctl -t help
Available unit types:
service
mount
swap
socket
target
device
automount
timer
path
slice
scope
</code></pre><p>Each unit type can be recognized by the file extension. A service unit will end on <code>.service</code> while a target unit will end on <code>.target</code>.</p>
<p>Unit files are located in three different locations:</p>
<ul>
<li>
<p><code>/usr/lib/systemd/system</code> contains default unit files that have been installed by RPM packages. These should not be edited since package updates can overwrite your changes.</p>
</li>
<li>
<p><code>/etc/systemd/system</code> contains custom unit files written by a system administrator or generated by the <strong>systemctl edit</strong> command.</p>
</li>
<li>
<p><code>/run/systemd/system</code> contains unit files that have been generated automatically by the system during runtime.</p>
</li>
</ul>
<p>The above locations have a specific priority if a unit file happens to exist in more than one location. Units in the <code>/run</code> directory have the highest priority, followed by custom unit files in the <code>/etc/systemd/system</code> directory and lastly the files inside <code>/usr/lib/systemd/system</code>.</p>
<h2 id="systemd-service-units">Systemd Service Units</h2>
<p>The most important unit type is the service unit. You can start any type of process by using a service unit, including daemon processes and regular commands.</p>
<p>Let&rsquo;s take a look at the service unit file for the Very Secure Ftp Daemon:</p>
<pre><code>[root@server1 ~]# systemctl cat vsftpd
# /usr/lib/systemd/system/vsftpd.service
[Unit]
Description=Vsftpd ftp daemon
After=network.target

[Service]
Type=forking
ExecStart=/usr/sbin/vsftpd /etc/vsftpd/vsftpd.conf

[Install]
WantedBy=multi-user.target
</code></pre><p>We can identify three sections in this service unit file:</p>
<ul>
<li>
<p><strong>[Unit]</strong> This sections describes the unit and dependencies (more on that later). It contains the important <strong>After</strong> statement and optional <strong>Before</strong> statement. These statements define dependencies between different units and how they relate from the perspective of this unit. In the above example, the <strong>After</strong> statement indicates that this unit should be started after the <code>network.target</code> unit.</p>
</li>
<li>
<p><strong>[Service]</strong> describes how to start and stop the unit. Usually you will see the <strong>ExecStart</strong> statement to indicate how to start the unit and the <strong>ExecStop</strong> statement for how to stop the unit. The <strong>Type</strong> statement is used to specify how the process should start. The <code>forking</code> type is commonly used by daemon processes.</p>
</li>
<li>
<p><strong>[Install]</strong> indicates in which <code>target</code> unit this service has to start. We&rsquo;ll cover more on target units later.</p>
</li>
</ul>
<p>For more detailed information on the above, see <strong>man 5 systemd.service</strong>.</p>
<h2 id="systemd-mount-units">Systemd Mount Units</h2>
<p>A mount unit specifies how a file system can be mounted on a specific directory. Configuring mount points through <code>/etc/fstab</code> is the preferred approach, any mount points configured in the <code>/etc/fstab</code> file will be converted to a Systemd mount unit at boot time.</p>
<pre><code>[root@server1 ~]# systemctl cat tmp.mount
[Unit]
Description=Temporary Directory (/tmp)
Documentation=https://systemd.io/TEMPORARY_DIRECTORIES
Documentation=man:file-hierarchy(7)
Documentation=https://www.freedesktop.org/wiki/Software/systemd/APIFileSystems
ConditionPathIsSymbolicLink=!/tmp
DefaultDependencies=no
Conflicts=umount.target
Before=local-fs.target umount.target
After=swap.target

[Mount]
What=tmpfs
Where=/tmp
Type=tmpfs
Options=mode=1777,strictatime,nosuid,nodev&lt;/code&gt;&lt;/pre&gt;
</code></pre><p>We typically see the following options inside the <strong>[Mount]</strong> section:</p>
<ul>
<li>
<p><strong>What</strong> takes an absolute path of a file or device node. This is a mandatory option.</p>
</li>
<li>
<p><strong>Where</strong> defines the absolute path for the mount point, which cannot be a symbolic link. If it does not exist at the time of mounting, it is created as a directory. This is a mandatory option as well and the string must reflect the unit filename.</p>
</li>
<li>
<p><strong>Type</strong> defines the file system type. This is optional.</p>
</li>
</ul>
<p>See <strong>man 5 systemd.mount</strong> for more details.</p>
<h2 id="systemd-socket-units">Systemd Socket Units</h2>
<p>A socket creates a method for applications to communicate either by means of a file or a TCP/UDP port on which Systemd will be listening for incoming connections.</p>
<p>This means a specific service doesn&rsquo;t have to be running continuously, if Systemd detects an incoming connection on the socket it can start the associated service (on-demand starting). With that in mind, each socket unit <em>must</em> have a corresponding service unit file.</p>
<pre><code>[root@server1 ~]# systemctl cat cockpit.socket
# /usr/lib/systemd/system/cockpit.socket
[Unit]
Description=Cockpit Web Service Socket
Documentation=man:cockpit-ws(8)
Wants=cockpit-motd.service

[Socket]
ListenStream=9090
ExecStartPost=-/usr/share/cockpit/motd/update-motd '' localhost
ExecStartPost=-/bin/ln -snf active.motd /run/cockpit/motd
ExecStopPost=-/bin/ln -snf /usr/share/cockpit/motd/inactive.motd /run/cockpit/motd

[Install]
WantedBy=sockets.target
</code></pre><p>The important option in the above example is <em><strong>ListenStream</strong></em>. This options specifies the address and/or <strong>TCP</strong> port number to listen on. It can be written as e.g. <code>192.168.0.1:9090</code> or <code>9090</code> or <code>/path/to/file.socket</code> or <code>[ipv6]:port</code>number</p>
<p>For UDP ports you would use <em><strong>ListenDatagram</strong></em> instead.</p>
<p>See <strong>man 5 systemd.socket</strong> for more information.</p>
<h2 id="systemd-target-units">Systemd Target Units</h2>
<p>A target unit makes it possible to load unit files in a specific order to define the state the machine should be started in, very similar to the runlevels used in other init systems. A target unit is basically a group of units, they don&rsquo;t have additional functionality on top of the units they group.</p>
<pre><code>[root@server1 ~]# systemctl cat multi-user.target
# /usr/lib/systemd/system/multi-user.target

[Unit]
Description=Multi-User System
Documentation=man:systemd.special(7)
Requires=basic.target
Conflicts=rescue.service rescue.target
After=basic.target rescue.service rescue.target
AllowIsolate=yes
</code></pre><p>The target unit has definitions on what it requires and what other targets it cannot coexist with (Conflicts). It defines load ordering by using the <strong>After</strong> option.</p>
<p>When using the <strong>systemctl enable</strong> command on a unit to automatically start it at boot time, the <strong>[Install]</strong> section of that unit determines to what target unit it should be added. Behind the scenes, the <strong>systemctl enable</strong> command creates a symbolic link in the target directory (defined by [Install]) inside <code>/etc/systemd/system</code>.</p>
<p>For example, <strong>systemctl enable vsftpd</strong> adds a symbolic link in <code>/etc/systemd/system/multi-user.target/wants/vsftpd.service</code> which points to <code>/usr/lib/systemd/system/vsftpd.service</code> ensuring the <code>vsftpd.service</code> unit will be started automatically with the Multi-User System target. This symbolic link is called a <em><strong>want</strong></em>, it defines what the target wants to start when it is processed.</p>
<pre><code>[root@server1 ~]# systemctl enable vsftpd
Created symlink /etc/systemd/system/multi-user.target.wants/vsftpd.service → /usr/lib/systemd/system/vsftpd.service.  
</code></pre><h2 id="using-systemd-to-manage-units">Using Systemd to Manage Units</h2>
<p>You should&rsquo;ve noticed we can use the <strong>systemctl</strong> command to manage Systemd units. <strong>systemctl start/stop</strong> will start/stop a unit, while <strong>systemctl enable/disable</strong> will either start the unit at boot or prevent the unit from starting at boot.</p>
<p>The <strong>systemctl status</strong> command shows us runtime status information on the unit as well as the most recent journal log data:</p>
<pre><code>[root@server1 ~]# systemctl status vsftpd
● vsftpd.service - Vsftpd ftp daemon
     Loaded: loaded (/usr/lib/systemd/system/vsftpd.service; disabled; vendor preset: disabled)
     Active: active (running) since Thu 2020-06-25 14:27:13 +04; 1s ago
    Process: 66451 ExecStart=/usr/sbin/vsftpd /etc/vsftpd/vsftpd.conf (code=exited, status=0/SUCCESS)
   Main PID: 66452 (vsftpd)
      Tasks: 1 (limit: 28430)
     Memory: 636.0K
        CPU: 4ms
     CGroup: /system.slice/vsftpd.service
             └─66452 /usr/sbin/vsftpd /etc/vsftpd/vsftpd.conf

Jun 25 14:27:13 DELLG5 systemd&amp;#91;1]: Starting Vsftpd ftp daemon...
Jun 25 14:27:13 DELLG5 systemd&amp;#91;1]: Started Vsftpd ftp daemon.
</code></pre><p>The second line shows us the unit has been loaded into memory, but is not enabled at boot. The status of <strong>Loaded</strong> can also be <code>error</code>, <code>not-found</code>, <code>bad-setting</code>, <code>masked</code> or <code>static</code>.</p>
<p>The <strong>Active</strong> line shows the current state:</p>
<ul>
<li><em><strong>running</strong></em>: The unit is running with active processes</li>
<li><em><strong>exited</strong></em>: A one time run was successfully completed.</li>
<li><em><strong>waiting</strong></em>: The unit is running and waiting for events.</li>
<li><em><strong>inactive (dead)</strong></em>: The unit is not running</li>
</ul>
<p>In addition to the <strong>systemctl status</strong> command for a specific unit, the following commands can help you get a bigger picture of unit statuses:</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>systemctl –type=service</td>
<td>Show service units</td>
</tr>
<tr>
<td>systemctl list-units –type-service</td>
<td>idem</td>
</tr>
<tr>
<td>systemctl list-units –type=service –all</td>
<td>Show active and inactive service units</td>
</tr>
<tr>
<td>systemctl –failed –type=service</td>
<td>Show failed services</td>
</tr>
</tbody>
</table>

<h2 id="managing-systemd-dependencies">Managing Systemd Dependencies</h2>
<p>Unit files can have dependencies, a unit file may <strong>Want</strong>, <strong>Require</strong> or <strong>Requisite</strong> one or more other units <strong>After</strong> or <strong>Before</strong> it can run. These keywords can be used in the <strong>[Unit]</strong> section of a unit file.</p>
<ul>
<li><em><strong>Requires:</strong></em> If this unit loads, then units specified here will also load. Deactivating either unit will result in both units being deactivated.</li>
<li><em><strong>Requisite:</strong></em> If the unit listed here is not already loaded, then the unit will fail.</li>
<li><em><strong>Wants:</strong></em> The unit will try to load units specified here, but it will not fail if any of the specified units do fail.</li>
<li><em><strong>Before:</strong></em> The unit will start before the unit listed here.</li>
<li><em><strong>After:</strong></em> The unit will start after the unit listed here.</li>
</ul>
<p>You can request a list of unit dependencies using the <strong>systemctl list-dependencies</strong> command. Adding a unit to the command will show dependencies for that specific unit, and adding the <strong>- -reverse</strong> flag will show what units are dependent on that specific unit.</p>
<pre><code>[root@server1 ~]# systemctl list-dependencies vsftpd
vsftpd.service
● ├─system.slice
● └─sysinit.target
●   ├─dev-hugepages.mount
●   ├─dev-mqueue.mount
●   ├─dmraid-activation.service
....

[oot@server1 ~]# systemctl list-dependencies multi-user.target
multi-user.target
● ├─abrt-journal-core.service
● ├─abrt-oops.service
....

[root@server1 ~]# systemctl list-dependencies --reverse multi-user.target
multi-user.target
● └─graphical.target
</code></pre><h3 id="managing-unit-options">Managing Unit Options</h3>
<p>There&rsquo;s a huge amount of options available for each unit file, to see what options (and their default values) are available for a specific unit use the <strong>systemctl show</strong> command.</p>
<p>Changes to unit files need to be written to the <code>/etc/systemd/system</code> directory which is where custom unit files are located. The recommended way is to do so by using the <strong>systemctl edit</strong> command on a unit, this will create an override file for the unit and store that in the correct location. All settings in this file overwrite existing settings in <code>/usr/lib/systemd/system</code>.</p>
<p>By default, <strong>systemctl edit</strong> will use the <strong>nano</strong> editor, but you can change the default file editor by including the following in <code>~/.bash_profile</code> or <code>/etc/bash_profile</code> to change this system wide:<br>
<code>export SYSTEMD_EDITOR=&quot;/bin/vim&quot;</code></p>
<h2 id="practical-example">Practical Example</h2>
<p>Let&rsquo;s have a look at a practical example where we will install the <code>httpd</code> and <code>vsftpd</code> services, enable them at boot time and have them auto restart after 5 seconds should they fail. When the <code>httpd</code> service is started, the <code>vsftpd</code> service should be started as well, but the <code>httpd</code> service should <em>not</em> fail if for some reason <code>vsftpd</code> can not be started.</p>
<pre><code>[root@server1 ~]# dnf install httpd vsftpd -y
Last metadata expiration check: 0:02:49 ago on Fri 26 Jun 2020 12:36:40 +04.
Dependencies resolved.
.....
[root@server1 ~]# systemctl enable httpd &amp;&amp; systemctl enable vsftpd
Created symlink /etc/systemd/system/multi-user.target.wants/httpd.service → /usr/lib/systemd/system/httpd.service.
Created symlink /etc/systemd/system/multi-user.target.wants/vsftpd.service → /usr/lib/systemd/system/vsftpd.service.
[root@server1 ~]# systemctl start  httpd &amp;&amp; systemctl start vsftpd
[root@server1 ~]#
</code></pre><p>Now that we have both services installed, enabled at boot time and running, let&rsquo;s make sure they automatically restart in case they fail:</p>
<pre><code>[root@server1 ~]# systemctl edit httpd
[root@server1 ~]# systemctl cat httpd
# /etc/systemd/system/httpd.service.d/override.conf
[Service]
Restart=on-failure
RestartSec=5s
[root@server1 ~]# systemctl daemon-reload
</code></pre><p>Apply the same configuration for the <code>vsftpd</code> service and test it:<br>
Check for the Main PID of the service using the <strong>systemctl status</strong> command and send a <strong>SIGKILL</strong> signal. When checking the status again you should see <code>Active: activating (auto-restart)</code> and a few seconds later the service will be running again.</p>
<pre><code>[root@server1 ~]# systemctl status httpd
● httpd.service - The Apache HTTP Server
   Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/httpd.service.d
           └─override.conf
   Active: active (running) since Fri 2020-06-26 12:42:04 +04; 9min ago
     Docs: man:httpd.service(8)
 Main PID: 32234 (httpd)

[root@server1 ~]# kill -9 32234
[root@server1 ~]# systemctl status httpd
● httpd.service - The Apache HTTP Server
   Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/httpd.service.d
           └─override.conf
   Active: activating (auto-restart) (Result: signal) since Fri 2020-06-26 12:51:33 +04; 2s ago
     Docs: man:httpd.service(8)
  Process: 32234 ExecStart=/usr/sbin/httpd $OPTIONS -DFOREGROUND (code=killed, signal=KILL)
</code></pre><p>Now, let&rsquo;s make sure that the <code>vsftpd</code> service is automatically started when <code>httpd</code> is started:</p>
<pre><code>[root@server1 ~]# systemctl edit httpd
[root@server1 ~]# systemctl cat httpd
# /etc/systemd/system/httpd.service.d/override.conf
[Service]
Restart=on-failure
RestartSec=5s

[Unit]
Wants=vsftpd.service
[root@server1 ~]# systemctl daemon-reload
</code></pre><p>Stop both services then <em>only</em> start the <code>httpd</code> service again:</p>
<pre><code>[root@server1 ~]# systemctl stop httpd &amp;&amp; systemctl stop vsftpd
[root@server1 ~]# systemctl start httpd
[root@server1 ~]# systemctl status vsftpd
● vsftpd.service - Vsftpd ftp daemon
   Loaded: loaded (/usr/lib/systemd/system/vsftpd.service; enabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/vsftpd.service.d
           └─override.conf
   Active: active (running) since Fri 2020-06-26 13:21:15 +04; 4s ago
</code></pre><h2 id="summary">Summary</h2>
<p>We learned about different unit types in Systemd, the three sections of a Systemd unit file, understanding Target Units and managing units with the <strong>systemctl</strong> command.</p>
]]></content>
        </item>
        
        <item>
            <title>Managing Shell Jobs and Processes on RHEL8</title>
            <link>https://joerismissaert.dev/managing-shell-jobs-and-processes-on-rhel8/</link>
            <pubDate>Sat, 20 Jun 2020 13:57:41 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/managing-shell-jobs-and-processes-on-rhel8/</guid>
            <description>On Linux, we can distinct between three major process types: Shell jobs, Daemons and Kernel threads.
 Shell Jobs are commands started from the command line and are also referred to as interactive processes. Daemons are processes that provides services. They usually start when a computer is booted. Kernel threads are part of the Linux kernel. You can not manage them using the common tools discussed in this post.  Managing Shell Jobs Running Jobs in the Foreground and Background When you type a command, a shell job is started automatically in the foreground occupying the terminal until it finishes.</description>
            <content type="html"><![CDATA[
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />


<p>On Linux, we can distinct between three major process types: Shell jobs, Daemons and Kernel threads.</p>
<ul>
<li><strong>Shell Jobs</strong> are commands started from the command line and are also referred to as interactive processes.</li>
<li><strong>Daemons</strong> are processes that provides services. They usually start when a computer is booted.</li>
<li><strong>Kernel threads</strong> are part of the Linux kernel. You can <em>not</em> manage them using the common tools discussed in this post.</li>
</ul>
<h2 id="managing-shell-jobs">Managing Shell Jobs</h2>
<h3 id="running-jobs-in-the-foreground-and-background">Running Jobs in the Foreground and Background</h3>
<p>When you type a command, a shell job is started automatically in the foreground occupying the terminal until it finishes. This makes sense for commands that take little time to complete or commands that require user interaction.</p>
<p>If you know a command will take a significant amount to complete and it doesn&rsquo;t require user interaction, you can start it in the background by appending an <code>&amp;</code> to the command, e.g. <code>dnf update -y &amp;</code></p>
<p>You can move the last job that was started in the background to the foreground using the <code>fg</code> command. If multiple jobs are running, append the job ID, shown by the <code>jobs</code> command, to the <code>fg</code> command.</p>
<p>It can happen you already executed a command and want to move it to the background. In this case use <code>CTRL+Z</code> to temporarily stop the job and execute the <code>bg</code> command to move it to the background. This does not remove the job from memory, it just pauses the job so that it can be managed. You can use <code>CTRL+C</code> to stop the current job and remove it from memory.</p>
<pre><code>[student@server1 ~]$ dd if=/dev/zero of=/dev/null
^Z
[1]+  Stopped                 dd if=/dev/zero of=/dev/null
[student@server1 ~]$ bg
[1]+ dd if=/dev/zero of=/dev/null &amp;
[student@server ~]# jobs
[1]+  Running                 dd if=/dev/zero of=/dev/null &amp;
[student@server1 ~]# fg 1
dd if=/dev/zero of=/dev/null
^C
9744376+0 records in
9744375+0 records out
4989120000 bytes (5.0 GB, 4.6 GiB) copied, 20.619 s, 242 MB/s
</code></pre><p><code>CTRL+C</code> will terminate the job immediately without closing properly, which could result in data loss. An alternative key sequence you can use is <code>CTRL-D</code>, which sends the End Of File (EOF) character to the current job. The job will stop waiting for further input, complete what it was doing and terminate in a proper way.</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Command</th>
<th>Use</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>&amp;</code> (at the end of a command line)</td>
<td>Start the command in the background.</td>
</tr>
<tr>
<td><code>CTRL+Z</code></td>
<td>Stop the current job temporarily so it can be managed.</td>
</tr>
<tr>
<td><code>CTRL+D</code></td>
<td>Send the EOF character to indicate it should stop waiting for input and close properly.</td>
</tr>
<tr>
<td><code>CTRL+C</code></td>
<td>Cancel the current job and remove it from memory.</td>
</tr>
<tr>
<td><code>bg</code></td>
<td>Continues the job that has been temporarily stopped with CTRL+Z in the background.</td>
</tr>
<tr>
<td><code>fg</code></td>
<td>Brings back to the foreground the last command that was moved to the background.</td>
</tr>
<tr>
<td><code>jobs</code></td>
<td>Shows which jobs are currently running in the background. Displays job ID&rsquo;s that can be used as an argument to the <code>bg</code> and <code>fg</code> commands.</td>
</tr>
</tbody>
</table>

<h3 id="parent-child-relations">Parent-Child Relations</h3>
<p>When a process is started from a shell, it becomes a child process of that shell. The parent is needed to manage the child, all processes started from a shell are terminated when that shell is stopped.</p>
<p>Processes started in the background will not be killed when the parent shell from which they are started is killed. If the parent is killed, the child process becomes a child of <code>systemd</code>:</p>
<pre><code>[student@server1 ~]$ dd if=/dev/zero of=/dev/null &amp;
[1] 3471
[student@server1 ~]$ exit
</code></pre><p>Open a new Terminal and check the running processes using <code>ps fax</code>:</p>
<pre><code>2293 ?        Ss     0:00 /usr/lib/systemd/systemd --user
3540 ?        R      0:32  \_ dd if=/dev/zero of=/dev/null
</code></pre><h2 id="using-common-command-line-tools-for-process-management">Using Common Command-Line Tools for Process Management</h2>
<h3 id="understanding-processes-and-threads">Understanding Processes and Threads</h3>
<p>One process can start several worker threads. Threads can be handled by the different CPUs or CPU cores available on the machine. You can not manage individual threads, the programmer of the multithreaded application needs to define how threads relate to one another.</p>
<p>As such, kernel threads can not be managed. You can not adjust priority and neither can you kill them except by taking the entire machine down. It&rsquo;s easy to recognize kernel threads, they have a name that is between square brackets:</p>
<pre><code>[root@server1 ~]# ps aux | head
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.1  0.7 179196 13988 ?        Ss   16:09   0:02 /usr/lib/systemd/systemd --switched-root --system --deserialize 18
root         2  0.0  0.0      0     0 ?        S    16:09   0:00 [kthreadd]
root         3  0.0  0.0      0     0 ?        I    16:09   0:00 [rcu_gp]
root         4  0.0  0.0      0     0 ?        I    16:09   0:00 [rcu_par_gp]
root         6  0.0  0.0      0     0 ?        I    16:09   0:00 [kworker/0:0H-kblockd]
root         8  0.0  0.0      0     0 ?        I    16:09   0:00 [mm_percpu_wq]
root         9  0.0  0.0      0     0 ?        S    16:09   0:00 [ksoftirqd/0]
root        10  0.0  0.0      0     0 ?        I    16:09   0:00 [rcu_sched]
root        11  0.0  0.0      0     0 ?        S    16:09   0:00 [migration/0]
</code></pre><h3 id="using-ps-to-get-process-information">Using ps to Get Process Information</h3>
<p>The most common command to get an overview of currently running processes is <code>ps</code>. Without arguments, the <code>ps</code> command shows only the processes started by the current user.</p>
<p>There are different options to display different process properties:</p>
<ul>
<li><code>ps aux</code> displays a short summary of the active processes.</li>
<li><code>ps -ef</code> shows the name of the process, but also the exact command it was started with.</li>
<li><code>ps fax</code> shows hierarchical relationships between parent and child processes.</li>
</ul>
<p>Note that some options to the <code>ps</code> command don&rsquo;t have to start with a hyphen.</p>
<pre><code>[student@server1 ~]$ dd if=/dev/zero of=/dev/null &amp;
[1] 4303
[student@server1 ~]$ ps | grep 4303
 4303 pts/0    00:00:08 dd

[student@server1 ~]$ ps aux | grep 4303
student   4303 96.5  0.0   7324   952 pts/0    R    16:50   0:17 dd if=/dev/zero of=/dev/null
student   4319  0.0  0.0  12108   976 pts/0    R+   16:51   0:00 grep --color=auto 4303

[student@server1 ~]$ ps -ef | grep 4303
student   4303  4271 99 16:50 pts/0    00:00:23 dd if=/dev/zero of=/dev/null
student   4327  4271  0 16:51 pts/0    00:00:00 grep --color=auto 4303

[student@server1 ~]$ ps fax | grep -b5 4303
16305- 2727 ?        Ssl    0:00  \_ /usr/libexec/evolution-addressbook-factory
16379- 2764 ?        Sl     0:00  |   \_ /usr/libexec/evolution-addressbook-factory-subprocess --factory all --bus-name org.gnome.evolution.dataserver.Subprocess.Backend.AddressBookx2727x2 --own-path /org/gnome/evolution/dataserver/Subprocess/Backend/AddressBook/2727/2
16643- 3811 ?        Ssl    0:00  \_ /usr/libexec/gvfsd-metadata
16702- 4192 ?        Ssl    0:00  \_ /usr/libexec/gnome-terminal-server
16768- 4271 pts/0    Ss     0:00  |   \_ bash
16808: 4303 pts/0    R      0:39  |       \_ dd if=/dev/zero of=/dev/null
16876- 4342 pts/0    R+     0:00  |       \_ ps fax
16922: 4343 pts/0    S+     0:00  |       \_ grep --color=auto -b5 4303
16988- 4202 ?        Sl     0:00  \_ /usr/libexec/gnome-control-center-search-provider
17069- 2316 ?        Sl     0:00 /usr/bin/gnome-keyring-daemon --daemonize --login
17146- 2446 tty2     Sl     0:00 /usr/libexec/ibus-x11 --kill-daemon
17209- 2529 ?        Ss     0:04 /usr/libexec/sssd/sssd_kcm --uid 0 --gid 0 --logger=files
17294- 2646 ?        Ssl    0:00 /usr/bin/spice-vdagent
[student@server1 ~]$ kill 4303
[1]+  Terminated              dd if=/dev/zero of=/dev/null
[student@server1 ~]$
</code></pre><p>You may have noticed I&rsquo;ve used an important piece of information in my <code>ps |grep</code> commands above: the PID or process ID. An alternative way would be to use the <code>pgrep</code> command to get a list of all PIDs that have a name containing the string <code>dd</code>.</p>
<h3 id="adjusting-process-priority-with-nice-and-renice">Adjusting Process Priority with nice and renice</h3>
<p>Processes are started with a specific priority. All regular processes are equal and started with the same priority: 20.</p>
<p>In some cases it&rsquo;s useful to change the default priority and to do that we can use <code>nice</code> and <code>renice</code>. Use <code>nice</code> to start a process with an adjusted priority and <code>renice</code> to change the priority of an already running process.</p>
<p>You can select values ranging from -20 to 19. The default <em>niceness</em> of a process is set to 0, which results in the priority value of 20 (the lowest priority available). A negative niceness increases the process priority while a positive niceness decreases the priority. Best practice would be to use increments of 5 to see how that impacts the process.</p>
<p>Kernel threads are started as real-time processes, you will never be able to block out kernel threads from CPU time by increasing the priority of a user process.</p>
<p>Single-threaded processes running with the highest priority (niceness -20), can never get beyond the boundaries of the CPU its running on.</p>
<p>Regular users can only decrease priority of a running process, you need to be root to give processes increased priority.</p>
<pre><code>[student@server1 ~]$ nice -n 5 dd if=/dev/zero of=/dev/null &amp;
[1] 4666
[student@server1 ~]$ renice -n 0 -p 4666
renice: failed to set priority for 4666 (process ID): Permission denied
[student@server1 ~]$ renice -n 10 -p 4666
4666 (process ID) old priority 5, new priority 10
[student@server1 ~]$ kill 4666
[1]+  Terminated              nice -n 5 dd if=/dev/zero of=/dev/null
[student@server1 ~]$
</code></pre><p>Notice the line that says <code>old priority 5, new priority 10</code>.<br>
This is actually misleading, it should say <em>niceness</em> instead:<br>
The default process priority is 20 (which is a niceness of 0), so setting the niceness to 5 will lower the priority to 25. You can check that with the <code>top</code> command which I will come back to later on:</p>

    <img src="/img/priority-niceness.png"  alt="TOP Priority Niceness"  class="center"  />


<h3 id="kill-signals-with-kill-killall-and-pkill">Kill Signals with kill, killall and pkill</h3>
<p>Remember that process have a parent-child relationship, the parent is responsible for the child process it created and killing a parent process will make all child process become children of the <code>systemd</code> process.</p>
<p>The Linux kernel allows many signals to be sent to process. We&rsquo;ll discuss 3 major signals that work for all processes:</p>
<ul>
<li><code>SIGTERM (15)</code>: Ask a process to stop.</li>
<li><code>SIGKILL (9)</code>: Force a process to stop.</li>
<li><code>SIGHUP (1)</code>: Hang up a process. The process will reread its configuration files. This comes in handy after making changes to a process configuration file.</li>
</ul>
<p>To send a signal to a process, we use the <code>kill</code> command followed by the PID of the process. By default this will send the <code>SIGTERM</code> signal, the process will stop gracefully and close all open files.</p>
<p>A process can however choose to ignore the <code>SIGTERM</code> signal, in that case we can force stop the process by sending the <code>SIGKILL</code> signal: <code>kill -9 &lt;pid&gt;</code><br>
In general it&rsquo;s a bad idea to use the <code>SIGKILL</code> signal since you risk losing data and your system may become unstable if other processes depend on the killed process.</p>
<p>As an alternative to the <code>kill</code> command we have the <code>pkill</code> and <code>killall</code> commands. <code>pkill</code> takes the process name as an argument instead of the PID and <code>killall</code> will kill all processes using the same name simultaneously.</p>
<h3 id="using-top-to-manage-processes">Using top to Manage Processes</h3>
<p><code>top</code> gives an overview of the most active processes currently running and allows you to do all previously discussed process management tasks.&lt;</p>

    <img src="/img/top.png"  alt="The top program"  class="center"  />


<p>The 8th column (S) shows the process state:





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>State</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Running (R)</strong></td>
<td>The process is currently running and using CPU time.</td>
</tr>
<tr>
<td><strong>Sleeping (S)</strong></td>
<td>The process is waiting for an event to complete.</td>
</tr>
<tr>
<td><strong>Uninterruptible sleep (D)</strong></td>
<td>The process is in a sleep state that can not be stopped, usually while waiting for I/O.</td>
</tr>
<tr>
<td><strong>Stopped (T)</strong></td>
<td>The process has been stopped. This typically happens to a shell job using the CTRL+Z sequence.</td>
</tr>
<tr>
<td><strong>Zombie (Z)</strong></td>
<td>The process was stopped but could not be removed by the parent, putting it into an unmanageable state.</td>
</tr>
</tbody>
</table>
</p>
<p>From within <code>top</code> use the <code>r</code> keyboard button to adjust the priority/niceness of a process and the <code>k</code> keyboard button to send kill signals to a process.</p>
<p>The load average is another important piece of information you can get with <code>top</code>. The load average is expressed as the number of processes that are in a running state (R) or blocking state (D) and is shown for the last 1, 5 and 15 minutes. You can get the same load average information using the <code>uptime</code> command:</p>
<pre><code>[student@server1 ~]$ uptime
17:50:22 up  1:41,  1 user,  load average: 0.01, 0.08, 0.27
</code></pre><p>As a rule of thumb, the load average should not be higher than the number of CPUs or CPU cores on the system. If the load average over a longer period of time is higher than the number of CPUs there may be a performance issue. You can check the number of CPUs and/or cores using the <code>lscpu</code> command:</p>
<pre><code>[student@server1 ~]$ lscpu
Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Byte Order:          Little Endian
CPU(s):              2
On-line CPU(s) list: 0,1
Thread(s) per core:  1
Core(s) per socket:  1
Socket(s):           2&lt;/code&gt;&lt;/pre&gt;
</code></pre><p>In the above example I have 2 CPUs, so the load average on my system over a longer period of time should not be above 2.</p>
<h3 id="summary">Summary</h3>
<p>We learned how to create and manage background jobs, lookup specific processes, terminating processes and changing priorities.</p>
]]></content>
        </item>
        
        <item>
            <title>Package Management with Yum: Understanding Repo’s, Groups, Package Module Streams and RPM Queries</title>
            <link>https://joerismissaert.dev/package-management-with-yum-understanding-repos-groups-package-module-streams-and-rpm-queries/</link>
            <pubDate>Thu, 21 May 2020 16:54:46 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/package-management-with-yum-understanding-repos-groups-package-module-streams-and-rpm-queries/</guid>
            <description>The Yellowdog Updater, Modified, is the default utility to manage software packages on Red Hat Enterprise Linux. On Fedora (the upstream version of RHEL) Yum has been replaced with dnf but Red Hat decided to keep the Yum name for the RHEL releases. Although you&amp;rsquo;ll be using yum, under the hood you&amp;rsquo;re in fact using dnf which is why sometimes you&amp;rsquo;ll see references to dnf or dnf resources.
The Role of Repositories Yum is designed to work with repositories which are depots of available software packages.</description>
            <content type="html"><![CDATA[
    <img src="/img/Yum.png"  alt="The YUM logo"  class="center"  />


<p>The Yellowdog Updater, Modified, is the default utility to manage software packages on Red Hat Enterprise Linux. On Fedora (the upstream version of RHEL) Yum has been replaced with <code>dnf</code> but Red Hat decided to keep the Yum name for the RHEL releases. Although you&rsquo;ll be using <code>yum</code>, under the hood you&rsquo;re in fact using <code>dnf</code> which is why sometimes you&rsquo;ll see references to <code>dnf</code> or <code>dnf</code> resources.</p>
<h2 id="the-role-of-repositories">The Role of Repositories</h2>
<p>Yum is designed to work with repositories which are depots of available software packages. Repositories makes it easy to keep your machine up to date, the maintainer of the repositories publishes updated packages and whenever you use <code>yum</code> to install software the most recent version is automatically used.</p>
<p>As an added benefit, <code>yum</code> manages package dependencies so you don&rsquo;t have to deal with <a href="https://en.wikipedia.org/wiki/Dependency_hell" target="_blank">dependency hell</a>

. When a single package is installed, that same package will contain information about the required dependencies which <code>yum</code> will automatically install for you.</p>
<p>On Red Hat Enterprise Linux, you need to register the system on the Red Hat Customer Portal in order to obtain access to the Red Hat repositories. If you don&rsquo;t register the system you will end up with no repositories at all, but you can create your own repo from the installation media (more on that later).</p>
<p>Repositories are configured in the <code>/etc/yum.repos.d/</code> directory as <code>.repo</code> files. Each file could contain multiple repositories, but each repository should have at least the following contents:</p>
<ul>
<li><code>[label]</code> Identifies the specific repository.</li>
<li><code>name=</code> Specifies the name of the repository you want to use.</li>
<li><code>baseurl=</code> Contains the URL that points to the repo files. Can be HTTP, FTP, or a file path.</li>
</ul>
<p>Here&rsquo;s an example of one of the default CentOS 8 repositories:</p>
<pre><code>[BaseOS]
name=CentOS-$releasever - Base
mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=BaseOS&amp;infra=$infra
#baseurl=http://mirror.centos.org/$contentdir/$releasever/BaseOS/$basearch/os/
gpgcheck=1
enabled=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficial
</code></pre><p>Packages in Internet repositories are often signed with a GPG key which makes it possible to check whether they have been changed since the owner of the repository published them. If for some reason the repository security has been compromised, the GPG key signature will not match and the <code>yum</code> command will raise your attention about this.<br>
GPG-signed packages are not a requirement for internal or local repositories.</p>
<h3 id="creating-your-own-repository">Creating Your Own Repository</h3>
<p>It&rsquo;s fairly straightforward to setup your own repository in case you can&rsquo;t or don&rsquo;t want to register your RHEL system. You can put your own RPM packages (or those from the installation media) in a directory and publish that directory as a repository.</p>
<p>If you&rsquo;re not using the installation media as the source for your own repository, you will need to run the <code>createrepo</code> command inside your repository directory to generate the metadata for the RPM files.</p>
<p>You can create repo from the installation media by mounting the ISO file persistently and creating the necessary <code>.repo</code> file.</p>
<p>Create the empty <code>/repo</code> directory and add the following line to the bottom of the <code>/etc/fstab</code> file to mount the ISO file at next boot:</p>
<pre><code>/path/to/file.iso    /repo    iso9660    defaults    0 0```
</code></pre><p>Next, mount the ISO file:</p>
<pre><code>[root@server1 ~]# mount /repo
mount: /repo: WARNING: device write-protected, mounted read-only.
[root@server1 ~]#
</code></pre><p>Create the <code>/etc/yum.repos.d/mycustom.repo</code> file and add the following content:</p>
<pre><code>[AppStream]
name=AppStream
baseurl=file:///repo/AppStream
gpgcheck=0

[BaseOS]
name=BaseOS
baseurl=file:///repo/BaseOS
gpgcheck=0
</code></pre><p>Check the installed repo&rsquo;s using <code>yum repolist</code>.</p>
<h2 id="working-with-yum">Working with Yum</h2>
<p>To use repositories you need the <code>yum</code> command. Below you&rsquo;ll find an overview of the most common <code>yum</code> tasks.</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>search</td>
<td>Searches for the string you provide in package names and summaries.</td>
</tr>
<tr>
<td>[what]provides */name</td>
<td>Look for specific files inside a package.</td>
</tr>
<tr>
<td>info</td>
<td>Return more info about a package.</td>
</tr>
<tr>
<td>install</td>
<td>Install a package.</td>
</tr>
<tr>
<td>remove</td>
<td>Remove a package.</td>
</tr>
<tr>
<td>list [all</td>
<td>installed]</td>
</tr>
<tr>
<td>group list</td>
<td>List package groups.</td>
</tr>
<tr>
<td>group install [–with-optional]</td>
<td>Install all packages from a group.</td>
</tr>
<tr>
<td>update</td>
<td>Update packages or a specific packages.</td>
</tr>
<tr>
<td>clean all</td>
<td>Remove all stored metadata.</td>
</tr>
<tr>
<td>history [undo <!-- raw HTML omitted -->]</td>
<td>List the command history / undo a specific command.</td>
</tr>
</tbody>
</table>

<p>To install a package you need the exact name, if you don&rsquo;t know the exact name <code>yum search</code> can help you narrow that down. Remember it searches for the string you provide in package names and summaries, so you won&rsquo;t have an exact match:</p>
<pre><code>[root@server1 ~]# yum search user
============ Name &amp; Summary Matched: user =============
trousers-lib.x86_64 : TrouSerS libtspi library
trousers-lib.i686 : TrouSerS libtspi library
trousers-lib.x86_64 : TrouSerS libtspi library
gnome-user-docs.noarch : GNOME User Documentation
gnome-user-docs.noarch : GNOME User Documentation
xdg-user-dirs.x86_64 : Handles user special directories
xdg-user-dirs.x86_64 : Handles user special directories
util-linux-user.x86_64 : libuser based util-linux utilities
util-linux-user.x86_64 : libuser based util-linux utilities
</code></pre><p>The <code>yum provides</code> command can help you find files inside a package which can be helpful if you know the name of the binary for example:</p>
<pre><code>[root@server1 ~]# yum provides */sepolicy
policycoreutils-devel-2.9-3.el8.i686 : SELinux policy core policy devel utilities
Repo : BaseOS
Matched from:
Filename : /usr/bin/sepolicy
Filename : /usr/share/bash-completion/completions/sepolicy

policycoreutils-devel-2.9-3.el8.x86_64 : SELinux policy core policy devel utilities
Repo : BaseOS
Matched from:
Filename : /usr/bin/sepolicy
Filename : /usr/share/bash-completion/completions/sepolicy
</code></pre><p>We can obtain more information about a package using <code>yum info</code>:</p>
<pre><code>[root@server1 ~]# yum info nmap
Available Packages
Name         : nmap
Epoch        : 2
Version      : 7.70
Release      : 5.el8
Architecture : x86_64
Size         : 5.8 M
Source       : nmap-7.70-5.el8.src.rpm
Repository   : AppStream
Summary      : Network exploration tool and security scanner
</code></pre><p><code>yum list | less</code> will show us a list of available and installed packages.<br>
If the repository name is shown, i.e. <code>@AppStream</code>, the package is available for installation in that repository. If <code>@anaconda</code> is shown, then that package has already been installed:</p>
<pre><code class="language-[root@server1" data-lang="[root@server1">Installed Packages
GConf2.x86_64                 3.2.6-22.el8             @AppStream
ModemManager.x86_64           1.10.4-1.el8              @anaconda
</code></pre><p>Packages can be updated using <code>yum update</code>. The old version of a package is replaced with a new version, except for the <em>kernel</em> package. The newer kernel is installed along the old kernel so you can select the kernel you want to use in Grub when booting.</p>
<p>To make it easier to manage specific functionality instead of specific packages, we can work with package groups. Use <code>yum groups list</code> to show available package groups and <code>yum groups info &lt;groupname&gt;</code> to see what packages are in the specified group:</p>
<pre><code>[root@server1 ~]# yum groups list
Last metadata expiration check: 0:00:12 ago on Wed 20 May 2020 17:01:09 +04.
Available Environment Groups:
   Server
   Minimal Install
   Workstation
   Virtualization Host
   Custom Operating System
Installed Environment Groups:
   Server with GUI
Installed Groups:
   Container Management
   Headless Management
Available Groups:
   .NET Core Development
   RPM Development Tools
   Development Tools
   Graphical Administration Tools
   Legacy UNIX Compatibility
   Network Servers
   Scientific Support
   Security Tools
   Smart Card Support
   System Tools

[root@server1 ~]# yum groups info &quot;System Tools&quot;
Last metadata expiration check: 0:00:25 ago on Wed 20 May 2020 17:01:09 +04.

Group: System Tools
 Description: This group is a collection of various tools for the system, such as the client for connecting to SMB shares and tools to monitor network traffic.
 Default Packages:
   NetworkManager-libreswan
   chrony
   cifs-utils
   libreswan
   nmap
   openldap-clients
   samba-client
   setserial
   tigervnc
   tmux
   xdelta
   zsh
 Optional Packages:
   PackageKit-command-not-found
   aide
   amanda-client
   arpwatch
</code></pre><p>You can use <code>yum group install &quot;System Tools&quot;</code> to install the Default Packages inside that group. If you need the Optional Packages as well, use the <code>yum group install --with-optional &quot;System Tools&quot;</code> command instead. Hidden groups can be revealed using <code>yum groups list hidden</code>, these are subgroups of specific groups.</p>
<h2 id="package-module-streams">Package Module Streams</h2>
<p>To separate core operating system packages from user-space packages, we have two main repositories: <strong>BaseOS</strong> and <strong>AppStream</strong>.</p>
<p>In Red Hat Enterprise Linux 8 different versions of the same package can be offered using Package Module Streams which are found inside the AppStream repo.</p>
<p>A module is a delivery mechanism for a set of RPM packages that belong together, and are typically organized around a specific version of an application, with all dependencies for that specific version.</p>
<p>Each module can have one or more <em>streams</em>. A stream contains one specific version. Only one stream can be enabled at the same time which means that only one version can be installed on a system.</p>
<p>Each module can have a default stream. Default streams make it easy to install packages using <code>yum install</code> without the need to learn about modules.</p>
<p>Module streams can be active or inactive. Active streams allow the installation of the module version. Streams are active if marked as default or if they are enabled by a user, unless the whole module has been disabled or another stream of that module is enabled.</p>
<p>Modules can also have one or more <em>profiles</em> which are a list of packages installed together for a particular use case.</p>
<p>Let&rsquo;s have a look at some of the modules using <code>yum module list</code>:</p>
<pre><code>[root@server1 ~]# yum module list | grep -E 'php|nginx'
nginx   1.14 [d] common [d]               nginx webserver                                                    
nginx   1.16     common                   nginx webserver                                                    
php     7.2 [d]  common [d], devel, minimal PHP scripting language                                             
php     7.3      common, devel, minimal     PHP scripting language
Hint: [d]efault, [e]nabled, [x]disabled, [i]nstalled
</code></pre><p>From the above output we see the <em>nginx</em> and <em>php</em> modules with their respective stream (<em>1.14 &amp; 1.16, 7.2 &amp; 7.3</em>) and their profiles (<em>common, devel, minimal)</em>. You can see the same information for a specific module using <code>yum module list &lt;modulename&gt;</code>.</p>
<p>For each module we can get more detailed information using <code>yum module info &lt;modulename&gt;</code> or for a specific stream using <code>yum module info &lt;modulename:version&gt;</code>:</p>
<pre><code>[root@server1 ~]# yum module info php:7.3
Last metadata expiration check: 0:02:58 ago on Thu 21 May 2020 09:05:40 +04.
Name         : php
Stream       : 7.3
Version      : 8010020191122191516
Context      : 2430b045
Architecture : x86_64
Profiles     : common, devel, minimal
Repo         : AppStream
Summary      : PHP scripting language
Description  : php 7.3 module
...........
</code></pre><p>To investigate packages in a specific application stream, we use <code>yum module info </code>&ndash;<code>profile &lt;modulename:version&gt;</code>. This will list the available profiles and the packages for that specific profile:</p>
<pre><code>[root@server1 ~]# yum module info --profile php:7.3
Last metadata expiration check: 0:06:09 ago on Thu 21 May 2020 09:05:40 +04.
Name    : php:7.3:8010020191122191516:2430b045:x86_64
common  : php-cli
        : php-common
        : php-fpm
        : php-json
        : php-mbstring
        : php-xml
devel   : libzip
        : php-cli
        : php-common
        : php-devel
        : php-fpm
        : php-json
        : php-mbstring
        : php-pear
        : php-pecl-zip
        : php-process
        : php-xml
minimal : php-cli
        : php-common
</code></pre><p>Once you have the necessary information, we can enable a module stream and install the module. Every module has a default module stream, if that is the version you need then you don&rsquo;t need to enable anything.<br>
If, for example, we need php7.3 we would need to <em>enable</em> it before installing it. Note that this will also enable dependencies:</p>

    <img src="/img/yum_module_enable-1024x309.png"  alt="yum module enable"  class="center"  />


<p>You can now install the module with with <code>yum module install php</code>.</p>

    <img src="/img/yum_module_install-1024x469.png"  alt="yum module enable"  class="center"  />


<p>Notice in the above output how <code>yum</code> was complaining there was no default profile set for the PHP7.3 stream, so I needed to specify profile using <code>yum module install php/minimal</code>.</p>
<p>Now that I have PHP7.3 installed, I can easily switch to PHP7.2. I don&rsquo;t have to enable the PHP7.2 module stream since that one is the default.</p>
<pre><code>[root@server1 ~]# yum module reset php
[root@server1 ~]# yum module install php:7.2/minimal
[root@server1 ~]# yum distro-sync
</code></pre><p><code>yum distro-sync</code> ensures that all dependent packages which are not in the module itself are updated as well. The output of this command should be:</p>
<pre><code>Dependencies resolved.
Nothing to do.
Complete!
</code></pre><blockquote>
<p>When using <strong>yum install packagename</strong>, the default module stream of a package will be installed if that module stream is enabled. You would only need to use <strong>yum module install packagename:version&gt;/profile</strong> if you have specific version and/or profile requirements.</p>
</blockquote>
<h2 id="querying-software-packages-with-rpm">Querying Software Packages with RPM</h2>
<p>There are two reasons why you should <em>not</em> use the <code>rpm</code> command to manage software packages.</p>
<ol>
<li>Yum takes care of resolving package dependencies for you while <code>rpm</code> does not.</li>
<li>There are two package databases on a RHEL system, the YUM database and the RPM database. When you install packages via <code>yum</code>, the YUM database is updated first and the information is synchronized with the RPM database. Installing packages with RPM will update the RPM database only.</li>
</ol>
<p>That doesn&rsquo;t mean RPM isn&rsquo;t useful. If you downloaded an RPM package you can still install it via <code>yum install package.rpm</code>.<br>
More importantly, the <code>rpm</code> command enables us to get more information about packages:</p>
<p>We can use <code>rpm -qa</code> to show a list of all software that is installed on the system, similar to <code>yum list installed</code>. We can use grep on this command to find out specific package names: <code>rpm -qa | grep php</code></p>
<pre><code>root@server1 ~]# rpm -qa | grep php
php-common-7.2.11-2.module_el8.1.0+209+03b9a8ff.x86_64
php-cli-7.2.11-2.module_el8.1.0+209+03b9a8ff.x86_64
</code></pre><p>Let&rsquo;s find out more about the <code>php-common</code> package usin <code>rpm -qi</code>:</p>
<pre><code>[root@server1 ~]# rpm -qi php-common
Name        : php-common
Version     : 7.2.11
Release     : 2.module_el8.1.0+209+03b9a8ff
Architecture: x86_64
Install Date: Thu 21 May 2020 09:31:12 +04
Group       : Unspecified
Size        : 6472361
License     : PHP and BSD
Signature   : RSA/SHA256, Thu 05 Dec 2019 06:42:19 +04, Key ID 05b555b38483c65d
Source RPM  : php-7.2.11-2.module_el8.1.0+209+03b9a8ff.src.rpm
Build Date  : Thu 14 Nov 2019 08:15:12 +04
Build Host  : x86-01.mbox.centos.org
Relocations : (not relocatable)
Packager    : CentOS Buildsys &amp;lt;bugs@centos.org&gt;
Vendor      : CentOS
URL         : http://www.php.net/
Summary     : Common files for PHP
Description :
The php-common package contains files used by both the php
package and the php-cli package.
</code></pre><p>We can list the files inside the package using <code>rpm -ql</code>:</p>
<pre><code>[root@server1 ~]# rpm -ql php-common
/etc/php.d
/etc/php.d/20-bz2.ini
/etc/php.d/20-calendar.ini
/etc/php.d/20-ctype.ini
.....
</code></pre><p>Or, we can list only the documentation using <code>rpm -qd</code>, or the configuration files using <code>rpm -qc</code>:</p>
<pre><code>[root@server1 ~]# rpm -qd php-common
/usr/share/doc/php-common/CODING_STANDARDS
/usr/share/doc/php-common/CREDITS
.....
[root@server1 ~]# rpm -qc php-common
/etc/php.d/20-bz2.ini
/etc/php.d/20-calendar.ini
/etc/php.d/20-ctype.ini
...
</code></pre><p>If you have a filename and want to know what package it belongs to, use <code>rpm -qf</code>:</p>
<pre><code>[root@server1 ~]# rpm -qf /bin/bash
bash-4.4.19-10.el8.x86_64
[root@server1 ~]# rpm -qf /bin/lsblk
util-linux-2.32.1-17.el8.x86_64
</code></pre><p>All the above queries were used on the RPM database and what we were querying were installed packages. Sometimes it makes sense to query an RPM package file before installing it, in that case we need to add the <code>-p</code> option in addition to any of the previous mentioned options. We can use <code>yumdownloader</code> to download a specific package from our repository to run an RPM query against it before installing it:</p>
<pre><code>[root@server1 ~]# yum whatprovides */yumdownloader
Last metadata expiration check: 0:00:38 ago on Thu 21 May 2020 20:34:10 +04.
yum-utils-4.0.8-3.el8.noarch : Yum-utils CLI compatibility layer
Repo        : BaseOS
Matched from:
Filename    : /usr/bin/yumdownloader

[root@server1 ~]# yum install yum-utils -y
....

[root@server1 ~]# yumdownloader httpd
[root@server1 ~]# rpm -qpi httpd-2.4.37-16.module_el8.1.0+256+ae790463.x86_64.rpm 
Name        : httpd
Version     : 2.4.37
Release     : 16.module_el8.1.0+256+ae790463
....
</code></pre><p>We can also query packages directly from the repository instead of downloading the package first. Use the <code>repoquery</code> command for this.</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Command</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>rpm -qf</td>
<td>Use a filename to find the specific RPM package the file belongs to</td>
</tr>
<tr>
<td>rpm -ql</td>
<td>Provide a list of files inside the RPM package</td>
</tr>
<tr>
<td>rpm -qi</td>
<td>Provide package information</td>
</tr>
<tr>
<td>rpm -qd</td>
<td>Show all documentation available in the package</td>
</tr>
<tr>
<td>rpm -qc</td>
<td>Show all configuration files</td>
</tr>
<tr>
<td>rpm -q -scripts</td>
<td>Show scripts that are used in the package</td>
</tr>
<tr>
<td>rpm -qp <!-- raw HTML omitted --></td>
<td>Query individual .rpm files instead of the RPM database</td>
</tr>
<tr>
<td>rpm -qR</td>
<td>Show package dependencies</td>
</tr>
<tr>
<td>rpm -V</td>
<td>Shows which parts of a package has been changed since installation.</td>
</tr>
<tr>
<td>rpm -Va</td>
<td>Verifies all installed packages and shows which part of the package has been changed since installation.</td>
</tr>
<tr>
<td>rpm -qa</td>
<td>List all installed packages</td>
</tr>
</tbody>
</table>

]]></content>
        </item>
        
        <item>
            <title>Configuring the Network on RHEL 8 with nmcli and nmtui</title>
            <link>https://joerismissaert.dev/configuring-the-network-on-rhel-8-with-nmcli-and-nmtui/</link>
            <pubDate>Tue, 19 May 2020 08:03:28 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/configuring-the-network-on-rhel-8-with-nmcli-and-nmtui/</guid>
            <description>In the previous post we learned how the check the runtime configuration of the network using the ip command. To make persistent changes to the network configuration that will survive a reboot, we need to use either nmcli or nmtui.
Networking is managed by the NetworkManager service. When the NetworkManager service comes up, it reads the network card configuration scripts that are located in /etc/sysconfig/network-scripts and that have a name which starts with ifcgf followed by the name of the network card.</description>
            <content type="html"><![CDATA[
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />


<p>In the previous post we learned how the check the runtime configuration of the network using the <code>ip</code> command. To make persistent changes to the network configuration that will survive a reboot, we need to use either <code>nmcli</code> or <code>nmtui</code>.</p>
<p>Networking is managed by the <em>NetworkManager</em> service. When the <em>NetworkManager</em> service comes up, it reads the network card configuration scripts that are located in <code>/etc/sysconfig/network-scripts</code> and that have a name which starts with <code>ifcgf</code> followed by the name of the network card.</p>
<p>You can check the status of the NetworkManager using <code>systemctl status NetworkManager</code>.</p>
<p>On RHEL 8, we differentiate between a device and a connection as follows:</p>
<ul>
<li>A device is a network interface card</li>
<li>A connection is the configuration that is applied to the device</li>
</ul>
<p>You can create multiple connections for one device and manage the connections we assign to devices using <code>nmcli</code> or <code>nmtui</code>.</p>
<h2 id="required-permissions">Required Permissions</h2>
<p>The root user can make modifications to the network configuration and so can regular users <em>if they are logged in to the local console</em>: if a regular user is using the system keyboard to enter either a graphical or text-based console, some permissions to change the network configuration are granted. This is because users are supposed to be able to connect their local system to a network.</p>
<p>Users that have used <code>ssh</code> to connect to a server are <em>not</em> allowed to change the network configuration.</p>
<p>You can check the current permissions using the <code>nmcli gen permissions</code> command:</p>

    <img src="/img/nmcli_gen_permissions.png"  alt="nmcli permissions"  class="center"  />


<h2 id="configuring-the-network-with-nmcli">Configuring the Network with nmcli</h2>
<p>We can use the <code>nmcli</code> command to make persistent changes to our network configuration. <code>nmcli</code> will write your configuration to the network card configuration scripts located in <code>/etc/sysconfig/network-scripts</code> from where it will be read by the <em>NetworkManager</em> service during boot time or when restarting the service.</p>
<p>Active and inactive connections can be shown using the <code>nmcli con show</code> command. Inactive connections are not assigned to a device:</p>

    <img src="/img/nmcli_con_show-1.png"  alt="nmcli permissions"  class="center"  />


<p>Once you have an overview of the connections, you can see the details of a connection using <code>nmcli con show _connectionname_</code>, e.g. <code>nmcli con show ens1</code>. This will show all properties of the given connection. Check <code>man nm-settings</code> to find out what these settings do exactly.</p>
<p>Just as with connections, we can see the currently configured devices and their status: <code>nmcli dev status</code></p>

    <img src="/img/nmcli_dev_status.png"  alt="nmcli dev status"  class="center"  />


<p>We use <code>nmcli dev show devicename</code> to reveal the settings for a specific device:

    <img src="/img/nmcli_dev_show.png"  alt="nmcli dev status"  class="center"  />

</p>
<h3 id="creating-network-connections">Creating Network Connections</h3>
<p>We can use the <code>nmcli con add</code> command to create a new connection on a specific device. Bash completion and the <code>nmcli-examples</code> man page can help you on this. I&rsquo;ll list two examples below, one dhcp configured connection and one static configuration.</p>
<pre><code>[student@server1 ~]$ nmcli con add con-name dhcp-config type ethernet ifname ens1 ipv4.method auto
[student@server1 ~]$ nmcli con add con-name static-config type ethernet ifname ens1 autoconnect no ip4 10.0.0.10/24 gw4 10.0.0.1 ipv4.method manual

</code></pre><h3 id="modifying-connection-parameters">Modifying Connection Parameters</h3>
<p><code>nmcli con mod</code> allows us to modify the connections we added earlier.<br>
Let&rsquo;s make sure our static connection automatically connects:</p>
<pre><code>[student@server1 ~]$ nmcli con mod static-config autoconnect yes
</code></pre><p>We&rsquo;ll also add a DNS server, notice how i&rsquo;m using <code>ipv4</code> and not <code>ip4</code> like when adding a connection:</p>
<pre><code>[student@server1 ~]$ nmcli con mod static-config ipv4.dns 10.0.0.10
</code></pre><p>We can add a secondary DNS server using the <em>+</em> sign:</p>
<pre><code>[student@server1 ~]$ nmcli con mod static-config +ipv4.dns 8.8.8.8
</code></pre><p>Let&rsquo;s also change the current IP address and add a second IP address:</p>
<pre><code>[student@server1 ~]$ nmcli con mod static-config ipv4.addresses 10.0.0.100/24
[student@server1 ~]$ nmcli con mod static-config +ipv4.addresses 10.20.30.40/16
</code></pre><p>When we are done with our modifications, we should activate our changes:</p>
<pre><code>[student@server1 ~]$ nmcli con up static-config
Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/5)
[student@server1 ~]$
</code></pre><h2 id="configuring-the-network-with-nmtui">Configuring the Network with nmtui</h2>
<p><code>nmtui</code> is a textual user interface for the fairly complicated syntax of the <code>nmcli</code> command. Everything that can be done with <code>nmcli</code> can also be done with <code>nmtui</code> so I won&rsquo;t cover it too much as it&rsquo;s pretty much self-explanatory.</p>

    <img src="/img/nmtui-1.png"  alt="nmtui"  class="center"  />



    <img src="/img/nmtui-2.png"  alt="nmtui"  class="center"  style="margin-top: 10px; margin-bottom: 10px;"  />



    <img src="/img/nmtui-3.png"  alt="nmtui"  class="center"  />


<h2 id="working-with-network-configuration-files">Working with Network Configuration Files</h2>
<p>If you don&rsquo;t like making configuration changes using <code>nmcli</code> or <code>nmtui</code> you can directly edit the network interface card configuration file itself. After making changes to the configuration file, use the <code>nmcli con up</code> command to activate the new configuration.</p>

    <img src="/img/ifcg-ens1.png"  alt="interface card configuration file"  class="center"  />


<blockquote>
<p>You can set both a fixed IP address and a dynamic IP address in one network configuration. Set the BOOTPROTO option to in the configuration file to dhcp while also specifying an IP address and network prefix.  You can also do this from <code>nmtui</code> by making sure the IPv4 configuration is set to Automatic while also specifying an IP address.</p>
</blockquote>

    <img src="/img/nmtui-staticdhcp.png"  alt="Static DHCP with nmtui"  class="center"  />


<h2 id="hostname-and-name-resolution">Hostname and Name Resolution</h2>
<p>Hostnames are used to communicate with other hosts. A hostname consists of the name of the host and the DNS domain in which they reside, these two parts together make up for the fully qualified domain name (FQDN), e.g. server1.example.com. An FQDN would provide a unique identity on the internet.</p>
<h3 id="hostnames">Hostnames</h3>
<p>We can use different ways to set the hostname:</p>
<ul>
<li>Use <code>nmtui</code> and select the <code>Change Hostname</code> option</li>
<li>Use <code>hostnamectl set-hostname</code></li>
<li>Edit the <code>/etc/hostname</code> configuration file</li>
</ul>
<p>After setting the hostname, you can use <code>hostnamectl status</code> to show the current hostname:</p>
<pre><code>[root@server1 ~]# hostnamectl set-hostname server1.example.local
[root@server1 ~]# hostnamectl status
Static hostname: server1.example.local
</code></pre><p>We can configure hostname resolution in the <code>/etc/hosts</code> file.<br>
The first column has the IP address of the host, the second specifies the hostname (either short or FQDN). If it has more than one name (short and FQDN) the second column must be the FQDN and the third one the alias or shortname:</p>
<pre><code>bash[root@server1 ~]# cat /etc/hosts
127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
::1 localhost localhost.localdomain localhost6 localhost6.localdomain6
10.0.0.2 server2.example.local server2
</code></pre><p>Definitions in the <code>/etc/hosts</code> file will be applied before the hostname in DNS resolution is used. This priority is set in <code>/etc/nsswitch.conf</code> where <code>files</code> is a reference to the <code>/etc/hosts</code> file.</p>
<pre><code>hosts: files dns myhostname
</code></pre><h3 id="dns-name-resolution">DNS Name Resolution</h3>
<p>In order to communicate with other hosts on the Internet we will need DNS.<br>
We already know how to set a DNS server using <code>nmcli</code> or <code>nmtui</code>.<br>
The <em>NetworkManager</em> reads the configuration script in <code>/etc/sysconfig/network-scripts</code> and pushes the DNS configuration to <code>/etc/resolv.conf</code>.<br>
If you edit the <code>/etc/resolv.conf</code> file manually, your changes will be overwritten by <em>NetworkManager</em> the next time it starts.</p>
<p>It&rsquo;s recommended to setup at least two DNS Name Servers for redundancy:</p>
<ul>
<li>Use <code>nmtui</code></li>
<li>Set the DNS1 and DNS2 parameters in the <code>ifcfg</code> network configuration file</li>
<li>Use DHCP</li>
<li>Use <code>nmcli con mod &lt;connection name&gt; [+]ipv4.dns &lt;ip-of-dns&gt;</code></li>
</ul>
<p>If the connection is configured to get the configuration from a DHCP server then the DNS server is also set via DHCP. If you don&rsquo;t want that to happen you have two options:</p>
<ul>
<li>Edit the <code>ifcfg</code> configuration file to include the option <code>PEERDNS=no</code></li>
<li>Use <code>nmcli cod mod &lt;connection name&gt; ipv4.ignore-auto-dns yes</code></li>
</ul>
<p>To verify hostname resolution use the <code>getent hosts &lt;hostname&gt;</code> command. This searches in both <code>/etc/hosts</code> and DNS to resolve the specified hostname.</p>
]]></content>
        </item>
        
        <item>
            <title>Validating Network Configuration on Red Hat Enterprise Linux 8</title>
            <link>https://joerismissaert.dev/validating-network-configuration-on-red-hat-enterprise-linux-8/</link>
            <pubDate>Mon, 18 May 2020 17:33:16 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/validating-network-configuration-on-red-hat-enterprise-linux-8/</guid>
            <description>Networking is one of the essential items on a server. On Red Hat Enterprise Linux 8, networking is managed by the NetworkManager service. We&amp;rsquo;ll cover new tools that were introduced to help manage networks during runtime and how to make make the configuration persistent.
Validating Network Addresses and Interfaces In RHEL 8, the default names for network cards are based on firmware, device topology, and device firmware. Network card names will always consist of the following parts:</description>
            <content type="html"><![CDATA[
    <img src="/img/redhat-8-logo.png"  alt="Red Hat logo"  class="center"  />


<p>Networking is one of the essential items on a server. On Red Hat Enterprise Linux 8, networking is managed by the NetworkManager service. We&rsquo;ll cover new tools that were introduced to help manage networks during runtime and how to make make the configuration persistent.</p>
<h2 id="validating-network-addresses-and-interfaces">Validating Network Addresses and Interfaces</h2>
<p>In RHEL 8, the default names for network cards are based on firmware, device topology, and device firmware. Network card names will always consist of the following parts:</p>
<ul>
<li>Ethernet interfaces begin with <em>en,</em> WLAN interfaces begin with <em>wl</em> and WWAN interfaces begin with <em>ww</em></li>
<li>The next part of the name represents the type of adapter.<br>
An <em>o</em> is used for onboard, <em>s</em> for hotplug slot, <em>p</em> for PCI location.</li>
<li>A number is used to represent an index, ID or port.</li>
</ul>
<p>e.g. <em>enp3p1</em> would indicate an ethernet device on PCI slot 3, port 1.<br>
Apart from this default device naming scheme, BIOS device naming can be used as well if the <em>biosdevname</em> package is installed.</p>
<h3 id="validating-network-address-configuration">Validating Network Address Configuration</h3>
<p>To verify the runtime configuration of the network, we can use the <code>ip</code> utility. We can use this utility to monitor many aspects of networking:</p>
<ul>
<li><code>ip addr</code> to show and configure network addresses</li>
<li><code>ip route</code> to show and configure routing information</li>
<li><code>ip link</code> to show and configure network link state</li>
</ul>
<p>We can use the <code>ip addr show</code> command to show the current network settings:</p>
<pre><code>[student@server1 ~]$ ip addr show
1: lo: mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
valid_lft forever preferred_lft forever
inet6 ::1/128 scope host
valid_lft forever preferred_lft forever
2: ens1: mtu 1500 qdisc fq_codel state UP group default qlen 1000
link/ether 52:54:00:d4:91:88 brd ff:ff:ff:ff:ff:ff
inet 192.168.122.237/24 brd 192.168.122.255 scope global dynamic noprefixroute ens1
valid_lft 3504sec preferred_lft 3504sec
inet6 fe80::ccd3:f5b4:4faf:2321/64 scope link noprefixroute
valid_lft forever preferred_lft forever
[student@server1 ~]$
</code></pre><p>This lists all network interfaces on the system.<br>
In the above case we see 2 network interfaces, the loopback interface <code>lo</code> and the hot-pluggable device <code>ens1</code>. The loopback interface is used for IP communication between processes on the machine.</p>
<p>We can see:</p>
<ul>
<li>The current link state:<br>
<code>2: ens1: mtu 1500 qdisc fq_codel state UP group default qlen 1000</code></li>
<li>The MAC address configuration:<br>
<code>link/ether 52:54:00:d4:91:88</code></li>
<li>the IPv4 configuration:<br>
<code>inet 192.168.122.237/24</code></li>
<li>and the IPv6 configuration:<br>
<code>inet6 fe80::ccd3:f5b4:4faf:2321/64</code></li>
</ul>
<p>Every interface automatically gets an IPv6 address which can only be used for communication on the local network. Such addresses start with <code>fe80</code>.</p>
<p>If you are interested in the link status only of the network interfaces, you can use the <code>ip link show</code> command. You can reveal statistics on received packets (RX) and transmitted packets (TX) by adding the <code>-s</code> option: <code>ip -s link show</code></p>
<h3 id="validating-routing">Validating Routing</h3>
<p>Routing is required for every network that needs to communicate to devices on other networks. For that to work, the network but have at least one default router or gateway. The default gateway must always be on the same network or subnet. You can check which gateway is being used with the <code>ip route show</code> command:</p>
<pre><code>[student@server1 ~]$ ip route show
default via 192.168.122.1 dev ens1 proto dhcp metric 100
192.168.122.0/24 dev ens1 proto kernel scope link src 192.168.122.237 metric 100
</code></pre><p>The most important part is the first line that shows the default route goes through <code>192.168.122.1</code> and also shows that device <code>ens1</code> must be used to access that gateway. We can also see that this route was assigned by <code>dhcp</code>.<br>
In case of multiple routes, the route with the lowest metric will be used.</p>
<h3 id="validating-port-and-service-availability">Validating Port and Service Availability</h3>
<p>Network issues can be related to the local IP address or router settings but can also come from network ports that are not available on the server.<br>
We use the <code>ss</code> command to verify the availability of ports. By using <code>ss -lt</code> we can see the listing TCP ports on the local system and by using <code>ss -lu</code> we see the UDP ports. We can combine the commands to <code>ss -tul</code>:</p>

    <img src="/img/ss_netstat-1.png"  alt="ss as an alternative to netstat"  class="center"  />


<p>Notice that some ports/services are only listening on the loopback address while others are listening on <code>0.0.0.0</code> (all IPv4 addresses) or on <code>[::]</code> (all IPv6 addresses).</p>
]]></content>
        </item>
        
        <item>
            <title>Auto Deploy Branch Changes to Kinsta using GitLab CI/CD</title>
            <link>https://joerismissaert.dev/auto-deploy-branch-changes-to-kinsta-using-gitlab-ci-cd/</link>
            <pubDate>Thu, 14 May 2020 10:28:03 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/auto-deploy-branch-changes-to-kinsta-using-gitlab-ci-cd/</guid>
            <description>If you&amp;rsquo;re comfortable using the command line, you&amp;rsquo;re probably aware there are different ways you can push your Kinsta Staging site to Live without overwriting the Live database. This can be useful when you need to push code changes, and at the same time need to leave the Live database in its current state.
This post will cover how you can achieve this using GitLab CI/CD and assumes you&amp;rsquo;re familiar with version control using git, and using the Kinsta Staging environment for testing and development.</description>
            <content type="html"><![CDATA[
    <img src="/img/kinsta-logo-alpha-purple-300x60.png"  alt="Kinsta Inc. logo"  class="center"  />



    <img src="/img/gitlab-icon-rgb-300x276.png"  alt="GitLab logo"  class="center"  style="width:100px; margin-top:10px"  />


<p>If you&rsquo;re comfortable using the command line, you&rsquo;re probably aware there are different ways you can push your Kinsta Staging site to Live without overwriting the Live database. This can be useful when you need to push code changes, and at the same time need to leave the Live database in its current state.</p>
<p>This post will cover how you can achieve this using <a href="https://about.gitlab.com/stages-devops-lifecycle/continuous-integration/" target="_blank">GitLab CI/CD</a>

 and assumes you&rsquo;re familiar with version control using <code>git</code>, and using the Kinsta Staging environment for testing and development. If you&rsquo;re developing locally, you should be able to apply the information in this post as well.</p>
<p>The end result will be that whenever you merge changes into the <code>master</code> branch on <em>Staging</em> and push those changes to the remote <code>master</code> branch, GitLab will automatically deploy the updated <code>master</code> branch to Kinsta <em>Live</em>.</p>
<p>As an added bonus you will also have implemented version control so you can easily roll back any changes you make to your code.</p>
<p>Let&rsquo;s get started!</p>
<h2 id="create-a-kinsta-site-and-gitlab-project">Create a Kinsta Site and GitLab Project</h2>
<p>If you don&rsquo;t have a Kinsta site already, create one following the instructions <a href="https://kinsta.com/knowledgebase/new-site/" target="_blank">here</a>

. In this case, I&rsquo;ll create <em>an empty site</em> with the name <code>gitlabautodeploy</code> and add WordPress later on.</p>
<p>Next, we&rsquo;ll <a href="https://docs.gitlab.com/ee/gitlab-basics/create-project.html" target="_blank">create an empty project on Gitlab</a>

. This repository will contain our website&rsquo;s files. In my case I&rsquo;ll create a private project.</p>
<!-- raw HTML omitted -->
<h2 id="cloning-configuring-and-populating-the-gitlab-repo-on-kinsta">Cloning, Configuring and Populating the GitLab Repo on Kinsta</h2>
<h3 id="setting-up-ssh-keys">Setting up SSH Keys</h3>
<p>In order to clone the repository on our Live environment, we&rsquo;ll have to add our SSH public key to our GitLab SSH Keys.</p>
<p><a href="https://kinsta.com/knowledgebase/connect-to-ssh/" target="_blank">Connect to Live via SSH</a>

 and copy the output of the below command, then add it your GitLab <em>account settings</em> following the instructions <a href="https://docs.gitlab.com/ee/ssh/#adding-an-ssh-key-to-your-gitlab-account" target="_blank">here</a>

.</p>
<pre><code>gitlabautodeploy@gsY-gitlabautodeploy:~$ cat .ssh/id_rsa.pub
</code></pre><p>This will allow us to access and clone the repository over SSH without the need for authentication.</p>
<p><a href="https://kinsta.com/feature-updates/add-ssh-keys/" target="_blank">Add the same public key to your SSH Keys in My Kinsta</a>

 This step is equally important as the next. Without this step GitLab will not be able to connect to the Live environment.</p>
<p>Next, we want to add our SSH <em>private</em> key as a variable to the repository&rsquo;s CI/CD Pipeline settings. Copy the output of the below command:</p>
<pre><code>gitlabautodeploy@gsY-gitlabautodeploy:~$ cat .ssh/id_rsa
</code></pre><p>Under <code>Settings &gt; CI/CD</code> <em>in your GitLab repo</em>, add a new Variable containing your private key. Give the variable the name <code>SSH_PRIVATE_KEY</code>, paste the private key in the Value field, and add the variable.</p>

    <img src="/img/gitlab_variable.png"  alt="GitLab SSH Privat Key Variable"  class="center"  />


<p>The private key will allow GitLab to connect to our Kinsta Live environment and deploy the changes we&rsquo;ve pushed from Staging.</p>
<h3 id="clone-and-configure-the-repo">Clone and Configure the Repo</h3>
<p>We can now clone the empty repository onto Kinsta Live. Go to your repository overview on GitLab and click the blue Clone dropdown, select Clone with SSH and copy the value.</p>

    <img src="/img/git_clone-1-1024x290.png"  alt="GitLab Clone Repository"  class="center"  />


<p>Paste the value after the <code>git clone</code> command inside the home directory on Live and execute it:</p>
<pre><code>gitlabautodeploy@gsY-gitlabautodeploy:~$ git clone git@gitlab.com:joerismissaert/gitlabautodeploy.git
</code></pre><p>Once the cloning has finished, you&rsquo;ll notice an extra folder was created with the name of our repository:</p>
<pre><code>gitlabautodeploy@gsY-gitlabautodeploy:~$ ls -lh
total 19K
drwxr-xr-x 3 gitlabautodeploy www-data 3 May 14 08:57 gitlabautodeploy
</code></pre><p>By default, Kinsta serves websites from the <code>~/public</code> folder. You can reach out to support to have this changed, but in this case I&rsquo;ll delete the existing <code>public</code> folder and rename my cloned repository to <code>public</code>:</p>
<pre><code>gitlabautodeploy@gsY-gitlabautodeploy:~$ rm -rf public/
gitlabautodeploy@gsY-gitlabautodeploy:~$ mv gitlabautodeploy public
gitlabautodeploy@gsY-gitlabautodeploy:~$
</code></pre><p>In the next step we&rsquo;ll configure our repo:</p>
<pre><code>gitlabautodeploy@gsY-gitlabautodeploy:~$ cd public
gitlabautodeploy@gsY-gitlabautodeploy:~/public$ git config --global user.email &quot;youremail@address.com&quot;
gitlabautodeploy@gsY-gitlabautodeploy:~/public$ git config --global user.name &quot;Joeri&quot;
gitlabautodeploy@gsY-gitlabautodeploy:~/public$
</code></pre><h2 id="install-and-configure-wordpress">Install and Configure WordPress</h2>
<p>We are now ready to install and configure WordPress. Go ahead and download the latest version of WordPress inside your repository on Live:</p>
<pre><code>gitlabautodeploy@gsY-gitlabautodeploy:~/public$ wp core download
Downloading WordPress 5.4.1 (en_US)…
md5 hash verified: 346afd52e893b2492e5899e4f8c91c43
Success: WordPress downloaded.
gitlabautodeploy@gsY-gitlabautodeploy:~/public$
</code></pre><p>If you know how to setup WordPress manually, great! Else you can use the Web Interface by going to the Primary Domain for your Kinsta site. Follow the instructions and configure the site.</p>
<h4 id="configuring-the-gitlab-cicd-pipeline">Configuring the GitLab CI/CD Pipeline</h4>
<p>We need to add a YAML file called <code>.gitlab-ci.yml</code> to our repository that contains instructions for GitLab to deploy our changes to Live. When GitLab detects this file, it will start up a Docker container and configure the Docker environment based on the instructions in our file to be able to connect to our Live environment and deploy the updated <code>master</code> branch.</p>
<p>Create the <code>.gitlab-ci.yml</code> file and add the following:</p>
<pre><code>before_script:
    - apt-get update -qq
    - apt-get install -qq git
    # Setup SSH deploy keys
    - 'which ssh-agent || ( apt-get install -qq openssh-client )'
    - eval $(ssh-agent -s)
    - ssh-add &lt;(echo &quot;$SSH_PRIVATE_KEY&quot;)
    - mkdir -p ~/.ssh
    - '[[ -f /.dockerenv ]] &amp;&amp; echo -e &quot;Host *\n\tStrictHostKeyChecking no\n\n&quot; &amp;gt; ~/.ssh/config'

deploy_live:
    type: deploy
    environment:
        name: Live
        url: gitlabautodeploy.kinsta.cloud
    script:
        - ssh kinstauser@IPADDRESS -p PORTNUMBER &quot;cd /www/gitlabautodeploy_941/public &amp;&amp; git checkout master &amp;&amp; git pull origin master &amp;&amp; exit&quot;
    only:
        - master
</code></pre><p>Pay attention to the <code>deploy_live</code> block where you will want to modify the <code>url</code>, <code>ssh</code> connection string and the path in the command. Nothing is actually being done with the <code>url</code> value in this case though.</p>
<p>The <code>only: - master</code> part refers to the branch of your repository that should trigger this script when changes have been pushed to it. If you change this, make sure to change the SSH command accordingly.</p>
<p>You could add a <code>deploy_staging</code> block that contains the SSH connection string for the Staging environment and the <code>only: - staging</code> block list item so that a <code>git push</code> to the equally named <code>staging</code> branch would trigger a deploy to Kinsta&rsquo;s Staging environment from your local development environment for example.</p>
<h4 id="initial-commit">Initial Commit</h4>
<p>Once the site is set up and you&rsquo;ve confirmed it&rsquo;s working properly, we can commit and push our changes to the remote repository on GitLab.</p>
<p>We&rsquo;ll <strong>create a new branch</strong> for our repository called <code>v0.1</code>, then start tracking all the files we added to our Repo, commit our changes, and push our changes to the remote origin on the new branch.</p>
<pre><code>gitlabautodeploy@gsY-gitlabautodeploy:~/public$ git checkout -b v0.1
Switched to a new branch 'v0.1'
gitlabautodeploy@gsY-gitlabautodeploy:~/public$ git add .
gitlabautodeploy@gsY-gitlabautodeploy:~/public$ git commit -a -m 'Initial Commit'
gitlabautodeploy@gsY-gitlabautodeploy:~/public$ git push origin v0.1
</code></pre><h3 id="creating-staging-and-testing-the-deployment">Creating Staging and Testing the Deployment</h3>
<p>We&rsquo;re all set up, let&rsquo;s create the Staging environment and run some tests.<br>
Follow the instructions <a href="https://kinsta.com/knowledgebase/staging-environment/" target="_blank">here</a>

 to create the Staging environment and connect to Staging using SSH.</p>
<p>We should still be on the new branch we created earlier and we&rsquo;ll see the <code>wp-config.php</code> file has been modified. This was done by Kinsta&rsquo;s Staging creation script and is completely normal.</p>
<pre><code>gitlabautodeploy@XJl-staging-gitlabautodeploy:~/public$ git status
On branch v0.1
Changes not staged for commit:
(use &quot;git add …&quot; to update what will be committed)
(use &quot;git restore …&quot; to discard changes in working directory)
modified: wp-config.php
no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)
gitlabautodeploy@XJl-staging-gitlabautodeploy:~/public$
</code></pre><p>Before we go and commit the change, let&rsquo;s add a simple plugin: <code>classic-editor</code></p>
<pre><code>gitlabautodeploy@XJl-staging-gitlabautodeploy:~/public$ wp plugin install classic-editor
Downloading installation package from https://downloads.wordpress.org/plugin/classic-editor.1.5.zip…
Unpacking the package…
Installing the plugin…
Plugin installed successfully.
Success: Installed 1 of 1 plugins.
gitlabautodeploy@XJl-staging-gitlabautodeploy:~/public$
</code></pre><p>Add all untracked files, commit the changes and push the changes to the remote origin on the <code>v0.1</code> branch:</p>
<pre><code>gitlabautodeploy@XJl-staging-gitlabautodeploy:~/public$ git add .
gitlabautodeploy@XJl-staging-gitlabautodeploy:~/public$ git commit -a -m 'Installed classic-editor plugin'
gitlabautodeploy@XJl-staging-gitlabautodeploy:~/public$ git push origin v0.1
gitlabautodeploy@XJl-staging-gitlabautodeploy:~/public$
</code></pre><p>Now let&rsquo;s switch to our <code>master</code> branch, merge the <code>v0.1</code> branch and push the merge to the remote <code>master</code> branch:</p>
<pre><code>gitlabautodeploy@XJl-staging-gitlabautodeploy:~/public$ git checkout master
Switched to branch 'master'
gitlabautodeploy@XJl-staging-gitlabautodeploy:~/public$ git merge v0.1
gitlabautodeploy@XJl-staging-gitlabautodeploy:~/public$ git push origin master
</code></pre><p>Go to your GitLab project: <code>CI / CD &gt; Jobs</code>, where you&rsquo;ll see the deployment running:</p>

    <img src="/img/gitlab_pipe_running-1024x157.png"  alt="GitLab deploy job running"  class="center"  />


<p>You can click on the Status to see the deployment in action:

    <img src="/img/gitlab_pipe_details-1024x269.png"  alt="GitLab deploy job running"  class="center"  />

</p>
<p>If everything went well, the Status will change from <code>running</code> to <code>passed</code> and your live site now has the <code>classic-editor</code> plugin installed.</p>

    <img src="/img/gitlab_pipe_passed.png"  alt="GitLab deploy job running"  class="center"  />


]]></content>
        </item>
        
        <item>
            <title>Lab: Permissions In Practice</title>
            <link>https://joerismissaert.dev/lab-permissions-in-practice/</link>
            <pubDate>Sun, 10 May 2020 16:57:26 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/lab-permissions-in-practice/</guid>
            <description>In the previous two posts we learned about basic and advanced permissions, Access Control Lists, umask and extended-attributes.
Let&amp;rsquo;s put this knowledge to work by setting up a shared group environment
Lab Objectives  Create 4 random users and two groups: finance &amp;amp; sales.
Add two users to the first group and two to the second group. Create 2 directories: /data/finance &amp;amp; /data/sales.
Make the group sales the group owner of the directory sales, and make the group finance the group owner of the directory finance.</description>
            <content type="html"><![CDATA[
    <img src="/img/laptop.png"  alt="Kinsta Inc. logo"  class="center"  />


<p>In the previous two posts we learned about basic and advanced permissions, Access Control Lists, umask and extended-attributes.<br>
Let&rsquo;s put this knowledge to work by setting up a shared group environment</p>
<h2 id="lab-objectives">Lab Objectives</h2>
<ol>
<li>Create 4 random users and two groups: <code>finance</code> &amp; <code>sales</code>.<br>
Add two users to the first group and two to the second group.</li>
<li>Create 2 directories: <code>/data/finance</code> &amp; <code>/data/sales</code>.<br>
Make the group <code>sales</code> the group owner of the directory <code>sales</code>, and make the group <code>finance</code> the group owner of the directory <code>finance</code>. Make sure the user owner of the directories is <code>root</code>.<br>
Group owners should have full access to their directories and, no permissions should be assigned to the others entity.</li>
<li>The others entity should have no permissions on newly created files and directory within the <em>entire</em> <code>/data</code> structure.</li>
<li>Set the permissions so that members of the group <code>sales</code> can read files in the <code>/data/finance</code> directory, and members of the group <code>finance</code> can read files in the <code>/data/sales</code> directory.</li>
<li>Ensure that all new files and directories inherit the group owner of their respective directory.</li>
<li>Ensure that users are only allowed to remove files of which they are the owner.</li>
</ol>
<h2 id="lab-solution">Lab Solution</h2>
<h3 id="objective-1">Objective 1</h3>
<p>We&rsquo;ll create the <code>finance</code> and <code>sales</code> group, as well as the users <code>betty, bob, bill</code> and <code>bea</code>. We&rsquo;ll add <code>betty</code> and <code>bob</code> to <code>finance</code>, and <code>bill</code> and <code>bea</code> to <code>sales</code>.</p>
<pre><code>[root@server1 ~]# groupadd finance &amp;&amp; groupadd sales
[root@server1 ~]# for i in betty bob bill bea; do useradd $i; done
[root@server1 ~]# usermod -aG finance betty &amp;&amp; usermod -aG finance bob
[root@server1 ~]# usermod -aG sales bill &amp;&amp; usermod -aG sales bea
[root@server1 ~]#
</code></pre><h3 id="objective-2">Objective 2</h3>
<pre><code>[root@server1 ~]# groupadd finance &amp;&amp; groupadd sales
[root@server1 ~]# for i in betty bob bill bea; do useradd $i; done
[root@server1 ~]# usermod -aG finance betty &amp;&amp; usermod -aG finance bob
[root@server1 ~]# usermod -aG sales bill &amp;&amp; usermod -aG sales bea
[root@server1 ~]# mkdir -p /data/finance /data/sales
[root@server1 ~]# chown :finance /data/finance
[root@server1 ~]# chown :sales /data/sales
[root@server1 ~]# chown root /data/finance
[root@server1 ~]# chown root /data/sales
[root@server1 ~]# chmod g+rwx /data/finance
[root@server1 ~]# chmod g+rwx /data/sales
[root@server1 ~]# chmod o-rwx /data/finance
[root@server1 ~]# chmod o-rwx /data/sales
[root@server1 ~]# ls -l /data
total 0
drwxrwx---. 2 root finance 6 May 10 20:16 finance
drwxrwx---. 2 root sales 6 May 10 20:16 sales
[root@server1 ~]#
</code></pre><h3 id="objective-3">Objective 3</h3>
<p>We set the default ACL for the <em>others</em> entity to no permissions recursively, these permissions will apply to all newly created files and directories. Then apply a regular ACL.</p>
<pre><code>[root@server1 ~]# setfacl -R -m d⭕:- /data
[root@server1 ~]# setfacl -R -m o::- /data
</code></pre><p>We&rsquo;ll make sure our two groups can access the <code>/data</code> directory and any new sub-directory.</p>
<pre><code>[root@server1 /]# setfacl -m d:g:sales:rx,d:g:finance:rx /data
[root@server1 /]# setfacl -m g:sales:rx,g:finance:rx /data
</code></pre><p>You can verify your work using <code>getfacl</code>.</p>
<h3 id="objective-4">Objective 4</h3>
<p>We start by setting the default ACLs:</p>
<pre><code>[root@server1 ~]# setfacl -m d:g:sales:rx,d:g:finance:rwx /data/finance/
[root@server1 ~]# setfacl -m d:g:finance:rx,d:g:sales:rwx /data/sales
</code></pre><p>And afterward, we set the ACLs for the current files and directories (although there are none, it&rsquo;s best practice):</p>
<pre><code>[root@server1 ~]# setfacl -R -m g:sales:rx,g:finance:rwx /data/finance
[root@server1 ~]# setfacl -R -m g:finance:rx,g:sales:rwx /data/sales
</code></pre><p>Remember that we need to set <em>execute</em> permissions on the directory level in order to be able to read a file contained inside that directory.</p>
<h3 id="objective-5">Objective 5</h3>
<p>We set the SGUID permission on both the <code>sales</code> and <code>finance</code> directories:</p>
<pre><code>[root@server1 ~]# chmod g+s /data/sales
[root@server1 ~]# chmod g+s /data/finance
[root@server1 ~]# ls -l /data
total 0
drwxrws---+ 2 root finance 6 May 10 20:16 finance
drwxrws---+ 2 root sales 6 May 10 20:16 sales
</code></pre><h3 id="objective-6">Objective 6</h3>
<p>We set the Sticky Bit permission on both directories:</p>
<pre><code>[root@server1 ~]# chmod +t /data/sales
[root@server1 ~]# chmod +t /data/finance
[root@server1 ~]# ls -l /data
total 0
drwxrws--T+ 2 root finance 6 May 10 20:16 finance
drwxrws--T+ 2 root sales 6 May 10 20:16 sales
</code></pre><h2 id="verifying-the-solution">Verifying the solution</h2>
<p>We&rsquo;ll test the applied permissions for some of the users we created.</p>
<p>Login as <code>bill</code> from <code>sales</code> and create a file:</p>
<pre><code>[root@server1 ~]# su - bill
[bill@server1 ~]$ echo 'Hello!' &amp;gt; /data/sales/bill_file
[bill@server1 ~]$ exit
[root@server 1 ~]#
</code></pre><p>Let&rsquo;s check what <code>betty</code> from <code>finance</code> can do:</p>
<pre><code>[root@server1 ~]# su - betty
[betty@server1 ~]$ cd /data
[betty@server1 data]$ ls
finance sales
[betty@server1 data]$ cd finance/
[betty@server1 finance]$ ls
[betty@server1 finance]$ touch betty_file
[betty@server1 finance]$ ls -l
total 0
-rw-rw----+ 1 betty finance 0 May 10 20:45 betty_file
[betty@server1 finance]$ cd /data/sales/
[betty@server1 sales]$ ls
bill_file
[betty@server1 sales]$ cat bill_file
Hello!
[betty@server1 sales]$ rm bill_file
rm: remove write-protected regular file 'bill_file'? y
rm: cannot remove 'bill_file': Permission denied
[betty@server1 sales]$ touch betty_sales
touch: cannot touch 'betty_sales': Permission denied
[betty@server1 sales]$
</code></pre><p>We see that <code>betty</code> can create new files in <code>/data/finance</code>, and the group owner is set to <code>finance</code>. Betty can read files in <code>/data/sales</code> but can&rsquo;t delete or create new files.</p>
<p>Let&rsquo;s login as <code>bob</code> from finance:</p>
<pre><code>[root@server1 /]# su - bob
[bob@server1 ~]$ cd /data/finance/
[bob@server1 finance]$ cat betty_file
Hello!
[bob@server1 finance]$ rm betty_file
rm: cannot remove 'betty_file': Operation not permitted
[bob@server1 finance]$
</code></pre><p>Bob can do everything Betty can, but won&rsquo;t be able to delete files that don&rsquo;t belong to him.</p>
<blockquote>
<h4 id="important">Important!</h4>
<p><strong>The Sticky Bit permission can not be inherited from the parent directory.</strong>
This means that if Bob creates a new directory in <code>/data/finance</code> but doesn&rsquo;t change the permissions on that directory, then Betty will be able to read, write and execute files in Bob&rsquo;s new directory.<br>
Bob can set any permission he likes on his new directory since he&rsquo;s the user owner.</p>
</blockquote>
]]></content>
        </item>
        
        <item>
            <title>Access Control Lists, umask &amp; User-Extended Attributes 101</title>
            <link>https://joerismissaert.dev/access-control-lists-umask-user-extended-attributes-101/</link>
            <pubDate>Sun, 10 May 2020 07:12:26 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/access-control-lists-umask-user-extended-attributes-101/</guid>
            <description>Access Control Lists allow you to add permissions to more than one user or group on the same file or directory, and allows you to set default permissions for newly created files and directories.
Managing ACLs It&amp;rsquo;s possible you need need to add file system support for ACLs during boot time by adding the acl mount option the the /etc/fstab file. That would be the case when you&amp;rsquo;re seeing the Operation not supported message when applying an ACL.</description>
            <content type="html"><![CDATA[
    <img src="/img/Tux.png"  alt="Tux"  class="center"  style="width: 100px;"  />


<p>Access Control Lists allow you to add permissions to more than one user or group on the same file or directory, and allows you to set default permissions for newly created files and directories.</p>
<h2 id="managing-acls">Managing ACLs</h2>
<p>It&rsquo;s possible you need need to add file system support for ACLs during boot time by adding the <code>acl</code> mount option the the <code>/etc/fstab</code> file. That would be the case when you&rsquo;re seeing the <code>Operation not supported</code> message when applying an ACL.</p>
<h3 id="viewing-and-changing-acl-settings">Viewing and Changing ACL Settings</h3>
<p>The <code>ls -l</code> command doesn&rsquo;t show any existing ACLs, it will show a <code>+</code> after the permission listing to indicate ACL permissions have been set:</p>
<pre><code>drwxrwsr-x+ 2 joeri joeri 4096 May 9 16:14 somedirectory/
</code></pre><p>To show the current ACL settings, use the <code>getfacl</code> command:</p>
<pre><code>$ getfacl somedirectory
# file: somedirectory
# owner: joeri
# group: joeri
# flags: -s-
user::rwx
group::rwx
group:joeri:rwx
mask::rwx
other::r-x
</code></pre><p>You can see the permissions are shown for the usual three entities.<br>
We use the <code>setfacl</code> command to add an ACL:</p>
<ul>
<li><code>setfacl -m g:users:rwx somedirectory</code> - Sets the RWX permissions for the group <em>users</em></li>
<li><code>setfacl -m u:joeri:rwx somedirectory</code> - Sets the RWX permissions for the user <em>joeri</em></li>
<li><code>setfacl -m g:sales:- somedirectory</code> - Removes all permissions for the group sales</li>
<li><code>setfacl -R -m o::- somedirectory</code> - Recursively removes all permissions for the <em>others</em> entitiy</li>
<li><code>setfacl -x g:sales somedirectory</code> - Removes the ACL for the group <em>sales</em>.</li>
</ul>
<h3 id="working-with-default-acls">Working with default ACLs</h3>
<p>Default ACLs allows you to enable inheritance. The default ACL will set the permissions on all new items that are created in a given directory, but will not change the permissions for existing files and subdirectories.</p>
<blockquote>
<p>Usually you will set ACLs twice:<br>
<code>setfacl -R -m</code> to modify the ACL for current files.<br>
<code>setfacl -m d:</code> to take care of all new items.</p>
</blockquote>
<p><code>setfacl -m d:g:sales:rx somedirectory</code> Would set the read and execute permissions for the <em>sales</em> group on all new files and subdirectories.</p>
<p>Always set basic permissions first before applying ACLs, and avoid changing the basic permissions after applying ACLs.</p>
<h2 id="setting-default-permissions-with-umask">Setting Default Permissions with umask</h2>
<p>When creating new files default permissions are set by the shell determined by the <code>umask</code> shell value which is applied during login.</p>
<p>The <code>umask</code> numerical value is substracted from the maximum permissions a file or directory can get, 666 and 777 respectively.</p>
<p>e.g. a <code>umask</code> setting of 022 will result in 644 for files (6-0, 6-2, 6-2) and 755 for directories (7-0, 7-2, 7-2).</p>
<p>There are two ways to change the <code>umask</code> setting, either for all users or for individual users. The value is set in <code>/etc/login.defs</code> for all users and in <code>~.bashrc_profile</code> for an individual user.</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Value</th>
<th>Files</th>
<th>Directories</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>RW</td>
<td>ALL</td>
</tr>
<tr>
<td>1</td>
<td>RW</td>
<td>RW</td>
</tr>
<tr>
<td>2</td>
<td>R</td>
<td>RX</td>
</tr>
<tr>
<td>3</td>
<td>R</td>
<td>R</td>
</tr>
<tr>
<td>4</td>
<td>W</td>
<td>WX</td>
</tr>
<tr>
<td>5</td>
<td>W</td>
<td>WX</td>
</tr>
<tr>
<td>6</td>
<td>-</td>
<td>X</td>
</tr>
<tr>
<td>7</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>

<h2 id="working-with-user-extended-attributes">Working with User-Extended Attributes</h2>
<p>When working with permissions, there&rsquo;s always been a relationship between a user or a group, and a file or directory. This isn&rsquo;t the case with User-Extended Attributes, they do their work regardless of the user or group who accesses a file.</p>
<p>Just like ACLs, it&rsquo;s possible you need to add the <code>user_xattr</code> mount option.</p>
<p>There&rsquo;s a lot of attributes you can apply to a file, but I&rsquo;ll only cover one in particular which seems the most useful to me: The immutable attribute.<br>
The immutable attribute makes the file immutable, no changes can be made at all to file.</p>
<p>You can set an attribute using the <code>chattr</code> command:<br>
<code>chattr +i somefile</code> adds the immutable attribute to the file.<br>
Similarly, you can remove it using <code>chattr -i somefile</code>.</p>
<p>To list the current applied attributes use the <code>lsattr</code> command on a file.</p>
]]></content>
        </item>
        
        <item>
            <title>Basic &amp; Advanced Permissions 101</title>
            <link>https://joerismissaert.dev/basic-advanced-permissions-101/</link>
            <pubDate>Sat, 09 May 2020 12:23:40 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/basic-advanced-permissions-101/</guid>
            <description>To get access to files on Linux, permissions are used. These permissions are assigned to three entities: the file owner, the group owner, and the others entity.
Managing File Ownership Displaying Ownership Every file and directory on Linux has two owners: a user owner and a group owner. There is also the &amp;ldquo;others&amp;rdquo; entity. The user , group and others are shown when listing permissions with the ls -l command.</description>
            <content type="html"><![CDATA[
    <img src="/img/Tux.png"  alt="Tux"  class="center"  style="width: 100px;"  />


<p>To get access to files on Linux, permissions are used. These permissions are assigned to three entities: the file owner, the group owner, and the others entity.</p>
<h2 id="managing-file-ownership">Managing File Ownership</h2>
<h3 id="displaying-ownership">Displaying Ownership</h3>
<p>Every file and directory on Linux has two owners: a user owner and a group owner. There is also the &ldquo;others&rdquo; entity. The <em>user</em> , <em>group</em> and <em>others</em> are shown when listing permissions with the <code>ls -l</code> command. The below <code>test</code> file has read/write permissions for the user owner and group owner and read permissions for anyone else.</p>
<pre><code>$ ls -l
# -rw-rw-r--. 1 joeri joeri 0 May  9 14:29 test
</code></pre><p>File ownership is checked in a specific order. The shell checks:</p>
<ol>
<li>if you are the user owner. If you are, the shells stops checking here.</li>
<li>if you have obtained permissions through user-assigned Access Control Lists (more on that in a next post).</li>
<li>if you are a member of the group owner.</li>
<li>if you have obtained permissions through a group ACL.</li>
<li>if you&rsquo;re not the user owner nor a member of the group owner and haven&rsquo;t obtained any permissions through ACLs, you get the permissions of the <em>others</em> entity.</li>
</ol>
<p>You can quickly find files owned by a specific user or group using the <code>find</code> command, e.g. <code>find / -user joeri</code> or <code>find / -group users</code></p>
<h3 id="changing-user-ownership">Changing User Ownership</h3>
<p><code>chown who what</code> will change file or directory ownership, e.g. <code>chown joeri myfile</code> will set the user <em>joeri</em> as the owner of <em>testfile.</em></p>
<p>A particular useful option is the <code>-R</code> flag which allows you to change ownership recursively on a directory and everything below:<br>
<code>chown -R joeri /home/joeri</code></p>
<h3 id="changing-group-ownership">Changing Group Ownership</h3>
<p>There are two commands to change group ownership: <code>chown</code> and <code>chgrp</code></p>
<p><code>chgrp users /home/account</code> will set the group owner to <em>users</em> on the <em>/home/account</em> directory. You can use the <code>-R</code> option as well.</p>
<p>With the <code>chown</code> command there are several ways to change the group owner:</p>
<ul>
<li><code>chown joeri.users myfile</code><br>
Sets the user <em>joeri</em> as user owner and the group <em>users</em> as the group owner.</li>
<li><code>chown joeri:users myfile</code></li>
<li><code>chown .users myfile</code></li>
<li><code>chown :users myfile</code></li>
</ul>
<p>Again, the <code>-R</code> option is available.</p>
<h3 id="default-ownership">Default Ownership</h3>
<p>The user who creates a file automatically becomes the user owner, and the primary group of that user automatically becomes group owner.<br>
If a user is a member of more groups, the effective primary group can be changed temporarily with the <code>newgrp</code> command so that new files will get a new group as group owner.</p>
<p>Check the effective primary group with the <code>groups</code> command. The primary group is the first group: <em>joeri</em></p>
<pre><code>$ groups joeri
joeri: joeri users
</code></pre><p>Change the effective primary group using the <code>newgrp</code> command, and undo the group change using <code>exit</code>.</p>
<pre><code>$ newgrp users
$ groups joeri
joeri: users joeri
$ touch file1
-rw-rw-r--. 1 joeri users 0 May 9 14:29 file1
$ exit
$ groups joeri
joeri: joeri users
</code></pre><h2 id="managing-basic-permissions">Managing Basic Permissions</h2>
<h3 id="understanding-read-write-and-execute-permissions">Understanding Read, Write and Execute Permissions</h3>
<p>The three basic permissions allow users to read, write and execute files. The effect of these permissions differ when applied to files or directories.</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Permission</th>
<th>Applied to Files</th>
<th>Applied to Directories</th>
</tr>
</thead>
<tbody>
<tr>
<td>Read</td>
<td>Open a file</td>
<td>List content</td>
</tr>
<tr>
<td>Write</td>
<td>Change content</td>
<td>Create and delete files</td>
</tr>
<tr>
<td>Execute</td>
<td>Run a program</td>
<td>Change into directory</td>
</tr>
</tbody>
</table>

<h4 id="applying-basic-permissions">Applying Basic Permissions</h4>
<p>You use the <code>chmod</code> command to apply permissions. These can be set for user, group and others in either absolute/numeric or relative mode:</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Permission</th>
<th>Absolute</th>
<th>Relative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Read</td>
<td>4</td>
<td>R</td>
</tr>
<tr>
<td>Write</td>
<td>2</td>
<td>W</td>
</tr>
<tr>
<td>Execute</td>
<td>1</td>
<td>X</td>
</tr>
</tbody>
</table>

<p>When setting absolute permissions, you calculate the value you need. For example, setting Read+Write+Execute (4+2+1) permissions for the owner and Read+Execute (4+1) permissions for the group owner and others you would use: <code>chown 755 somefile</code></p>
<p>Changing permissions in absolute mode will replace all current permissions. If instead you want to modify permission relative to the current ones you work with three indicators: <strong>u</strong>, <strong>g</strong>, <strong>o</strong> for User, Group and Others.<br>
<code>chmod u+rwx, g-w, o-rwx somefile</code> would set all permissions for the user owner, remove the write permission for the group owner and remove all permissions for others.</p>
<p>To set execute permissions recursively to all directories but <em>not</em> to files, you can use the uppercase X: <code>chown -R o+rX /somedirectory</code></p>
<h2 id="managing-advanced-permissions">Managing Advanced Permissions</h2>
<p>Understanding Advanced Permissions</p>
<p>There are three advanced permissions: SUID (set user id), SGID (set group id) and Sticky Bit.</p>
<p>Applying the SUID permission to a file will allow a user to execute that file as if he had the user owner rights on the file. A file that has root as user ower and has the SUID permission will be executed as <em>root</em> by the user even if that user isn&rsquo;t root at all. An example is the <code>passwd</code> command:</p>
<pre><code>$ ls -l /usr/bin/passwd&lt;br /&gt;-rwsr-xr-x. 1 root root 37600 Jan 30 02:12 /usr/bin/passwd
</code></pre><p><code>passwd</code> is owned by root and has the SUID permission, notice the <code>s</code> where the <code>x</code> should be in the user owner permissions.</p>
<p>You can set the SUID permission by executing <code>chown u+s somefile</code></p>
<p>The second advanced permission is SGID. If applied to an executable file it gives the user who executes the file the permissions of the group owner of that file. Applied to a directory it will set the default group ownership on files and subdirectories inside that directory:</p>
<pre><code>$ mkdir somedirectory
$ ls -ld somedirectory
drwxrwxr-x. 2 joeri joeri 4096 May 9 16:10 somedirectory
$ chown .users somedirectory
$ ls -ld somedirectory
drwxrwxr-x. 2 joeri users 4096 May 9 16:10 somedirectory
$ chmod g+s somedirectory
$ ls -ld somedirectory
drwxrwsr-x. 2 joeri users 4096 May 9 16:10 somedirectory
$ cd somedirectory &amp;&amp; touch somefile &amp;&amp; ls -l
-rw-rw-r--. 1 joeri users 0 May 9 16:14 somefile
</code></pre><p>The third advanced permission is Sticky Bit. This permission is useful to protect files against accidental deletion in an environment where multiple users have write permissions in the same directory.</p>
<p>If the Sticky Bit permission is applied to a directory, a user can delete a file only if he is the owner of the file or of the directory that contains the file.<br>
<em><strong>The Sticky Bit permission can not be inherited from the parent directory.</strong></em></p>
<p>Use <code>chmod +t</code> followed by the name of the file or directory you want to apply the permission on.</p>





<table class="table table-dark table-striped table-bordered">
<thead>
<tr>
<th>Permission</th>
<th>Absolute</th>
<th>Relative</th>
<th>Files</th>
<th>Directories</th>
</tr>
</thead>
<tbody>
<tr>
<td>SUID</td>
<td>4</td>
<td>u+s</td>
<td>Execute file with the permissions of the file owner</td>
<td>n/a</td>
</tr>
<tr>
<td>SGID</td>
<td>2</td>
<td>g+s</td>
<td>Execute file with the permissions of the group owner</td>
<td>Files created in the directory get the same group owner.</td>
</tr>
<tr>
<td>Sticky Bit</td>
<td>1</td>
<td>+t</td>
<td>n/a</td>
<td>Prevent users from deleting files belonging to other users.</td>
</tr>
</tbody>
</table>

]]></content>
        </item>
        
        <item>
            <title>CloudFlare Authenticated Origin Pulls</title>
            <link>https://joerismissaert.dev/cloudflare-authenticated-origin-pulls/</link>
            <pubDate>Thu, 30 Apr 2020 16:57:40 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/cloudflare-authenticated-origin-pulls/</guid>
            <description>In addition to my previous post on blocking requests that are hitting my websites directly without going through the CloudFlare network, we can enable the Authenticated Origin Pulls feature.
Authenticated Origin Pulls uses TLS Authentication to verify that the server hosting my website is communicating with CloudFlare and not some other server or client.
Nginx will be configured to only accept requests which use a valid client certificate from Cloudflare and requests which have not passed through CloudFlare will be dropped: The server will respond with a 400 Bad Request status code.</description>
            <content type="html"><![CDATA[
    <img src="/img/cf-logo-v-rgb.png"  alt="CloudFlare Logo"  class="center"  />


<p>In addition to <a href="/blocking-requests-not-originating-from-cloudflare-on-nginx">my previous post</a>

 on blocking requests that are hitting my websites directly without going through the CloudFlare network, we can enable the Authenticated Origin Pulls feature.</p>
<p>Authenticated Origin Pulls uses TLS Authentication to verify that the server hosting my website is communicating with CloudFlare and not some other server or client.</p>
<p>Nginx will be configured to only accept requests which use a valid client certificate from Cloudflare and requests which have not passed through CloudFlare will be dropped: The server will respond with a 400 Bad Request status code.</p>

    <img src="/img/http_400.png"  alt="CloudFlare Logo"  class="center"  />


<p>Let&rsquo;s start by downloading the CloudFlare origin <em>pull</em> certificate from <a href="https://support.cloudflare.com/hc/en-us/article_attachments/360044928032/origin-pull-ca.pem" target="_blank">here</a>

, and put it in an appropriate location. I&rsquo;ve renamed the certificate to <code>cloudflare.crt</code>.</p>
<pre><code># cd /etc/ssl/certs
# wget https://support.cloudflare.com/hc/en-us/article_attachments/360044928032/origin-pull-ca.pem
# mv origin-pull-ca.pem cloudflare.crt
</code></pre><p>Next, we need to specify where Nginx can find this certificate in our Nginx configuration Server block. Add the following to your server block after your already existing <code>ssl_certificate</code> and <code>ssl_certificate_key</code> directives:</p>
<pre><code>ssl_client_certificate /etc/ssl/certs/cloudflare.crt;
ssl_verify_client on;
</code></pre><p>The configuration should look similar to this:</p>
<pre><code>ssl on;
ssl_certificate /etc/ssl/certs/website.pem;
ssl_certificate_key /etc/ssl/private/website_privatekey.pem;
ssl_client_certificate /etc/ssl/certs/cloudflare.crt;
ssl_verify_client on;
</code></pre><p>Save the file and exit the text editor.<br>
Test the configuration changes by executing:</p>
<pre><code># nginx -t
</code></pre><p>If there were no problems, go ahead and apply the configuration by reloading Nginx:</p>
<pre><code># nginx -s reload
</code></pre><p>If you visit the website now, you should see the <em>400 Bad Request</em> error. That means everything is working as intended and as a final step we will need to enable the Authenticated Origin Pulls feature in CloudFlare.</p>
<p>Open the <em>SSL/TLS</em> section in the Cloudflare dashboard, head to the <em>Origin Server</em> subsection and toggle the <em>Authenticated Origin Pulls</em> option to <em>On</em>.</p>

    <img src="/img/cd_originpulls.png"  alt="CloudFlare Authenticated Origin Pull Setting"  class="center"  />


<p>Authenticated Origin Pulls on the Nginx server is now set up correctly to ensure that Nginx only accepts requests from Cloudflare’s servers, preventing anyone else from directly connecting to the Nginx server.</p>
<h3 id="optional">Optional</h3>
<p>While I was implementing this I decided to replace my Let&rsquo;s Encrypt certificates with CloudFlare&rsquo;s Origin Certificate (not origin pull certificate).</p>
<p>Origin Certificates are only valid for encryption between Cloudflare and the origin server, but that&rsquo;s okay since Authenticated Origin Pulls only allows traffic going through CloudFlare to connect to my server anyway.</p>
<p>You can create or download the Origin Certificate in the same section where you enabled Authenticated Origin Pulls.</p>

    <img src="/img/cf_origincerts-1.png"  alt="CloudFlare Origin Certificate"  class="center"  />


<p>If you don&rsquo;t have the private key, you will need to create a new certificate and either provide your own private key and CSR, or let CloudFlare generate both for you.</p>

    <img src="/img/cf_origincerts_create.png"  alt="Create CloudFlare Origin Certificate"  class="center"  />


<p>On the next screen, choose the PEM format and copy the content of the Origin Certificate section to a file called <code>yourdomain.com.pem</code> and the content of the private key section to a file called <code>yourdomain.com.key</code>.</p>
<p>Upload both files to your server and place them in the following paths:<br>
<code>/etc/ssl/certs/yourdomain.com.pem</code><br>
<code>/etc/ssl/private/yourdomain.com.key</code></p>
<p>Just like before, we need to tell Nginx where to find those files:</p>
<pre><code>ssl on;
ssl_certificate /etc/ssl/certs/yourdomain.com.pem;
ssl_certificate_key /etc/ssl/private/yourdomain.com.key;
ssl_client_certificate /etc/ssl/certs/cloudflare.crt;
ssl_verify_client on;
</code></pre><p>Test and reload the Nginx configuration.</p>
]]></content>
        </item>
        
        <item>
            <title>Blocking requests not originating from CloudFlare on Nginx</title>
            <link>https://joerismissaert.dev/blocking-requests-not-originating-from-cloudflare-on-nginx/</link>
            <pubDate>Sat, 25 Jan 2020 07:36:18 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/blocking-requests-not-originating-from-cloudflare-on-nginx/</guid>
            <description>My websites are behind CloudFlare , which acts as a reverse proxy and which can help in mitigating attacks, malicious traffic and requests.
CloudFlare is masking the real IP address of this site. If you look up the DNS A record for this domain, you&amp;rsquo;ll see one of CloudFlare&amp;rsquo;s IP addresses. Essentially, CloudFlare is forwarding traffic from their servers to the server where my sites are hosted.
The inconvenience of this is that I can&amp;rsquo;t see the real IP address of the visitor, instead, I&amp;rsquo;m seeing CloudFlare&amp;rsquo;s IP addresses in the Nginx log.</description>
            <content type="html"><![CDATA[
    <img src="/img/cf-logo-v-rgb.png"  alt="CloudFlare Logo"  class="center"  />


<p>My websites are behind <a href="https://www.cloudflare.com/" target="_blank">CloudFlare</a>

, which acts as a reverse proxy and which can help in mitigating attacks, malicious traffic and requests.</p>
<p>CloudFlare is masking the real IP address of this site. If you look up the DNS A record for this domain, you&rsquo;ll see one of CloudFlare&rsquo;s IP addresses. Essentially, CloudFlare is forwarding traffic from their servers to the server where my sites are hosted.</p>
<p>The inconvenience of this is that I can&rsquo;t see the real IP address of the visitor, instead, I&rsquo;m seeing CloudFlare&rsquo;s IP addresses in the Nginx log.</p>
<p>Luckily, <a href="https://support.cloudflare.com/hc/en-us/articles/200170786-Restoring-original-visitor-IPs-Logging-visitor-IP-addresses-with-mod-cloudflare-" target="_blank">Cloudflare includes the original visitor IP address</a>

 in the <code>X-Forwarded-For</code> and <code>CF-Connecting-IP</code> headers. I would only need to make a simple Nginx configuration change, and, with this information, I can also block anyone who happens to know the real IP address my server and could be ypassing CloudFlare.</p>
<p>Inside <code>/etc/nginx</code>, I created an additional configuration file, <code>cloudflare_ips.conf</code>, where I map two variables using the <a href="http://nginx.org/en/docs/http/ngx_http_geo_module.html" target="_blank">ngx_http_geo_module</a>

. If the remote IP is a  <a href="https://www.cloudflare.com/ips/" target="_blank">CloudFlare IP address</a>

, then I set it as allowed.</p>
<pre><code>geo $remote_addr $is_allowed_ip  {
    173.245.48.0/20 yes;
    103.21.244.0/22 yes;
    103.22.200.0/22 yes;
    103.31.4.0/22 yes;
    141.101.64.0/18 yes;
    108.162.192.0/18 yes;
    190.93.240.0/20 yes;
    188.114.96.0/20 yes;
    197.234.240.0/22 yes;
    198.41.128.0/17 yes;
    162.158.0.0/15 yes;
    104.16.0.0/12 yes;
    172.64.0.0/13 yes;
    131.0.72.0/22 yes;
	2400:cb00::/32 yes;
	2606:4700::/32 yes;
	2803:f800::/32 yes;
	2405:b500::/32 yes;
	2405:8100::/32 yes;
	2a06:98c0::/29 yes;
	2c0f:f248::/32 yes;
    default no;
}
</code></pre><p>You can then add the following <code>if</code> block into the site&rsquo;s server block configuration. Basically, if the remote IP is not in the list above, the <code>$is_allowed_ip</code> variable will be set to <code>no</code> and a <code>403 Forbidden</code> status code is returned.</p>
<pre><code>if ($is_allowed_ip = no ){ return 403; }
</code></pre><p>We shouldn&rsquo;t forget the include the <code>cloudflare_ips.conf</code> file inside our main <code>nginx.conf</code>. Add the below into the <code>http</code> block.</p>
<pre><code>include /etc/nginx/cloudflare_ips.conf;
</code></pre><p>The last step would be to modify the existing log format, so that it includes the real ip address of the visitor, in addition to the CloudFlare IP. The log format can be found in <code>nginx.conf</code>. You would need to add the<code>$http_x_forwarded_for</code> variable in the <code>log_format</code> directive.</p>
<p>Test the Nginx config and reload Nginx to apply the above changes.</p>
]]></content>
        </item>
        
        <item>
            <title>Automate Let’s Encrypt SSL with Certbot on Centos 8</title>
            <link>https://joerismissaert.dev/automate-lets-encrypt-ssl-with-certbot-on-centos-8/</link>
            <pubDate>Fri, 10 Jan 2020 08:30:48 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/automate-lets-encrypt-ssl-with-certbot-on-centos-8/</guid>
            <description>An SSL Certificate is a text file with encrypted data that you install on your server so that you can secure/encrypt sensitive communications between your site and your visitors. They are also used to verify that you are connected with the service you wish to be connecting with, and, as a website owner it validates your trustworthiness.
SSL certificates can be expensive, so here&amp;rsquo;s where Let’s Encrypt comes into play.</description>
            <content type="html"><![CDATA[
    <img src="/img/le-logo-wide.png"  alt="Let&#39;s Encrypt Logo"  class="center"  />


<p>An SSL Certificate is a text file with encrypted data that you install on your server so that you can secure/encrypt sensitive communications between your site and your visitors. They are also used to verify that you are connected with the service you wish to be connecting with, and, as a website owner it validates your trustworthiness.</p>
<p>SSL certificates can be expensive, so here&rsquo;s where <a href="https://letsencrypt.org/about/" target="_blank">Let’s Encrypt</a>

 comes into play.</p>
<blockquote>
<p>Let’s Encrypt is a free, automated, and open certificate authority (CA), run for the public’s benefit. It is a service provided by the <a href="https://www.abetterinternet.org" target="_blank">Internet Security Research Group (ISRG</a>

: <em>We give people the digital certificates they need in order to enable HTTPS (SSL/TLS) for websites, for free, in the most user-friendly way we can. We do this because we want to create a more secure and privacy-respecting Web.</em></p>
</blockquote>
<p>In order to have Let&rsquo;s Encrypt issue a valid certificate for your site, it needs to validate your domain. In other words, it needs to make sure that whoever is requesting the certificate is also in full control of the domain name it&rsquo;s being issued for.</p>
<p>There are different ways you can prove you&rsquo;re the owner of a specific domain:</p>
<ul>
<li>Provisioning a DNS record under <code>example.com</code>, or</li>
<li>Provisioning an HTTP resource under a well-known URI on <code>http://example.com/</code></li>
</ul>
<p>The first method is done by manually adding a specific DNS record to your domain. The latter is done automatically by the Let&rsquo;s Encrypt agent on your server.</p>
<p>In this guide, we&rsquo;ll be using the second method with <code>certbot</code> to install an SSL certificate in a matter of minutes. This method requires that your domain is pointing to the server you&rsquo;re running <code>certbot</code> on with its DNS.</p>
<blockquote>
<p><a href="https://certbot.eff.org/about/" target="_blank">Certbot</a>

 is a free, open source software tool for automatically using Let’s Encrypt certificates on manually-administrated websites to enable HTTPS.</p>
</blockquote>
<p>First, let&rsquo;s download <code>certbot</code>, copy it into our <code>PATH</code> and apply the necessary permissions:</p>
<pre><code># wget https://dl.eff.org/certbot-auto 
# mv certbot-auto /usr/local/bin/certbot-auto 
# chown root /usr/local/bin/certbot-auto 
# chmod 0755 /usr/local/bin/certbot-auto
</code></pre><p>When you run <code>certbot-auto --nginx</code>, certbot will look into your Nginx configuration files for domains it can request an SSL certificate for. You&rsquo;ll be presented with a menu:</p>

    <img src="/img/certbot-1.png"  alt="Certbot menu"  class="center"  />


<p>Enter the numbers of the domains you wish to generate an SSL certificate for, separated by commas or spaces and hit ENTER. Or leave the input blank to select all domains. <!-- raw HTML omitted --></p>

    <img src="/img/certbot-2.png"  alt="Certbot menu"  class="center"  />


<p>Choose if you wish to have the webserver enforce HTTPS traffic (option 2) or not (option 1).</p>
<p>You now have a valid SSL certificate on for your domain:</p>

    <img src="/img/certbot-3.png"  alt="Certbot menu"  class="center"  />


<p>Check your Nginx configuration to see what exactly was added by Certbot.</p>
]]></content>
        </item>
        
        <item>
            <title>Serving a Python Flask app with Gunicorn, Nginx and Systemd.</title>
            <link>https://joerismissaert.dev/serving-a-python-flask-app-with-gunicorn-nginx-and-systemd/</link>
            <pubDate>Tue, 31 Dec 2019 08:05:24 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/serving-a-python-flask-app-with-gunicorn-nginx-and-systemd/</guid>
            <description>This post explains how to serve a Python app from a Virtualenv with the Gunicorn WSGI and using Nginx as a proxy server.
You should already have Nginx installed, and have a sample Flask app in a Virtualenv.
Install Gunicorn in your Virtualenv Activate your Virtualenv and install Gunicorn by typing:
$ source myprojectenv/bin/activate (myprojectenv)$ pip install gunicorn Creating the WSGI Entry Point Next, we&amp;rsquo;ll create a file called wsgi.</description>
            <content type="html"><![CDATA[
    <img src="/img/icons8-python-64.png"  alt="Python Logo"  class="center"  />


<p>This post explains how to serve a Python app from a <a href="https://pypi.org/project/virtualenv/" target="_blank">Virtualenv</a>

 with the Gunicorn WSGI and using Nginx as a proxy server.<br>
You should already have Nginx installed, and have a sample Flask app in a Virtualenv.</p>
<h3 id="install-gunicorn-in-your-virtualenv">Install Gunicorn in your Virtualenv</h3>
<p>Activate your Virtualenv and install Gunicorn by typing:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ source myprojectenv/bin/activate
<span style="color:#f92672">(</span>myprojectenv<span style="color:#f92672">)</span>$ pip install gunicorn
</code></pre></div><h3 id="creating-the-wsgi-entry-point">Creating the WSGI Entry Point</h3>
<p>Next, we&rsquo;ll create a file called <em>wsgi.py</em> that will serve as the entry point for our application code in <em>app.py</em>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">(</span>myprojectenv<span style="color:#f92672">)</span>$ nano wsgi.py
</code></pre></div><p>We&rsquo;ll import the Flask application code from <em>app.py</em> inside the entry point file:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> app <span style="color:#f92672">import</span> app

<span style="color:#66d9ef">if</span> name <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;main&#34;</span>:
     app<span style="color:#f92672">.</span>run()
</code></pre></div><p>Save and close the file.</p>
<h3 id="testing-gunicorn">Testing Gunicorn</h3>
<p>We should check that Gunicorn can serve the application correctly.</p>
<p>We can do this by simply passing the <em>gunicorn</em> command the name of our entry point. This is the name of the entry point file (minus the .py extension), plus the name of the application. In this case, this is <em>wsgi:app</em>.</p>
<p>We’ll also specify a publicly available interface and port to bind to:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">(</span>myprojectenv<span style="color:#f92672">)</span>$ gunicorn --bind 0.0.0.0:5000 wsgi:app
</code></pre></div><p>You&rsquo;ll see similar output as:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">Jan <span style="color:#ae81ff">03</span> 22:13:26 odin gunicorn<span style="color:#f92672">[</span>347<span style="color:#f92672">]</span>: <span style="color:#f92672">[</span>2020-01-03 22:13:26 +0400<span style="color:#f92672">]</span> <span style="color:#f92672">[</span>347<span style="color:#f92672">]</span> <span style="color:#f92672">[</span>INFO<span style="color:#f92672">]</span> Starting gunicorn 19.9.0
Jan <span style="color:#ae81ff">03</span> 22:13:26 odin gunicorn<span style="color:#f92672">[</span>347<span style="color:#f92672">]</span>: <span style="color:#f92672">[</span>2020-01-03 22:13:26 +0400<span style="color:#f92672">]</span> <span style="color:#f92672">[</span>347<span style="color:#f92672">]</span> <span style="color:#f92672">[</span>&lt;code&gt;INFO<span style="color:#f92672">]</span> Listening at: http://0.0.0.0:5000 <span style="color:#f92672">(</span>28217<span style="color:#f92672">)</span>
Jan <span style="color:#ae81ff">03</span> 22:13:26 odin gunicorn<span style="color:#f92672">[</span>347<span style="color:#f92672">]</span>: <span style="color:#f92672">[</span>2020-01-03 22:13:26 +0400<span style="color:#f92672">]</span> <span style="color:#f92672">[</span>347<span style="color:#f92672">]</span> <span style="color:#f92672">[</span>INFO<span style="color:#f92672">]</span> Using worker: sync
Jan <span style="color:#ae81ff">03</span> 22:13:26 odin gunicorn<span style="color:#f92672">[</span>347<span style="color:#f92672">]</span>: <span style="color:#f92672">[</span>2020-01-03 22:13:26 +0400<span style="color:#f92672">]</span> <span style="color:#f92672">[</span>367<span style="color:#f92672">]</span> <span style="color:#f92672">[</span>INFO<span style="color:#f92672">]</span> Booting worker with pid: <span style="color:#ae81ff">367</span>
</code></pre></div><p>Visit your server’s IP address with <code>:5000</code> appended to the end in your web browser to see your application:</p>
<pre><code>http://your_server_ip:5000
</code></pre><p>When you have confirmed that it’s functioning properly, press <code>CTRL-C</code> in your terminal window and deactivate the virtual environment.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">(</span>myprojectenv<span style="color:#f92672">)</span>$ deactivate
</code></pre></div><h3 id="creating-the-systemd-script">Creating the Systemd Script</h3>
<p>A Systemd service unit file will allow the OS&rsquo;s init system to automatically start Gunicorn and serve the Flask application whenever the server boots.</p>
<p>Let&rsquo;s begin by creating a service unit file within the <code>/etc/systemd/system</code> directory:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo nano /etc/systemd/system/myproject.service
</code></pre></div><p>We&rsquo;ll add the <code>[Unit]</code> section which contains a description of the service and we&rsquo;ll only allow this service to start if required network services are running as well.</p>
<pre><code>[Unit]
Description=Gunicorn instance to serve myproject
After=network.target
</code></pre><p>In the <code>[Service]</code> section we&rsquo;ll specify the user and group we want our process to be running under. I&rsquo;m specifying my own username since it owns all of the files. So that Nginx can communicate with the Gunicorn processes, we&rsquo;ll give group ownership to the <em>www-data</em> group.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>Service<span style="color:#f92672">]</span>
User<span style="color:#f92672">=</span>joeri
Group<span style="color:#f92672">=</span>www-data
</code></pre></div><p>Next, let’s map out the working directory and set the <code>PATH</code> environmental variable so that the init system knows that the executables for the process are located within our virtual environment. Let’s also specify the command to start the service. This command will do the following:</p>
<ul>
<li>Start 3 worker processes</li>
<li>Create and bind to a Unix socket file, <code>myproject.sock</code>, within our project directory. We’ll set an umask value of <code>007</code> so that the socket file is created giving access to the owner and group, while restricting access to others.</li>
<li>Specify the WSGI entry point file name, along with the Python callable within that file (<code>wsgi:app</code>)</li>
</ul>
<p>Systemd requires that we give the full path to the Gunicorn executable, which is installed within our virtual environment.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">WorkingDirectory<span style="color:#f92672">=</span>/home/joeri/www/myproject
Environment<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;PATH=/home/joeri/www/myproject/myprojectenv/bin&#34;</span>
ExecStart<span style="color:#f92672">=</span>/home/joeri/www/myproject/myprojectenv/bin/gunicorn --workers <span style="color:#ae81ff">3</span> --bind unix:myproject.sock -m <span style="color:#ae81ff">007</span> wsgi:app
</code></pre></div><p>Next, we want this service to start when the system boots up in multi-user mode with networking: multi-user.target<br>
We do this by adding the [Install] section and specifying the target.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>Install<span style="color:#f92672">]</span>
WantedBy<span style="color:#f92672">=</span>multi-user.target
</code></pre></div><p>Our complete service unit file should look like the below:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>Unit<span style="color:#f92672">]</span>
Description<span style="color:#f92672">=</span>Gunicorn instance to serve myproject
After<span style="color:#f92672">=</span>network.target

<span style="color:#f92672">[</span>Service<span style="color:#f92672">]</span>
User<span style="color:#f92672">=</span>joeri
Group<span style="color:#f92672">=</span>www-data
WorkingDirectory<span style="color:#f92672">=</span>/home/joeri/www/myproject
Environment<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;PATH=/home/joeri/www/myproject/myprojectenv/bin&#34;</span>
ExecStart<span style="color:#f92672">=</span>/home/joeri/www/myproject/myprojectenv/bin/gunicorn --workers <span style="color:#ae81ff">3</span> --bind unix:myproject.sock -m <span style="color:#ae81ff">007</span> wsgi:app

<span style="color:#f92672">[</span>Install<span style="color:#f92672">]</span>
WantedBy<span style="color:#f92672">=</span>multi-user.target
</code></pre></div><p>We can now start our service and enable it at boot time:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo systemctl start myproject 
$ sudo systemctl enable myproject
</code></pre></div><h3 id="proxy-requests-with-nginx">Proxy requests with Nginx</h3>
<p>We can now configure Nginx to pass web requests to that socket by making some small additions to the nginx configuration file.</p>
<p>Verify if the <code>/etc/nginx/proxy_params</code> file exists. If not, create it with the following content:</p>
<pre><code>proxy_set_header Host $host;
proxy_set_header X-Real-IP $remote_addr;
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
client_max_body_size 100M;
client_body_buffer_size 1m;
proxy_intercept_errors on;
proxy_buffering on;
proxy_buffer_size 128k;
proxy_buffers 256 16k;
proxy_busy_buffers_size 256k;
proxy_temp_file_write_size 256k;
proxy_max_temp_file_size 0;
proxy_read_timeout 300;
</code></pre><p>Create a new Nginx configuration file for your application and add the below lines to it:</p>
<pre><code>sudo nano /etc/nginx/sites-available/myproject.conf

server {
     listen 80;
     server_name your_domain www.your_domain;

     location / {
         include proxy_params;
         proxy_pass http://unix:/home/joeri/www/myproject/myproject.sock;
     }
 }
</code></pre><p>Save and close the file when you’re finished.</p>
<p>To enable the Nginx server block configuration you’ve just created, link the file to the sites-enabled directory:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo ln -s /etc/nginx/sites-available/myproject /etc/nginx/sites-enabled
</code></pre></div><p>With the file in that directory, you can test for syntax errors:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo nginx -t
</code></pre></div><p>If no errors are returned, restart the Nginx process to read the new configuration:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ sudo systemctl restart nginx
</code></pre></div><p>You should now be able to navigate to your server’s domain name in your web browser and see your application&rsquo;s output. In the next post I&rsquo;ll set up SSL for this application.</p>
]]></content>
        </item>
        
        <item>
            <title>Installing Nginx and Setting up Server Blocks on CentOS 8</title>
            <link>https://joerismissaert.dev/installing-nginx-and-setting-up-server-blocks-on-centos-8/</link>
            <pubDate>Thu, 26 Dec 2019 08:29:00 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/installing-nginx-and-setting-up-server-blocks-on-centos-8/</guid>
            <description>In this post, I&amp;rsquo;ll walk through how to install Nginx and set up Nginx server blocks on CentOS 8, and, how to serve different content to different visitors depending on which domains they are requesting.
After initially installing CentOS, check if updates are available and apply them:
$ sudo dnf update -y firewalld Next, let&amp;rsquo;s install the firewalld daemon to secure the server, start the daemon and enable it at boot.</description>
            <content type="html"><![CDATA[
    <img src="/img/centos-logo-light.png"  alt="CentOS Logo"  class="center"  />


<p>In this post, I&rsquo;ll walk through how to install Nginx and set up Nginx server blocks on CentOS 8, and, how to serve different content to different visitors depending on which domains they are requesting.</p>
<p>After initially installing CentOS, check if updates are available and apply them:</p>
<pre><code>$ sudo dnf update -y
</code></pre><h2 id="firewalld">firewalld</h2>
<p>Next, let&rsquo;s install the <em>firewalld</em> daemon to secure the server, start the daemon and enable it at boot. We&rsquo;ll also configure the firewall daemon to only allow requests over the http and https protocols.</p>
<pre><code>$ sudo dnf install -y firewalld
$ sudo systemctl start firewalld
$ sudo systemctl enable firewalld
$ sudo firewall-cmd --zone=public --permanent --add-service=http
$ sudo firewall-cmd --zone=public --permanent --add-service=https
$ sudo firewall-cmd --reload
</code></pre><h2 id="install-nginx">Install Nginx</h2>
<p>We&rsquo;ll proceed with installing Nginx, starting the server and enabling the service at boot:</p>
<pre><code>$ sudo dnf install -y nginx
$ sudo systemctl start nginx
$ sudo systemctl enable nginx
</code></pre><p>We can run the following cURL command to test the service:</p>
<pre><code>$ curl http://localhost
</code></pre><p>You should see the HTML code of the Nginx test page, confirming that the service is running.</p>
<h2 id="creating-directory-structures-for-different-websites">Creating directory structures for different websites</h2>
<p>The example configuration in this guide will make one server block for <em>example.com</em> and another for <em>example2.com</em>.<br>
We&rsquo;ll configure DNS for these dummy domains locally in the <code>/etc/hosts</code> file.</p>
<p>First, we need to make a directory structure that will hold the site data to serve to visitors:</p>
<pre><code>$ sudo mkdir -p /var/www/example.com
$ sudo mkdir /var/www/example2.com
</code></pre><p>We now need to modify permissions on these directories so our regular user can make modifications to the files inside, and so that Nginx can read the files as well.</p>
<pre><code>$ sudo chown -R joeri:nginx /var/www/example.com
$ sudo chown -R joeri:nginx /var/www/example2.com
</code></pre><p>We&rsquo;ll also ensure read access for the user and group to the <code>/var/www/</code> directory and all files and directories inside:</p>
<pre><code>$ sudo chown -R 755 /var/www
</code></pre><h2 id="creating-demo-pages">Creating Demo Pages</h2>
<p>We need to create some content for Nginx to serve to visitors. We can do that by creating a simple HTML file inside the directories we previously created. As a regular user, create the HTML file and add your content:</p>
<pre><code>$ nano /var/www/example.com/index.html
&lt;html&gt; 
  &lt;body&gt;
   &lt;h1&gt;Welcome To Example.com&lt;/h1&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre><p>Save and exit, then do the same for /var/www/<em>example2.com</em>, replacing the content of the h1 element with <em>“Welcome To Example2.com”</em>.</p>
<h2 id="creating--enabling-the-server-block-files">Creating &amp; Enabling the Server Block Files</h2>
<p>Server block files specify the configuration of our separate sites and tell the Nginx web server how to respond to various domain requests.</p>
<p>I&rsquo;ll start by replicating the Ubuntu/Debian directory structure for Nginx server block files since I really like that method.</p>
<pre><code>$ sudo mkdir /etc/nginx/sites-available
$ sudo mkdir /etc/nginx/sites-enabled
</code></pre><p>Next, we tell Nginx to look for server blocks in the <code>sites-enabled</code> directory. Add the following line to the end of the <code>http {}</code> block in <em>/etc/nginx/nginx.conf</em>:</p>
<pre><code>include /etc/nginx/sites-enabled/*.conf;
</code></pre><p>Create the server block for the example.com site in the <em>/etc/nginx/sites-available/example.com.conf</em> file:</p>
<pre><code>$ sudo nano /etc/nginx/sites-available/example.com.conf
</code></pre><p>Add the following lines to the file:</p>
<pre><code>server {
    listen  80;
    server_name example.com www.example.com;

    location / {
        root  /var/www/example.com/html;
        index  index.html index.htm;
        try_files $uri $uri/ =404;
    }

    error_page  500 502 503 504  /50x.html;
    location = /50x.html {
        root  /usr/share/nginx/html;
    }
}
</code></pre><p>Do the same for the <em>example2.com</em> site, replacing <em>example.com</em> with <em>example2.com</em>.</p>
<p>We can now enable both server block files by creating a symlink to the <em>/etc/nginx/sites-enabled</em> directory:</p>
<pre><code>$ sudo ln -s /etc/nginx/sites-available/example.com.conf /etc/nginx/sites-enabled/example.com.conf

$ sudo ln -s /etc/nginx/sites-available/example2.com.conf /etc/nginx/sites-enabled/example2.com.conf
</code></pre><p>Test and restart Nginx to make the changes take effect.</p>
<pre><code>$ sudo nginx -t
$ sudo systemctl restart nginx
</code></pre><h2 id="creating-dns-entries">Creating DNS entries</h2>
<p>We have two dummy domains, but they aren&rsquo;t pointing to our Nginx server with their DNS. You can create a local DNS entry for those domains in <em>/etc/hosts</em>. I&rsquo;m doing this locally on the machine that&rsquo;s running the webserver:</p>
<pre><code>$ sudo nano /etc/hosts
127.0.0.1 localhost example.com www.example.com example2.com www.example2.com
</code></pre><p>On a remote (Linux) machine you would add an additional line, starting with the IP address of your server:</p>
<pre><code>192.168.100.99 example.com www.example.com example2.com www.example2.com
</code></pre><p>Our machine can now resolve those domains to a specific server.</p>
<h2 id="testing-results">Testing results</h2>
<p>Now that DNS is in place, we can run some tests. Either with cURL or by opening the domains in your browser.</p>
<pre><code>$ curl http://example.com
$ curl http://example2.com
</code></pre><p>You should see different outputs depending on the domain that was called.</p>
<p>If you keep seeing a 403 Forbidden error, and, you have SELinux enabled, you will want to relabel the <code>/var/www/</code> directory with the appropriate SELinux context label:</p>
<pre><code>$ sudo restorecon -Rv /var/www/
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Mounting an unclean NTFS file system</title>
            <link>https://joerismissaert.dev/mounting-an-unclean-ntfs-file-system/</link>
            <pubDate>Fri, 06 Dec 2019 11:22:00 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/mounting-an-unclean-ntfs-file-system/</guid>
            <description>I&amp;rsquo;m dual booting between RHEL 8 and Windows 10 and the NTFS drive I use to share data between my two operating systems was suddenly mounted Read Only on Linux.
While trying to mount the drive again, I was facing the following message:
[root@rhel8 mnt]# umount -l /dev/sda1 [root@rhel8 mnt]# mount -t ntfs-3g -o rw /dev/sda1 /mnt/data The disk contains an unclean file system (0, 0). Metadata kept in Windows cache, refused to mount.</description>
            <content type="html"><![CDATA[
    <img src="/img/ntfs-image.png"  alt="NTFS"  class="center"  />


<!-- raw HTML omitted -->
<p>I&rsquo;m dual booting between RHEL 8 and Windows 10 and the NTFS drive I use to share data between my two operating systems was suddenly mounted Read Only on Linux.</p>
<p>While trying to mount the drive again, I was facing the following message:</p>
<pre><code>[root@rhel8 mnt]# umount -l /dev/sda1
[root@rhel8 mnt]# mount -t ntfs-3g -o rw /dev/sda1 /mnt/data
The disk contains an unclean file system (0, 0).
Metadata kept in Windows cache, refused to mount.
Falling back to read-only mount because the NTFS partition is in an
unsafe state. Please resume and shutdown Windows fully (no hibernation
or fast restarting.)
</code></pre><p>I don&rsquo;t use hibernation in Windows and fast restarting is definitely turned off. I didn&rsquo;t put Windows to sleep either, so I&rsquo;m not sure where this is coming from at this point, but since I needed to write to the disk I needed an immediate fix without rebooting.</p>
<p>To fix the issue, I need the <code>ntfsfix</code> application:</p>
<pre><code>[root@rhel8 ~]# dnf whatprovides ntfsfix
Updating Subscription Management repositories.
ntfsprogs-2:2017.3.23-11.el8.x86_64 : NTFS filesystem libraries and utilities
Repo        : epel
Matched from:
Filename    : /usr/bin/ntfsfix

[root@rhel8 ~]# dnf install -y ntfsprogs
</code></pre><p>Then I ran <code>ntfsfix</code> against <code>/dev/sda1</code>:</p>
<pre><code>[root@rhel8 ~]# ntfsfix /dev/sda1
Mounting volume... The disk contains an unclean file system (0, 0).
Metadata kept in Windows cache, refused to mount.
FAILED
Attempting to correct errors... 
Processing $MFT and $MFTMirr...
Reading $MFT... OK
Reading $MFTMirr... OK
Comparing $MFTMirr to $MFT... OK
Processing of $MFT and $MFTMirr completed successfully.
Setting required flags on partition... OK
Going to empty the journal ($LogFile)... OK
Checking the alternate boot sector... OK
NTFS volume version is 3.1.
NTFS partition /dev/sda1 was processed successfully.
</code></pre><p>That&rsquo;s it, the system then mounted the disk according to the options I&rsquo;ve specified in <code>/etc/fstab</code> and I can now write again to my NTFS disk.</p>
]]></content>
        </item>
        
        <item>
            <title>Red Hat Enterprise Linux 8 and Nvidia Optimus</title>
            <link>https://joerismissaert.dev/red-hat-enterprise-linux-8-and-nvidia-optimus/</link>
            <pubDate>Tue, 26 Nov 2019 06:37:47 +0000</pubDate>
            
            <guid>https://joerismissaert.dev/red-hat-enterprise-linux-8-and-nvidia-optimus/</guid>
            <description>Yesterday I decided to go ahead and install RHEL8 on my laptop and use it as my everyday workstation as well as a learning platform to move forward on my Red Hat certification path.
In case you&amp;rsquo;re interested, you can actually use RHEL for free with a No-Cost RHEL Developer Subscription as long as you don&amp;rsquo;t use the machine &amp;ldquo;in production&amp;rdquo;.
After a pretty straightforward installation of RHEL 8 I was immediately confronted with a first issue.</description>
            <content type="html"><![CDATA[
    <img src="/img/redhat-8-logo.png"  alt="Red Hat Logo"  class="center"  />


<p>Yesterday I decided to go ahead and install RHEL8 on my laptop and use it as my everyday workstation as well as a learning platform to move forward on my Red Hat certification path.</p>
<p>In case you&rsquo;re interested, you can actually use RHEL for free with a <a href="https://developers.redhat.com/blog/2016/03/31/no-cost-rhel-developer-subscription-now-available/" target="_blank">No-Cost RHEL Developer Subscription</a>

 as long as you don&rsquo;t use the machine &ldquo;in production&rdquo;.</p>
<p>After a pretty straightforward installation of RHEL 8 I was immediately confronted with a first issue. My external HDMI monitor wasn&rsquo;t detected.</p>
<p>My laptop uses the Nvidia Optimus technology to seamlessly switch between the integrated Intel GPU and the Nvidia discrete graphics. I couldn&rsquo;t get it to work with the default opensource <em>nouveau</em> drivers, so I went ahead and installed the non-free Nvidia proprietary drivers.</p>
<p>By default, RHEL installs the (newer) Wayland display server, but the Nvidia drivers don&rsquo;t seem to work with those so I had to switch to Xorg. Either way, I didn&rsquo;t mind since the Nvidia drivers offer better performance.</p>
<h2 id="installing-the-non-free-nvidia-drivers-on-rhel-8">Installing the non-free Nvidia drivers on RHEL 8</h2>
<p>The <em>nouveau</em> kernel module should be blacklisted first, to prevent it from loading during the next boot:</p>
<pre><code># echo 'blacklist nouveau' &gt;&gt; /etc/modprobe.d/blacklist.conf
</code></pre><p>The next step will be to install the Xorg display server and the kernel development tools:</p>
<pre><code># dnf groupinstall &quot;base-x&quot; &quot;Legacy X Window System Compatibility&quot; &quot;Development Tools&quot;
# dnf install elfutils-libelf-devel &quot;kernel-devel-uname-r == $(uname -r)&quot;
</code></pre><p>Backup and rebuild the <em>initramfs:</em></p>
<pre><code># mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r)-nouveau.img
# dracut -f
</code></pre><p>Make sure you have Dynamic Kernel Module Support installed. This will automatically rebuild the Nvidia kernel modules when you update the kernel, so you won&rsquo;t have to reinstall them afterward.</p>
<pre><code># dnf install dkms
</code></pre><p>Download the Nvidia drivers from <a href="https://www.nvidia.com/">https://www.nvidia.com/</a></p>
<p>Change the default runlevel:</p>
<pre><code># systemctl set-default multi-user.target
</code></pre><p>Reboot your system and install the driver:</p>
<pre><code># chmod +x NVIDIA-$version.run
# ./NVIDIA-$version.run 
</code></pre><p>Test the new driver by switching to the <em>graphical.target</em> runlevel:</p>
<pre><code># systemctl isolate graphical.target 
</code></pre><p>Correct the default runlevel:</p>
<pre><code># systemctl set-default graphical.target
</code></pre><h2 id="configuring-xorg">Configuring Xorg</h2>
<p>First, search for your hardware and take note of the PCI bus it&rsquo;s operating on. The PCI bus for my Intel GPU is <em>00:02.0</em> and for my Nvidia GPU <em>01:00.0.</em></p>
<pre><code># lspci | grep -EA1 'VGA|3D'
00:02.0 VGA compatible controller: Intel Corporation UHD Graphics 630 (Mobile)
01:00.0 3D controller: NVIDIA Corporation GP107M &amp;#91;GeForce GTX 1050 Ti Mobile] (rev a1)
</code></pre><p>Next, if you have an existing Xorg configuration (<em>/etc/X11/xorg.conf)</em>, make a backup, then replace the existing config with the below. Make sure you modify the BusID&rsquo;s.</p>
<pre><code>Section &quot;ServerLayout&quot;
    Identifier &quot;layout&quot;
    Screen 0 &quot;nvidia&quot;
    Inactive &quot;intel&quot;
EndSection

Section &quot;Device&quot;
    Identifier &quot;nvidia&quot;
    Driver &quot;nvidia&quot;
    BusID &quot;PCI:01:00:0&quot;
EndSection

Section &quot;Screen&quot;
    Identifier &quot;nvidia&quot;
    Device &quot;nvidia&quot;
    Option &quot;AllowEmptyInitialConfiguration&quot;
EndSection

Section &quot;Device&quot;
    Identifier &quot;intel&quot;
    Driver &quot;modesetting&quot;
    BusID &quot;PCI:00:2:0&quot;
EndSection

Section &quot;Screen&quot;
    Identifier &quot;intel&quot;
    Device &quot;intel&quot;
EndSection
</code></pre><p>The last steps consist of autostarting the dual-display configuration using <code>xrandr</code>. Execute <code>nano /etc/xdg/autostart/nvidia-optimus.desktop</code> and add the following lines :</p>
<pre><code>[Desktop Entry]
Type=Application
Name=NVIDIA Optimus
Exec=sh -c &quot;xrandr --setprovideroutputsource modesetting NVIDIA-0; xrandr --auto&quot;
NoDisplay=true
X-GNOME-Autostart-Phase=DisplayServer
</code></pre><p>Copy the same file to <code>/usr/share/gdm/greeter/autostart/nvidia-optimus.desktop</code>.</p>
<p>You should now have a working dual display after logging out and logging back in or rebooting!</p>
]]></content>
        </item>
        
    </channel>
</rss>
